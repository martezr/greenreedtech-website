[{"categories":["VMware","NSX"],"contents":"VMware recently presented at the Tech Field Day Extra event during VMware Explore 2023. The presentation covered VMware\u0026rsquo;s networking and security product updates.\nCross-Cloud Vision VMware has been building out a cross-cloud solution that enables enterprise organizations to seamlessly deploy workloads to \u0026ldquo;any\u0026rdquo; cloud. This is being realized by building out operating environments in hyperscaler clouds such as AWS (VMC on AWS), Azure, GCP (Google Cloud VMware Engine), IBM Cloud (IBM Cloud for VMware Solutions), Oracle Cloud (Oracle Cloud VMware Solutions), and others. Each of these offerings is built using VMware\u0026rsquo;s SDDC solution which leverages vSphere for compute, vSAN for storage, and NSX for networking.\nNSX serves as the networking layer across the various VMware based solutions from on-prem deployments to the hyperscaler based deployments. This provides customers with a similar operating experience for each of the different deployments but still requires them to be managed in a disaggregated manner. The introduction of NSX+ now provides companies with the ability to leverage a centralized cloud service to manage the NSX networking layer in an aggregated fashion to reduce the overall management overhead and benefit from new features as they are introduced due to it being a SaaS solution.\nThe value of VMware\u0026rsquo;s growing portfolio of hyperscaler based offerings is their adjacency to the cloud provider environment coupled with the ability to offload ongoing management of the underlying infrastructure as organizations contiue to evaluate their desire to manage datacenters themselves.\nVMware NSX+ VMware introducted vSphere+ and vSAN+ last year to provide customers with a centralized cloud control plane for managing vSphere and vSAN. NSX+ aims to offer customers running NSX with the same benefits of simplifying configuration and management from a hosted control plane.\nNSX+ Features The initial release includes the following features:\nNSX+ Policy Management: Centralized management of network and security policies (Distributed Firewall, Gateway Firewall, and Distributed IPS/IDS). Currently only supports on-prem NSX\nNSX+ Intelligence: Security oriented network traffic visibility solution. Currently only supports on-prem NSX\nNSX+ Network Detection and Response: Centralized aggregation and correlation of network security events. Currently only supports on-prem NSX\nNSX+ ALB Cloud Services: Centralized management of NSX Advanced Load Balancers (NSX Advanced Load Balancer is the load balancer technology from AVI Networks, which VMware acquired in 2019). Currently only supports VMC on AWS\nFinal Thoughts NSX+ is a compelling offering that aligns with where the industry has been moving for several years in terms of a centralized cloud control plane to manage distributed installations at scale. The part that will be the most interesting to see in the upcoming months is the ability to integrate the policy management with the native cloud networking and security to provide a robust policy management solution that extends beyond just VMware NSX.\nUltimately, the cloud control plane management overlay will effectively allow the different individual hyperscaler deployments to be treated as just another datacenter.\n","date":"06 Sep, 2023","image":"\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://s3.us-west-2.amazonaws.com/greenreedtech.com/vmware_nsx%2B/vmware-nsx%2B-title.png\" alt=\"\" class=\"w-100 img-fluid rounded\" height=\"\" width=\"\"\u003e\n","permalink":"https://www.greenreedtech.com/vmware-nsx-overview-tech-field-day-extra/","tags":["VMware","vSphere","NSX"],"title":"VMware NSX+ Overview - Tech Field Day Extra"},{"categories":["Vault"],"contents":"Security of a HashiCorp Vault deployment is of paramount importance given the sensitive nature of the information contained within the platform. Policies within the platform are used to grant and deny access to the sensitive information stored in the platform.\nAll operations in HashiCorp Vault are audited and can be shipped to a centralized logging server. In this scenario we want to utilize the audit log to find out when a policy is changed outside of the CI/CD process used to define all of our policies using code.\nVault Configuration The Vault instance needs to be configured to ship the log files to our logging server and in this example we\u0026rsquo;re using a combination of Vector, NATS and Golang. This post assumes that you know how to configure Vault audit logging and ship the logs to a centralized logging server. Details on how to configure this are covered in a previous post (https://www.greenreedtech.com/vault-audit-logging/).\nAudit Log Example Once the audit log has been configured and the logs are being shipped we can see the operations occurring in the Vault cluster. Now we need to figure out what a policy change looks like in the audit log. The easiest way to do this is to perform a policy change and identify what that operation is from a log perspective. The payload below is a policy create/update event.\n{ \u0026#34;time\u0026#34;: \u0026#34;2022-04-01T03:26:50.198973857Z\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;request\u0026#34;, \u0026#34;auth\u0026#34;: { \u0026#34;client_token\u0026#34;: \u0026#34;hmac-sha256:be5774638ad2cc9c6ad749e43b71756759d44073d8e8285ec9de4dd6cde4a5d7\u0026#34;, \u0026#34;accessor\u0026#34;: \u0026#34;hmac-sha256:286d047c74b34c1c0df8d04c978ecf11076a0a8b557ef197a5e8bfc851aa1808\u0026#34;, \u0026#34;display_name\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;policies\u0026#34;: [ \u0026#34;root\u0026#34; ], \u0026#34;token_policies\u0026#34;: [ \u0026#34;root\u0026#34; ], \u0026#34;token_type\u0026#34;: \u0026#34;service\u0026#34;, \u0026#34;token_issue_time\u0026#34;: \u0026#34;2021-11-17T20:45:23-06:00\u0026#34; }, \u0026#34;request\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;c6cec2d3-2f3e-6648-d3c0-4ffd23820004\u0026#34;, \u0026#34;client_id\u0026#34;: \u0026#34;0DHqvq2D77kL2/JTPSZkTMJbkFVmUu0TzMi0jiXcFy8=\u0026#34;, \u0026#34;operation\u0026#34;: \u0026#34;update\u0026#34;, \u0026#34;mount_type\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;client_token\u0026#34;: \u0026#34;hmac-sha256:be5774638ad2cc9c6ad749e43b71756759d44073d8e8285ec9de4dd6cde4a5d7\u0026#34;, \u0026#34;client_token_accessor\u0026#34;: \u0026#34;hmac-sha256:286d047c74b34c1c0df8d04c978ecf11076a0a8b557ef197a5e8bfc851aa1808\u0026#34;, \u0026#34;namespace\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;root\u0026#34; }, \u0026#34;path\u0026#34;: \u0026#34;sys/policies/acl/demo\u0026#34;, \u0026#34;data\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;hmac-sha256:7e83d9290c65d460fbfc3627eac3ce68b8a9e338eac22b283e009c231ab5b04d\u0026#34;, \u0026#34;policy\u0026#34;: \u0026#34;hmac-sha256:ba02f9b757ce2b0d325de063c24f613ee791b6cc03b4ce76173cfbac8d0e8fd4\u0026#34; }, \u0026#34;remote_address\u0026#34;: \u0026#34;10.0.0.136\u0026#34;, \u0026#34;remote_port\u0026#34;: 50159 } } Now that we know what the log entry looks like, we\u0026rsquo;re able to create the logic to find update events.\nThe example below keys in on the path of the request, the operation which is an update, the type which is the response to the request and finally the display name of the user account.\nrequest[\u0026#34;path\u0026#34;] = \u0026#34;sys/policies/acl/*\u0026#34; request[\u0026#34;operation\u0026#34;] = update type = response auth[\u0026#34;display_name\u0026#34;] != vaultci If there has been a recent policy change by a user outside of the CI/CD pipeline service account, vaultci in this case then the event should be flagged.\nThe following code is an example of a solution used for testing Vault events and sends a slack message when a policy update is detected.\nfunc PolicyUpdate(entry utils.AuditLogEntry) { if entry.Type == \u0026#34;response\u0026#34; \u0026amp;\u0026amp; strings.Contains(entry.Request.Path, \u0026#34;sys/policies/acl\u0026#34;) \u0026amp;\u0026amp; entry.Auth.DisplayName != \u0026#34;vaultci\u0026#34; \u0026amp;\u0026amp; entry.Request.Operation == \u0026#34;update\u0026#34; { policypath := strings.Split(entry.Request.Path, \u0026#34;/\u0026#34;) policyName := policypath[len(policypath)-1] var message string if entry.Auth.DisplayName == \u0026#34;token\u0026#34; { message = fmt.Sprintf(\u0026#34;An out of band policy change has been made to the %s policy by %s\u0026#34;, policyName, \u0026#34;root\u0026#34;) } else { message = fmt.Sprintf(\u0026#34;An out of band policy change has been made to the %s policy by %s\u0026#34;, policyName, entry.Auth.DisplayName) } actions.SendSlackMessage(\u0026#34;Vault Policy Update Alert\u0026#34;, policyName, message) fmt.Println(\u0026#34;Update detected\u0026#34;) } } References The following content was used to create this blog post.\nVault Production Guide\nhttps://www.vaultproject.io/guides/operations/production\n","date":"13 Jul, 2022","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cpicture\u003e\n  \u003csource srcset=\"/images/post/vault-policy-update_hu1a9d8ea2e2abf38620af3634e0ec2fe2_27421_545x0_resize_q95_h2_box.webp\" media=\"(max-width: 575px)\"\u003e\n  \u003csource srcset=\"/images/post/vault-policy-update_hu1a9d8ea2e2abf38620af3634e0ec2fe2_27421_600x0_resize_q95_h2_box.webp\" media=\"(max-width: 767px)\"\u003e\n  \u003csource srcset=\"/images/post/vault-policy-update_hu1a9d8ea2e2abf38620af3634e0ec2fe2_27421_700x0_resize_q95_h2_box.webp\" media=\"(max-width: 991px)\"\u003e\n  \u003csource srcset=\"/images/post/vault-policy-update_hu1a9d8ea2e2abf38620af3634e0ec2fe2_27421_1110x0_resize_q95_h2_box.webp\"\u003e\n  \u003cimg loading=\"lazy\" decoding=\"async\" class=\"w-100 img-fluid rounded\" src=\"/images/post/vault-policy-update_hu1a9d8ea2e2abf38620af3634e0ec2fe2_27421_1110x0_resize_q95_box.jpeg\" alt=\"\" width=\"1284\" height=\"254\"\u003e\n\u003c/picture\u003e\n \n \n \n\n","permalink":"https://www.greenreedtech.com/detecting-hashicorp-vault-policy-changes/","tags":["HashiCorp","Vault","DevOps","Security"],"title":"Detecting HashiCorp Vault Policy Changes"},{"categories":["Terraform"],"contents":"How do we verify that the Terraform binary we download is the same as the one HashiCorp produced?\nEnsuring the integrity of software is important to help prevent a malicious actor from tricking us into using a modified version of Terraform as well as ensuring the version we\u0026rsquo;re using isn\u0026rsquo;t corrupted. Checksums are used when there is a need to verify the integrity of software or data. The checksum process compares hashes of the software similar to how fingerprints are used for verification.\nVerifying the Terraform binary integrity? HashiCorp includes a checksum file with their releases (https://releases.hashicorp.com/terraform/1.1.9) which contains the checksums for each of the archives associated with a given release. The Terraform binary itself does not include a checksum but the archive that includes the binary is checksummed. The contents of the file looks similar to the text show below.\nc902b3c12042ac1d950637c2dd72ff19139519658f69290b310f1a5924586286 terraform_1.1.9_darwin_amd64.zip 918a8684da5a5529285135f14b09766bd4eb0e8c6612a4db7c121174b4831739 terraform_1.1.9_darwin_arm64.zip a5890d9c9f08c9160b37e3156ff2a1bc33de1db68ee942f12c4f60e8e74c8e02 terraform_1.1.9_freebsd_386.zip c204f1ca8162feb59d39bf905d8a1d7687a72b2884d81214ced8ac327908352e terraform_1.1.9_freebsd_amd64.zip c27e4b9d88598a55fe5dd0e79746e6b77eb582e12aaf4689935d0c16aa9ceebe terraform_1.1.9_freebsd_arm.zip a29a5c069e1712753ed553f7c6e63f1cd35caefee73496210461c05158b836b4 terraform_1.1.9_linux_386.zip 9d2d8a89f5cc8bc1c06cb6f34ce76ec4b99184b07eb776f8b39183b513d7798a terraform_1.1.9_linux_amd64.zip e8a09d1fe5a68ed75e5fabe26c609ad12a7e459002dea6543f1084993b87a266 terraform_1.1.9_linux_arm64.zip 800eee18651b5e552772c60fc1b5eb00cdcefddf11969412203c6de6189aa10a terraform_1.1.9_linux_arm.zip b7b509b5a0bae6d1f7e2a61d6e4deccba41e691204148f9451efe353e15ece2d terraform_1.1.9_openbsd_386.zip c702a8b31d90c9ced4b95e7facc8d7828f2a31453acc9fc258b9fffeda5ded52 terraform_1.1.9_openbsd_amd64.zip 704190dfb5cd923c2949787505f72227b2b090674f1c8ce941ca180d82d7a4ff terraform_1.1.9_solaris_amd64.zip fd2b9bc7506a85f5293d0e2d12ab5ac3be34b5915f2ae7ae7dfdc178e0abad94 terraform_1.1.9_windows_386.zip ab4df98d2256a74c151ea7ccfd69a4ad9487b4deba86a61727fb07a1348311cc terraform_1.1.9_windows_amd64.zip In this case we\u0026rsquo;ll compare the checksum from the downloaded file with the corresponding checksum in the file to see if the two match.\nAdditional Resources HashiCorp Learn Guide: https://learn.hashicorp.com/tutorials/terraform/verify-archive\n","date":"17 May, 2022","image":"\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://s3.us-west-2.amazonaws.com/greenreedtech.com/hashicorp-terraform-checksum-verification/terraform-checksum-verification.png\" alt=\"\" class=\"w-100 img-fluid rounded\" height=\"\" width=\"\"\u003e\n","permalink":"https://www.greenreedtech.com/hashicorp-terraform-checksum-verification/","tags":["HashiCorp","Terraform","DevOps","Security"],"title":"HashiCorp Terraform Checksum Verification"},{"categories":["Terraform"],"contents":"Signing software has become critically important given the recent supply chain attacks. How do we verify that the software we\u0026rsquo;re downloading is actually created by who believe created the software? In this case we want to ensure that the Terraform binary we\u0026rsquo;re downloading was created by HashiCorp. This helps prevent a scenario where a malicious actor tricks you into downloading a compromised version of Terraform.\nHow is Terraform signed? To ensure that the software was created by HashiCorp we need a way to verify that only they could have signed it. This is where cryptography comes into play, HashiCorp uses a private key to sign the software and then we use the public key to perform the verification. HashiCorp never shares the private key so as to provide the assurance that they\u0026rsquo;re the ones that created the Terraform binary. The public key is shared publicly (https://www.hashicorp.com/security and keybase.io) to verify software signed by the private key.\nHashiCorp actually signs the file that contains the Terraform binary checksums. This is the file in the on releases.hashicorp.com that ends with SHA256SUMS (i.e. - terraform_1.1.9_SHA256SUMS). This means that the file used to verify the integrity of the Terraform binary archive is what is signed and not the binary itself.\nAdditional Resources HashiCorp Learn Guide: https://learn.hashicorp.com/tutorials/terraform/verify-archive\n","date":"16 May, 2022","image":"\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://s3.us-west-2.amazonaws.com/greenreedtech.com/hashicorp-terraform-code-signing/terraform-code-signing.png\" alt=\"\" class=\"w-100 img-fluid rounded\" height=\"\" width=\"\"\u003e\n","permalink":"https://www.greenreedtech.com/hashicorp-terraform-code-signing/","tags":["HashiCorp","Terraform","DevOps","Security"],"title":"HashiCorp Terraform Code Signing"},{"categories":["vSphere"],"contents":"Containerization and Kubernetes have dominated the IT conversation for the last handful of years. Containers enable rapid development, application portability and a myriad of other benefits. Containers rely on a host operating system in order to run and ideally it would be one that is extremely lightweight since the apps running in the containers are what we really care about.\nThis is where minimal or stripped down operating systems that have been optimized for just running containers comes into play. We\u0026rsquo;ve seen a steady flow of minimal container centric operating system like CoreOS, Project Atomic, FlatCar Linux, VMware Photon, Red Hat CoreOS, K3os and countless others. I\u0026rsquo;ve tried most of these and they do the job more or less.\nThe challenge that I\u0026rsquo;ve had is that I wanted something really lightweight and easy to deploy on VMware vSphere. I wanted something without needing to figure out how to use ignition or deploying an OVA/OVF was necessary. During my search I found out about LinuxKit.\nLinuxKit LinuxKit is interesting in that it offers many of the benefits of the afformentioned container centric operating systems with the added benefit of being extremely modular and small.\nLinuxKit has been around at least publicly since 2017 (https://www.docker.com/blog/introducing-linuxkit-container-os-toolkit/) and was covered in a few DockerCon talks that year. I hadn\u0026rsquo;t really heard much about LinuxKit at the time or recently for that matter. By chance I heard about it recently when digging into the tinkerbell project originaly created by Equinix (https://www.docker.com/blog/linuxkit-as-a-commodity-for-building-linux-distributions/). The project is still alive and is actually used under the covers for Docker so that\u0026rsquo;s a good sign of continued support. Now that we know about LinuxKit, let\u0026rsquo;s take a look at how to use it to build a lightweight container host.\nBuilding the operating system LinuxKit has a command-line tool that has to be installed on the system running the command. Instructions on installing LinuxKit are found in the repository\u0026rsquo;s README file.\nA YAML file is used to define the components that are built into the operating system. The example file below contains a minimal configuration that includes things like a linux kernel, containerd, a dhcp client and nginx.\nkernel: image: linuxkit/kernel:5.10.92 cmdline: \u0026#34;console=tty0\u0026#34; init: - linuxkit/init:8f1e6a0747acbbb4d7e24dc98f97faa8d1c6cec7 - linuxkit/runc:f01b88c7033180d50ae43562d72707c6881904e4 - linuxkit/containerd:de1b18eed76a266baa3092e5c154c84f595e56da - linuxkit/ca-certificates:c1c73ef590dffb6a0138cf758fe4a4305c9864f4 onboot: - name: sysctl image: linuxkit/sysctl:bdc99eeedc224439ff237990ee06e5b992c8c1ae services: - name: getty image: linuxkit/getty:76951a596aa5e0867a38e28f0b94d620e948e3e8 env: - INSECURE=true - name: rngd image: linuxkit/rngd:4f85d8de3f6f45973a8c88dc8fba9ec596e5495a - name: dhcpcd image: linuxkit/dhcpcd:52d2c4df0311b182e99241cdc382ff726755c450 - name: nginx image: nginx:1.13.8-alpine capabilities: - CAP_NET_BIND_SERVICE - CAP_CHOWN - CAP_SETUID - CAP_SETGID - CAP_DAC_OVERRIDE binds: - /etc/resolv.conf:/etc/resolv.conf Linuxkit supports outputting the operating system in various formats such as BIO ISO, EFI ISO among others. The following example builds an ISO file named linuxkitdemo.iso based upon the yaml file above.\nlinuxkit build -format iso-bios -name linuxkitdemo minimal.yaml A few minutes later, an iso file will be created in the current directory named linuxkitdemo.iso.\nExtract kernel image: docker.io/linuxkit/kernel:5.10.92 Add init containers: Process init image: docker.io/linuxkit/init:8f1e6a0747acbbb4d7e24dc98f97faa8d1c6cec7 Process init image: docker.io/linuxkit/runc:f01b88c7033180d50ae43562d72707c6881904e4 Process init image: docker.io/linuxkit/containerd:de1b18eed76a266baa3092e5c154c84f595e56da Process init image: docker.io/linuxkit/ca-certificates:c1c73ef590dffb6a0138cf758fe4a4305c9864f4 Add onboot containers: Create OCI config for linuxkit/sysctl:bdc99eeedc224439ff237990ee06e5b992c8c1ae Add service containers: Create OCI config for linuxkit/getty:v0.8 Create OCI config for linuxkit/rngd:4f85d8de3f6f45973a8c88dc8fba9ec596e5495a Create OCI config for linuxkit/dhcpcd:52d2c4df0311b182e99241cdc382ff726755c450 Create OCI config for nginx:1.13.8-alpine Create outputs: minimal.iso At this point we\u0026rsquo;re ready to upload the generate iso file to a datastore that is accessible by our ESXi host. We could manually upload the iso file and create a virtual machine but we\u0026rsquo;ll use Terraform to automate things.\nprovider \u0026#34;vsphere\u0026#34; { user = var.vsphere_username password = var.vsphere_password vsphere_server = var.vsphere_server allow_unverified_ssl = true } data \u0026#34;vsphere_datacenter\u0026#34; \u0026#34;dc\u0026#34; { name = var.vsphere_datacenter } data \u0026#34;vsphere_datastore\u0026#34; \u0026#34;datastore\u0026#34; { name = var.vsphere_datastore datacenter_id = data.vsphere_datacenter.dc.id } data \u0026#34;vsphere_compute_cluster\u0026#34; \u0026#34;cluster\u0026#34; { name = var.vsphere_cluster datacenter_id = data.vsphere_datacenter.dc.id } data \u0026#34;vsphere_network\u0026#34; \u0026#34;network\u0026#34; { name = var.vsphere_network datacenter_id = data.vsphere_datacenter.dc.id } data \u0026#34;vsphere_host\u0026#34; \u0026#34;host\u0026#34; { name = var.vsphere_host datacenter_id = data.vsphere_datacenter.dc.id } resource \u0026#34;vsphere_file\u0026#34; \u0026#34;linuxkit_iso_upload\u0026#34; { datacenter = data.vsphere_datacenter.dc.name datastore = data.vsphere_datastore.datastore.name source_file = \u0026#34;linuxkitdemo.iso\u0026#34; destination_file = \u0026#34;linuxkitdemo.iso\u0026#34; create_directories = false } resource \u0026#34;vsphere_virtual_machine\u0026#34; \u0026#34;vm\u0026#34; { name = var.vsphere_vm_name resource_pool_id = data.vsphere_compute_cluster.cluster.resource_pool_id datastore_id = data.vsphere_datastore.datastore.id host_system_id = data.vsphere_host.host.id num_cpus = var.vsphere_vm_cpus memory = var.vsphere_vm_memory guest_id = \u0026#34;other3xLinux64Guest\u0026#34; wait_for_guest_net_timeout = 0 cdrom { datastore_id = data.vsphere_datastore.datastore.id path = \u0026#34;linuxkitdemo.iso\u0026#34; } network_interface { network_id = data.vsphere_network.network.id } disk { label = \u0026#34;disk0\u0026#34; size = 10 } depends_on = [ vsphere_file.linuxkit_iso_upload, ] } After a minute or two the virtual machine will have finished provisioning and the nginx container will be running.\nThere\u0026rsquo;s also a terraform provider for interacting with LinuxKit to create ISO images.\nterraform { required_providers { linuxkit = { version = \u0026#34;0.0.7\u0026#34; source = \u0026#34;resinstack/linuxkit\u0026#34; } } } provider \u0026#34;linuxkit\u0026#34; {} data \u0026#34;linuxkit_kernel\u0026#34; \u0026#34;kernel\u0026#34; { image = \u0026#34;linuxkit/kernel:5.6.11\u0026#34; cmdline = \u0026#34;console=tty0 console=ttyS0 console=ttyAMA0\u0026#34; } data \u0026#34;linuxkit_init\u0026#34; \u0026#34;init\u0026#34; { containers = [ \u0026#34;linuxkit/init:v0.8\u0026#34;, \u0026#34;linuxkit/runc:v0.8\u0026#34;, \u0026#34;linuxkit/containerd:v0.8\u0026#34;, \u0026#34;linuxkit/ca-certificates:v0.8\u0026#34;, ] } data \u0026#34;linuxkit_image\u0026#34; \u0026#34;sysctl\u0026#34; { name = \u0026#34;sysctl\u0026#34; image = \u0026#34;linuxkit/sysctl:v0.8\u0026#34; } data \u0026#34;linuxkit_image\u0026#34; \u0026#34;rngd1\u0026#34; { name = \u0026#34;rngd1\u0026#34; image = \u0026#34;linuxkit/rngd:v0.8\u0026#34; command = [\u0026#34;/sbin/rngd\u0026#34;, \u0026#34;-1\u0026#34;] } data \u0026#34;linuxkit_image\u0026#34; \u0026#34;getty\u0026#34; { name = \u0026#34;getty\u0026#34; image = \u0026#34;linuxkit/getty:v0.8\u0026#34; env = [\u0026#34;INSECURE=true\u0026#34;] } data \u0026#34;linuxkit_image\u0026#34; \u0026#34;rngd\u0026#34; { name = \u0026#34;rngd\u0026#34; image = \u0026#34;linuxkit/rngd:v0.8\u0026#34; } data \u0026#34;linuxkit_image\u0026#34; \u0026#34;dhcpcd\u0026#34; { name = \u0026#34;dhcpcd\u0026#34; image = \u0026#34;linuxkit/dhcpcd:v0.8\u0026#34; } data \u0026#34;linuxkit_image\u0026#34; \u0026#34;sshd\u0026#34; { name = \u0026#34;sshd\u0026#34; image = \u0026#34;linuxkit/sshd:v0.8\u0026#34; } data \u0026#34;linuxkit_config\u0026#34; \u0026#34;sshd\u0026#34; { kernel = data.linuxkit_kernel.kernel.id init = [data.linuxkit_init.init.id] onboot = [ data.linuxkit_image.sysctl.id, data.linuxkit_image.rngd1.id, ] services = [ data.linuxkit_image.getty.id, data.linuxkit_image.rngd.id, data.linuxkit_image.dhcpcd.id, data.linuxkit_image.sshd.id, ] } resource \u0026#34;linuxkit_build\u0026#34; \u0026#34;sshd\u0026#34; { config_yaml = data.linuxkit_config.sshd.yaml destination = \u0026#34;${path.module}/sshd.tar\u0026#34; } resource \u0026#34;linuxkit_image_raw_bios\u0026#34; \u0026#34;sshd\u0026#34; { build = linuxkit_build.sshd.destination destination = \u0026#34;${path.module}/sshd.raw\u0026#34; } This capability unlocks several possibilites from spinning up on-prem ECS agent nodes to Kubernetes clusters or just lightweight hosts running a single container.\n","date":"22 Apr, 2022","image":"\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://s3.us-west-2.amazonaws.com/greenreedtech.com/building_a_lightweight_container_host_for_vmware_vsphere/building-a-lightweight-container-host-for-vmware-vsphere-title.png\" alt=\"\" class=\"w-100 img-fluid rounded\" height=\"\" width=\"\"\u003e\n","permalink":"https://www.greenreedtech.com/building-a-lightweight-container-host-for-vmware-vsphere/","tags":["DevOps","Containers"],"title":"Building a Lightweight Container Host for VMware vSphere"},{"categories":["vSphere"],"contents":"One of the major benefits of using the public cloud is the integrated identity and access management (IAM). This simplifies the process of granting workloads access to other cloud services. Think about how an AWS EC2 instance is granted access to write objects to an S3 bucket. An IAM role is assigned to the instance which has a policy that grants the S3 access. This enables AWS CLI tools and application built upon the AWS SDK to access the bucket without explicitly providing credentials.\nThe seamless access works becuase the tools and sdks actually make API calls to an API that contains the credentials. The API endpoint is accessible from the EC2 instance and contains information about the instance such as the network configuration and identity details. This is the instance metadata service and is accessible at http://169.254.169.254 from within the EC2 instance.\nIAM Roles for vSphere Virtual Machines? This led me to wonder about how a similar experience could be achieved for virtual machines running on VMware vSphere. This would enable a workload identity for vSphere virtual machines running on-prem or even those running on vSphere environments in the public cloud like VMware Cloud on AWS. This could also be used as a spring board to delivering such as experience for on-prem services like object storage or database as a service.\nSolution Overview Now that there was a goal, the only thing left to do was develop a design for the solution.\nThe core of the solution is built upon the vAuth platform that I\u0026rsquo;ve been iterating on the last few years and covered in a previous post. The goal was to extend the functionality further with the idea of directly mimicing the functionality offered in AWS or any of the other public clouds. This meant that there needed to be an instance metadata service that would present CLI tools or SDKs with an API endpoint accessible from the virtual machine.\nInstance Metadata Service\nTo accomplish this goal, an agent running in the guest operating system is needed. The agent would run a web server that mocks the instance metadata service that\u0026rsquo;s available at http://169.254.169.254 from an AWS EC2 instance. Based upon a google search I found out that others have done something similar and even AWS has a mock version of the instance metadata service.\nThe version of instance metadata service created by the New York Times included details about how to accomplish the network configuration. This meant creating a new address to listen on 169.254.169.254 on the virtual machine\u0026rsquo;s loopback interface.\nInstace Metadata Service Credentials\nThe instance metadata service would need to be able to dynamically fetch AWS credentials to present via the local API endpoint. The API server would need to return the name of the role associated with the virtual machine and then present the AWS access key and secret key upon request. This is where the HashiCorp Vault integration comes into play.\nHashiCorp Vault AWS Dynamic Credentials\nThe vAuth platform aims to turn VMware vSphere into a trusted platform for HashiCorp Vault. Since the generation of dynamic AWS credentials is a core use of Vault, it only made sense to just use the existing Vault integration functionality. The agent will use the AppRole credentials the vAuth platform was already presenting to the virtual machine for authentication to Vault.\nSolution Technical Details Now that we\u0026rsquo;ve walked through the high level parts of the solution, we\u0026rsquo;ll look at the lower level details to accomplish this.\nAWS IAM Role: The AWS IAM role that will be used to grant the virtual machine access to AWS services. This would the same configuration for granting an AWS EC2 instance access to AWS services. AWS IAM Policy: The AWS IAM policy attached to the IAM role that defines the level of access granted to the EC2 instance. HashiCorp Vault AWS Secrets Backend Role: The AWS secrets backend role in Vault that maps to the role in AWS. This utilizes role assumption to closely mimic the AWS experience. HashiCorp Vault Policy: The HashiCorp Vault policy used to grant a Vault entity access HashiCorp Vault AppRole Role: The HashiCorp Vault AppRole role used to authenticate to Vault. With all the components wired up we\u0026rsquo;re able to run a like aws s3 cp to fetch a file from AWS without needing to explicitly define any credentials. The screenshot below shows running the AWS CLI from a vSphere virtual machine to fetch an object from S3.\nAll the components are chained together forming a fairly complex but powerful integration. The solution unlocks an interesting capability for organizations running workloads on VMware vSphere and looking at ways to \u0026ldquo;modernize\u0026rdquo; these workloads.\nReferences Amazon EC2 Metadata Mock: https://github.com/aws/amazon-ec2-metadata-mock\nMock EC2 Metadata: https://github.com/nytimes/mock-ec2-metadata\nGCP Workload Identity Federation: https://medium.com/google-cloud/workload-identity-federation-for-on-premise-workloads-with-spiffe-24a861b3cf6c\nHashiCorp Vault AWS Dynamic Credentials: https://www.vaultproject.io/docs/secrets/aws#sts-assumerole\n","date":"21 Apr, 2022","image":"\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://s3.us-west-2.amazonaws.com/greenreedtech.com/enabling_cloud_identity_for_vsphere_virtual_machines/enabling-cloud-identity-for-vsphere-virtual-machines.png\" alt=\"\" class=\"w-100 img-fluid rounded\" height=\"\" width=\"\"\u003e\n","permalink":"https://www.greenreedtech.com/enabling-cloud-workload-identity-for-vsphere-virtual-machines/","tags":["HashiCorp","Vault","DevOps","Security"],"title":"Enabling Cloud Workload Identity for vSphere Virtual Machines"},{"categories":["Vault"],"contents":"HashiCorp Vault supports several authentication methods for human and non-human access. Several of the non-human authentication methods are tied to specific platforms or clouds such as AWS, Kubernetes, Azure, and others. For workloads that are running on non-supported platforms, the AppRole authentication method is typically recommended for authentication. The AppRole method uses a role as the core construct as the name implies. In order to authenticate a role ID and a secret ID are required.\nAppRole Conundrum The role is typically used in a non-unique or shared manner in that it is used by multiple systems for authentication purposes. It is possible to create a role that is unique to an individual workload but can greatly increase the management overhead. This poses the classic challenge of correctly identifying which system accessed a given path in Vault. The audit log does include the remote IP address of the system that makes the request, but this method of identification relies upon a point in time mapping of the IP address to the system.\nSolution The Vault API supports the ability to add custom metadata to a generated AppRole secret ID that is displayed in the Vault audit logs. This enables the system that is trusted to generate secret IDs for a given role to associate a unique identity with the secret ID. The custom metadata appears in the requests and responses of access attempts by the token associated with the generated secret ID. The screenshot below displays an example that specifies virtual_machine_name as the identifying piece of metadata associated with the secret ID.\nWithout the addition of the additional metadata all that we would know was that the potentially shared role was used to access a given path within HashiCorp Vault.\nAdding Identity Metadata Now that we know how the solution works let’s walk through adding the custom metadata during an API call to generate a secret ID for an example role. The API expects the metadata field to be a JSON formatted string (https://www.vaultproject.io/api/auth/approle#generate-new-secret-id).\ncurl -k \\ --header \u0026#34;X-Vault-Token: s.Zdd50L8a35S6FKxC1UNzvhgU\u0026#34; \\ --request POST --data \u0026#39;{\u0026#34;metadata\u0026#34;: \u0026#34;{ \\\u0026#34;tag1\\\u0026#34;: \\\u0026#34;production\\\u0026#34; }\u0026#34;}\u0026#39; \\ https://127.0.0.1:8200/v1/auth/approle/role/vsphereapp/secret-id Conclusion In this blog post we took a quick look at how we can utilize the AppRole authentication method in HashiCorp Vault without sacrificing the benefit of named or unique identities from an auditing perspective.\n","date":"01 Dec, 2021","image":"\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://www.datocms-assets.com/2885/1620159869-brandvaultprimaryattributedcolor.svg\" alt=\"\" class=\"w-100 img-fluid rounded\" height=\"\" width=\"\"\u003e\n","permalink":"https://www.greenreedtech.com/hashicorp-vault-unique-approle-identity-logging/","tags":["DevOps","HashiCorp","Security","Vault"],"title":"HashiCorp Vault Unique AppRole Identity Logging"},{"categories":["VMware"],"contents":"Network booting operating systems isn’t a new concept and has been around for years. Bare metal deployments are typically where network booting is commonly used. Most network cards are equipped with a network boot rom that enables the server to boot from the network using the PXE protocol. The PXE protocol is an old protocol that offers limited functionality. iPXE (https://ipxe.org/) is an open source network boot firmware that extends PXE with additional functionality. Network booting comes with a number of challenges, primarily configuring and managing the associated infrastructure required (DHCP, TFTP, etc.). TFTP isn’t typically used in most IT environments and DHCP isn’t commonly available on networks or VLANs used for hosting servers.\nNetwork booting vSphere VMs\nSimilar to bare metal servers a VMware vSphere virtual machine supports network booting via the PXE protocol embedded in the virtual network card boot rom. As we mention earlier, the PXE protocol is limited in terms of functionality and requires a DHCP and TFTP server on the network. iPXE enables offers more functionality than PXE but we need a way to use the iPXE protocol instead of PXE. This is sometimes done through a process known as chain loading in which we boot iPXE with PXE but we still need DHCP and TFTP. We could also burn an ISO image to eliminate the DHCP and TFTP requirement but that means every VM needs to have an ISO mounted.\nThe solution to our problem is to use a custom network boot ROM that includes iPXE. This means that we can quasi natively use iPXE instead of PXE to network boot without DHCP or TFTP. iPXE supports interacting with VMware guest info to set the network information to boot an OS image from the network.\nWhy network boot vSphere VMs?\nWe now know that network booting is a pain but we have a fairly elegant solution but why even consider it for virtual machines when a template can easily be used. I view this as the next wave of workload management in which virtual machines are stateless shells that can be used for hosting containers that offers a simple one to one mapping with persistent storage handle further up in the proverbial stack. In my opinion this decouples the underlying VM guest operating system from the application and unlocks the ability to patch or update the guest os on the fly. Hosting the OS in memory and not writing it to disk means that every boot is an opportunity to swap out the underlying operating system. Projects like LinuxKit make this all the more viable when you think about hosting a MySQL database container on a minimal container host that only host the database container. This means I can potentially swap out the host OS at a moment’s notice while maintaining continuity at the database layer. This starts to enable a huge shift in how IT workloads are managed.\nBuilding a network boot ROM Now that we know that we can replace the network boot rom of a virtual machine we need to create an iPXE boot rom to replace the VMware shipped network boot rom.\nReduce ROM Size\nIn order to boot iPXE from the custom network boot rom we need to disable some of the iPXE features that are enabled by default. This is due to a size limitation documented in this Github issue (https://github.com/ipxe/ipxe/issues/168). After some testing the one big component that would get us under the 64kb limit on the network boot rom was unfortunately HTTPS support. We could easily chainload a version of iPXE via HTTP to then use an image with HTTPS support that ultimately loads the operating system that we want.\nEnable VMware Settings\nThe VMware guest info support is easily added by uncommented one of the VMware iPXE setting in the config/settings.h file before building the rom (https://ipxe.org/buildcfg/vmware_settings).\nBuild the ROM\nIn order to build the network boot rom for our vSphere VM requires a number of tools but we can use Docker to avoid installing a bunch of tools on our local machine. I’ve simplified this process by creating automation to handle the end to end process in this Github repo – https://github.com/martezr/vsphere-network-boot-rom-builder.\nMove the boot ROM\nWith the network boot rom built we now need to move it somewhere that can be accessed by our ESXi host. There are two common options of where to place the rom, in the virtual machine directory on the ESXi datastore or in a global location that isn’t tied to the lifecycle of an individual VM. In this case the global location made sense to avoid needing to keep track of rom build versions across potentially dozens or hundreds of VMs.\nThe custom network boot rom can be uploaded to vSphere in a number of ways but I often use SCP to simply copy it to a datastore of a single ESXi host. The rom can be uploaded to a shared datastore hosted on any of the supported storage backends such as NFS or iSCSI. Here’s an example scp command to copy the boot rom to an ESXi host.\nscp bins/15ad07b0.rom root@grtesxi02.grt.local:/vmfs/volumes/5826d416-0504a9f8-cf5f-0026b954baa6 Creating a Virtual Machine With the iPXE network boot rom created we now need to create a new virtual machine that will act as our shell for booting our operating system. All of the iPXE configuration will be handled using guest information presented to the virtual machine. We just need to provide the information during the provisioning process. The network boot rom related values (nx3bios.filename and ethernet0.opromsize) are covered in this blog post (https://thewayeye.net/2012/april/1/pxe-booting-virtual-machines-using-vmware-fusionworkstation-and-gpxe-or-ipxe/) along with the appropriate values for other network adapter types such as e1000 and e1000e.\nNameDescriptionValueguestinfo.ipxe.filenameThehttp://boot.ipxe.org/demo/boot.phpguestinfo.ipxe.scriptletAn iPXE script to run on bootifopen net0 \u0026amp;\u0026amp; chain ${filename}guestinfo.ipxe.net0.ipThe IP address used by the first network interface during the iPXE boot phase10.0.0.10guestinfo.ipxe.net0.gatewayThe network gateway used by the first network interface during the iPXE boot phase10.0.0.1guestinfo.ipxe.dnsThe DNS server used during the iPXE boot phase10.0.0.200nx3bios.filenameThe path of the network boot rom relative to the vSphere environment./vmfs/volumes/5826d416-0504a9f8-cf5f-0026b954baa6/15ad07b0.romethernet0.opromsizeThe size in kilobytes of the network rom file58880 There’s several ways to create a virtual machine in vSphere but we’ll be using Terraform in this example to simplify the creation process.\ndata \u0026#34;vsphere_datacenter\u0026#34; \u0026#34;dc\u0026#34; { name = \u0026#34;GRT\u0026#34; } data \u0026#34;vsphere_datastore\u0026#34; \u0026#34;datastore\u0026#34; { name = \u0026#34;Local_Storage_2\u0026#34; datacenter_id = data.vsphere_datacenter.dc.id } data \u0026#34;vsphere_compute_cluster\u0026#34; \u0026#34;cluster\u0026#34; { name = \u0026#34;GRT-Cluster\u0026#34; datacenter_id = data.vsphere_datacenter.dc.id } data \u0026#34;vsphere_network\u0026#34; \u0026#34;network\u0026#34; { name = \u0026#34;VM Network\u0026#34; datacenter_id = data.vsphere_datacenter.dc.id } data \u0026#34;vsphere_host\u0026#34; \u0026#34;host\u0026#34; { name = \u0026#34;grtesxi02.grt.local\u0026#34; datacenter_id = data.vsphere_datacenter.dc.id } resource \u0026#34;vsphere_virtual_machine\u0026#34; \u0026#34;vm\u0026#34; { name = \u0026#34;ipxedemo01\u0026#34; resource_pool_id = data.vsphere_compute_cluster.cluster.resource_pool_id datastore_id = data.vsphere_datastore.datastore.id host_system_id = data.vsphere_host.host.id num_cpus = 2 memory = 2048 guest_id = \u0026#34;other3xLinux64Guest\u0026#34; wait_for_guest_net_timeout = 0 network_interface { network_id = data.vsphere_network.network.id } disk { label = \u0026#34;disk0\u0026#34; size = 10 } extra_config = { \u0026#34;guestinfo.ipxe.scriptlet\u0026#34; = \u0026#34;ifopen net0 \u0026amp;\u0026amp; chain $${filename}\u0026#34; \u0026#34;guestinfo.ipxe.filename\u0026#34; = \u0026#34;http://boot.ipxe.org/demo/boot.php\u0026#34; \u0026#34;guestinfo.ipxe.net0.ip\u0026#34; = \u0026#34;10.0.0.11\u0026#34; \u0026#34;guestinfo.ipxe.net0.gateway\u0026#34; = \u0026#34;10.0.0.1\u0026#34; \u0026#34;guestinfo.ipxe.dns\u0026#34; = \u0026#34;10.0.0.200\u0026#34; \u0026#34;nx3bios.filename\u0026#34; = \u0026#34;/vmfs/volumes/60eaabc3-b06bcfb0-091a-e4d3f1d05084/15ad07b0.rom\u0026#34; \u0026#34;ethernet0.opromsize\u0026#34; = \u0026#34;58880\u0026#34; } } Booting the Virtual Machine Now that everything has been configured we’re ready to boot the virtual machine using the network settings associated with the virtual machine. We’re using the iPXE demo linux image for testing the boot process, the image should boot pretty quickly if it everything works as expected.\nReferences The following sites were referenced during the creation of this blog post.\nhttps://ipxe.org/buildcfg/vmware_settings\nhttps://thewayeye.net/2012/april/1/pxe-booting-virtual-machines-using-vmware-fusionworkstation-and-gpxe-or-ipxe/\n","date":"27 Aug, 2021","image":"\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://s3.us-west-2.amazonaws.com/greenreedtech.com/vmware-vsphere-vm-ipxe-boot-without-dhcp/vSphere_ipxe_title.png\" alt=\"\" class=\"w-100 img-fluid rounded\" height=\"\" width=\"\"\u003e\n","permalink":"https://www.greenreedtech.com/vmware-vsphere-vm-ipxe-boot-without-dhcp/","tags":["DevOps","VMware"],"title":"VMware vSphere VM iPXE Boot without DHCP"},{"categories":["HashiCorp"],"contents":"HashiCorp Vault supports a number of authentication methods including methods that utilize what HashiCorp refers to as a \u0026ldquo;trusted platform\u0026rdquo;. These include public clouds such as AWS, Azure and GCP along with platforms like Kubernetes. This method of authentication simplifies the introduction of the initial credential or secret that a workload must present to Vault by making use of information about itself that it already knows. The information that is provided to the instance or Kubernetes pod by the platform is metadata typically in the form of cryptographic data. This metadata is presented to HashiCorp Vault for authentication and verified by an API call to the underlying platform.\nThe question is why isn\u0026rsquo;t this same method used in a VMware vSphere environment to enable this same functionality. All of the other platforms natively present metadata that is difficult to guess and accessible only to that entity from a workload context. VMware vSphere doesn\u0026rsquo;t have a native virtual machine metadata service to enable this functionality. At one point there was a metadata service that was part of the work done with vSphere Integrated OpenStack (VIO) to provide this virtual machine metadata. Ultimately, OpenStack never gained the traction that many anticipated and the technical implementation of the metadata service was difficult in my opinion given the networking requirements for presenting the service to workloads.\nTo overcome the challenge of the platform lacking a native metadata service we can take advantage of a native capability that vSphere has used for years for appliances that are deployed to vSphere. vSphere guest info is used by OVA/OVF appliances to effectively pass information from the admin to the guest operating system via VMware tools. The guest operating system uses the vmtoolsd command to perform an RPC operation that allows the virtual machine to query guest info that has been populated by the administrator.\nSolution Overview Now that we have some background and context on the problem let\u0026rsquo;s take a look into the solution. We need to present the virtual machine with a unique piece of data that no other virtual can fetch or easily guess. HashiCorp Vault provides an AppRole authentication method that is ideally used for machine authentication. The AppRole requires a role ID and a secret ID to be presented to Vault to authenticate. The solution is to use an external system such as VMware Event Broker Appliance to populate a virtual machine\u0026rsquo;s guest info with the role ID and Secret ID from Vault. This is only accessible by the virtual machine itself and vSphere accounts with the appropriate permissions. VMware tools allows the guest OS to query the credentials and ultimately use them to authenticate to Vault. Now we\u0026rsquo;re ready to walk through an example of configuring Vault and VEBA to enable this authentication method.\nVault Configuration The first thing we need to is configure HashiCorp Vault to enable and configure AppRole authentication. We also need to create a Vault policy and identity/token for our VEBA function to interact with Vault.\nEnable Vault AppRole authentication method\nvault auth enable approle Create a Vault policy to associate with the virtual machine. Write the following policy to a file named vsphereapp-policy.hcl.\n# Read-only permission on \u0026#39;secret/vspheresecret\u0026#39; path path \u0026#34;secret/data/vspheresecret\u0026#34; { capabilities = [ \u0026#34;read\u0026#34; ] } Create the Vault policy using the policy file that was just created.\nvault policy write vsphereapp vsphereapp-policy.hcl\nCreate an AppRole role with the policy that was just created assigned.\nvault write auth/approle/role/vsphereapp token_policies=\u0026#34;vsphereapp\u0026#34; secret_id_ttl=10m token_ttl=20m token_max_ttl=180m Create the secret that the policy grants access to.\nvault kv put secret/vspheresecret password=SuperSecurePassword VEBA Function Now that Vault is configured, we\u0026rsquo;re ready to deploy the VEBA function but there are a few things that need to be done first.\nClone the example github repository\ngit clone https://github.com/martezr/vauth Change the working directory to the vebavauth directory\ncd vauth/vebavauth Update the vebaconfig.toml file with the appropriate vCenter and Vault details.\n[vcenter] server = \u0026#34;grtvcenter01.grt.local\u0026#34; user = \u0026#34;administrator@vsphere.local\u0026#34; password = \u0026#34;password\u0026#34; insecure = true [vault] server = \u0026#34;http://grtmanage01.grt.local:8200\u0026#34; token = \u0026#34;vaultpassword\u0026#34; Update the stack.yml file with the correct gateway for the VMware Event Broker Appliance in your environment.\nversion: 1.0 provider: name: openfaas gateway: https://grtveba01.grt.local functions: vebavauth: lang: golang-http handler: ./handler image: public.ecr.aws/i4r5n0t9/vebavauth:latest environment: write_debug: true read_debug: true function_debug: false secrets: - vebaconfig annotations: topic: VmPoweredOnEvent Log into OpenFaaS\nVEBA_GATEWAY=https://grtveba01.grt.local export OPENFAAS_URL=${VEBA_GATEWAY} cat ~/faas_pass.txt | faas-cli login -g https://grtveba01.grt.local -u admin --password-stdin --tls-no-verify Create the OpenFaas secret\nfaas-cli secret create vebaconfig --from-file=vebaconfig.toml --tls-no-verify Deploy the function\nfaas-cli deploy -f stack.yml --tls-no-verify Virtual Machine Configuration The VEBA function will inject the Vault AppRole role ID and secret ID into the VM\u0026rsquo;s advanced settings when the virtual machine is powered on but it first must be assigned a role in the custom attributes. The attribute name must be vauth-role as that is what the VEBA function uses when interacting the HashiCorp Vault instance.\nThe guest operating system now has access to the approle role name, role id and secret id to authenticate to HashiCorp Vault. A configuration management tool such as Ansible, Chef or Puppet could be used to run the command to query the guest info and subsequently generate a HashiCorp Vault agent configuration with that information. We won\u0026rsquo;t delve into the specifics of how to automate that end to end process in this post but we\u0026rsquo;ll perform the authentication operation using a script. On the guest operating system (CentOS 7 in this example) create a file name vaultlogin.sh that handle interacting with the VMware guest info and fetching the secret from Vault.\n#!/bin/bash VAULTADDR=$1 ROLE=$(/usr/bin/vmtoolsd --cmd \u0026#34;info-get guestinfo.vault.role\u0026#34;) ROLEID=$(/usr/bin/vmtoolsd --cmd \u0026#34;info-get guestinfo.vault.roleid\u0026#34;) SECRETID=$(/usr/bin/vmtoolsd --cmd \u0026#34;info-get guestinfo.vault.secretid\u0026#34;) # Create JSON payload with role and secret IDs cat \u0026lt;\u0026lt; EOF \u0026gt; payload.json { \u0026#34;role_id\u0026#34;: \u0026#34;$ROLEID\u0026#34;, \u0026#34;secret_id\u0026#34;: \u0026#34;$SECRETID\u0026#34; } EOF # Fetch Vault token TOKENDATA=$(curl -s --request POST --data @payload.json $VAULTADDR/v1/auth/approle/login) # Extract Vault token TOKEN=$(echo $TOKENDATA | python -c \\ \u0026#39;import json,sys;print json.load(sys.stdin)[\u0026#34;auth\u0026#34;][\u0026#34;client_token\u0026#34;]\u0026#39;) # Output Vault token echo $TOKEN # Fetch example secret curl -s --header \u0026#34;X-Vault-Token: $TOKEN\u0026#34; $VAULTADDR/v1/secret/data/vspheresecret | python -m json.tool With the script created we are ready to run the script, the vault address must be passed to the script.\n./vaultlogin.sh http://grtvault01.grt.local:8200 The script should return the following output which is the token used for authentication and the JSON payload for the Vault secret. The authentication token output is for testing purposes only and should be removed for production environments.\ns.XBfeDjLCamnxpTik5zMpkNa8 { \u0026#34;auth\u0026#34;: null, \u0026#34;data\u0026#34;: { \u0026#34;data\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;SuperSecurePassword\u0026#34; }, \u0026#34;metadata\u0026#34;: { \u0026#34;created_time\u0026#34;: \u0026#34;2021-02-16T16:09:47.867249231Z\u0026#34;, \u0026#34;deletion_time\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;destroyed\u0026#34;: false, \u0026#34;version\u0026#34;: 2 } }, \u0026#34;lease_duration\u0026#34;: 0, \u0026#34;lease_id\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;renewable\u0026#34;: false, \u0026#34;request_id\u0026#34;: \u0026#34;dd2e325a-09f5-d416-6fb8-7b50041cd204\u0026#34;, \u0026#34;warnings\u0026#34;: null, \u0026#34;wrap_info\u0026#34;: null } There are a number of updates that can be made to the function to enable it to be used in a real environment but this blog post is to showcase how the solution can be used to simplify the authentication of VMware vSphere workloads to HashiCorp Vault.\nReferences https://www.vaultproject.io/docs/auth/approle https://www.vaultproject.io/api/secret/kv/kv-v2\n","date":"16 Feb, 2021","image":"\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://s3.us-west-2.amazonaws.com/greenreedtech.com/hashicorp-vault-vsphere-authentication-with-vmware-event-broker-appliance-veba/vsphere-vault-auth-veba-title.png\" alt=\"\" class=\"w-100 img-fluid rounded\" height=\"\" width=\"\"\u003e\n","permalink":"https://www.greenreedtech.com/hashicorp-vault-vsphere-authentication-with-vmware-event-broker-appliance-veba/","tags":["DevOps","HashiCorp","Security","Vault"],"title":"HashiCorp Vault vSphere Authentication with VMware Event Broker Appliance (VEBA)"},{"categories":["VMware"],"contents":"The VMware Event Broker Appliance (VEBA) fling is a really interesting project that enables administrators to take advantage of event driven automation in a VMware vSphere environment. I\u0026rsquo;ve been meaning to kick the tires on using the appliance as I\u0026rsquo;ve thought a lot about vSphere event driven security and what that looks like from a technical implementation perspective. In this blog post we\u0026rsquo;ll take a look at how to quickly get started with developing a Golang function for the VMware Event Broker Appliance (VEBA) that executes when a virtual machine is powered on.\nThe following requirements must be in place:\nDeployed VMware Event Broker Appliance (VEBA) with the OpenFaas event processor (https://vmweventbroker.io/kb/install-openfaas) Docker image registry (VMware Harbor, Docker Hub, AWS ECR, etc.) for hosting the created image Golang installed OpenFaas CLI installed (https://docs.openfaas.com/cli/install/) Create a directory named vebago to start creating our function.\nmkdir vebago Change the working directory to the vebago directory we just created.\ncd vebago Create a file named stack.yml with the following contents. The gateway should be replaced with the VEBA\u0026rsquo;s URL (\u0026ldquo;https://appliance_fqdn\u0026rdquo;) and the image should reflect the location of where the Docker image will be stored. The name of the function is vebago and the topic parameter underneath annotations is a comma separated list of the vSphere events, in this case we are using a VM power on event.\nversion: 1.0 provider: name: openfaas gateway: https://grtveba01.grt.local functions: vebago: lang: golang-http handler: ./handler image: grtharbor01.grt.local/library/vebago:latest environment: write_debug: true read_debug: true function_debug: false annotations: topic: VmPoweredOnEvent The next thing we need to do is create a directory for our Golang code. The name of the directory that hosts Golang code is the value specified for the handler parameter in our stack.yml file that we just created.\nmkdir handler Change the working directory to the handler directory that we just created.\ncd handler Create a file named handler.go with the code snippet displayed below. The Golang code parses the event payload and prints the payload to STDOUT.\npackage function import ( \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; handler \u0026#34;github.com/openfaas/templates-sdk/go-http\u0026#34; \u0026#34;github.com/vmware/govmomi/vim25/types\u0026#34; ) // cloudEvent is a subsection of a Cloud Event. type cloudEvent struct { Data types.Event `json:\u0026#34;data\u0026#34;` Source string `json:\u0026#34;source\u0026#34;` Subject string `json:\u0026#34;subject\u0026#34;` } // Handle a function invocation func Handle(req handler.Request) (handler.Response, error) { cloudEvt, err := parseCloudEvent(req.Body) if err != nil { return errRespondAndLog(fmt.Errorf(\u0026#34;parsing cloud event data: %w\u0026#34;, err)) } log.Println(cloudEvt) return handler.Response{ Body: req.Body, StatusCode: http.StatusOK, }, nil } func errRespondAndLog(err error) (handler.Response, error) { log.Println(err.Error()) return handler.Response{ Body: []byte(err.Error()), StatusCode: http.StatusInternalServerError, }, err } func parseCloudEvent(req []byte) (cloudEvent, error) { var event cloudEvent err := json.Unmarshal(req, \u0026amp;event) if err != nil { return cloudEvent{}, fmt.Errorf(\u0026#34;unmarshalling json: %w\u0026#34;, err) } return event, nil } Change the working directory back to the vebago directory.\ncd .. Now that we have our function components we need to download the OpenFaas template for Golang HTTP in order to properly create the function.\nfaas-cli template store pull golang-http\nWe now need to log into OpenFaas in order to deploy the function to OpenFaas.\ncat ~/faas_pass.txt | faas-cli login -g https://grtveba01.grt.local -u admin --password-stdin --tls-no-verify Once the template has been downloaded the function needs to be built, pushed to our docker registry and deployed. Fortunately we can do all of this with a single command to help save us some typing.\nfaas-cli up -f stack.yml --build-arg GO111MODULE=on --tls-no-verify The output of the command should be similar to that shown below.\nImage: grtharbor01.grt.local/library/vebago:latest built. [0] \u0026lt; Building vebago done in 1.23s. [0] Worker done. Total build time: 1.23s [0] \u0026gt; Pushing vebago [grtharbor01.grt.local/library/vebago:latest]. The push refers to repository [grtharbor01.grt.local/library/vebago] 03536ee2d42d: Layer already exists e8b6d1455770: Layer already exists 6ad006678b05: Layer already exists 5159f12913cf: Layer already exists 5f70bf18a086: Layer already exists e872e0b26f6c: Layer already exists 777b2c648970: Layer already exists latest: digest: sha256:755e3dbcdcaa490d069ecd2b547093b37f8abb37f1d2b7ed39d7402ce8fc08e1 size: 1784 [0] \u0026lt; Pushing vebago [grtharbor01.grt.local/library/vebago:latest] done. [0] Worker done. Deploying: vebago. Deployed. 202 Accepted. URL: https://grtveba01.grt.local/function/vebago.openfaas-fn With the function successfully deployed we are now ready to validate that the function is actually being called when a virtual machine is powered on and that the function outputs the event details. Since we aren\u0026rsquo;t sending the functions output via email or slack we have to view the STDOUT of the kubernetes pod. An easy way to view the output of the pod is to SSH into the VEBA virtual machine. The first thing to do is\nkubectl get pods -n openfaas-fn The name of the pods that back the OpenFaas functions should be displayed similar to the output displayed below. In this case we want to use the name of the pod that starts with vebago.\nNAME READY STATUS RESTARTS AGE template-compliance-899b5dbd8-qkbf9 1/1 Running 0 30h vebago-665f4c4f56-mwnjr 1/1 Running 0 9m18s The name of the pods start with the name of the OpenFaas function name specified in the stack.yml file.\nkubectl logs vebago-665f4c4f56-mwnjr -n openfaas-fn --follow At this point we can power on a virtual machine to validate if the function is working properly. The payload should include information about the virtual machine that was just powered on.\nForking - ./handler [] 2021/02/11 22:18:18 Started logging stderr from function. 2021/02/11 22:18:18 Started logging stdout from function. 2021/02/11 22:18:18 OperationalMode: http 2021/02/11 22:18:18 Timeouts: read: 10s, write: 10s hard: 10s. 2021/02/11 22:18:18 Listening on port: 8080 2021/02/11 22:18:18 Writing lock-file to: /tmp/.lock 2021/02/11 22:18:18 Metrics listening on port: 8081 2021/02/11 22:25:43 stderr: 2021/02/11 22:25:43 {{{} 18191 18189 2021-02-11 22:25:39.472999 +0000 UTC VSPHERE.LOCAL\\Administrator 0xc00005bc20 0xc00005bc50 0xc00005bc80 0xc00005bcb0 centos8base on grtesxi01.grt.local in GRT is powered on } https://grtvcenter01.grt.local/sdk VmPoweredOnEvent} 2021/02/11 22:25:43 POST / - 200 OK - ContentLength: 884 While this is a very simple example we can start to see the power being able trigger code in response to a vSphere event.\nReferences: http://www.patrickkremer.com/vmware-event-broker-appliance-part-xiii-deploying-go-functions/\n","date":"12 Feb, 2021","image":"\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://s3.us-west-2.amazonaws.com/greenreedtech.com/creating-a-vmware-event-broker-appliance-veba-golang-function/VEBA-Golang-OpenFaas-Function-2.png\" alt=\"\" class=\"w-100 img-fluid rounded\" height=\"\" width=\"\"\u003e\n","permalink":"https://www.greenreedtech.com/creating-a-vmware-event-broker-appliance-veba-golang-function/","tags":["VMware","VEBA"],"title":"Creating a VMware Event Broker Appliance (VEBA) Golang Function"},{"categories":["Career"],"contents":"I have made the decision to become the director of technical marketing at Morpheus Data and I start that new position today. Morpheus Data is a company that I became aware of during Cloud Field Day 3 (CFD3) while serving as a delegate for the event. Cloud Field Day is one of several events hosted by Gestalt IT in which several vendors present one at a time to a group of IT professionals that ask questions and provide feedback about the presentations. I was intrigued by what was covered during the presentation as it is a product that is right in my technical wheelhouse.\nWhat is Morpheus Data? Let\u0026rsquo;s get the first question out of the way for those that may have never heard of Morpheus Data. Morpheus Data is a software platform that fits in the Cloud Management Platform (CMP) space and can be be used for a lot more. One of the fundamental challenges it helps address is the complexity of provisioning workloads, especially on-premises workloads that often require multiple manual hand offs between teams to fully provision a server. The Morpheus platform has over 80 integrations with common tools and platforms such as Ansible, VMware NSX, InfoBlox and more to tie together the various parts of an end to end provisioning workflow. In addition to provisioning the platform has a number of other features that provide organizations with a centralized platform to enable IT agility along with governance at an enterprise scale.\nWhy Morpheus Data? One of the key value propositions of Morpheus Data is tying together disparate tools to help create business value. An aspect of technology that I enjoy most is creating complex orchestration workflows to solve complex problems. The broad number of Morpheus integrations allows me to tap into my varied experiences in IT and make use of the breadth of knowledge that I\u0026rsquo;ve gained during my career. In addition to the technical fit I like the opportunity that smaller companies provide to be able to wear a few different hats to directly impact the success of the organization. Something I took note of as I followed Morpheus Data from a far was the pace at which they were adding new features and capabilities to the platform and the ability to be able to speak to new and interesting things on a regular cadence is very appealing.\nWhy Technical Marketing? The majority of my career has been spent as an IT practitioner in various roles from desktop support to architecting large automation projects. The last several years I have focused on sharing the knowledge that I\u0026rsquo;ve gained over my years in IT. I\u0026rsquo;ve blogged for a number of years and began speaking at tech conferences a couple of years ago so I\u0026rsquo;ve had platforms through which I was able to fulfill that desire. In my role at Morpheus Data I\u0026rsquo;ll be creating marketing content with a focus on the technical aspects of the product and partner integrations. This is a dream role for me as it forces me to continue to grow non hands on keyboard skills such as storytelling and written communication. This role also allows me to to focus on the strategic aspects of helping to grow a business.\nI\u0026rsquo;m extremely excited about the opportunity to help grow Morpheus Data and the new challenge that I am about to begin.\n","date":"01 Feb, 2021","image":"\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://s3.us-west-2.amazonaws.com/greenreedtech.com/moving-to-morpheus-data/Morpheus-horizontal-v2.png\" alt=\"\" class=\"w-100 img-fluid rounded\" height=\"\" width=\"\"\u003e\n","permalink":"https://www.greenreedtech.com/moving-to-morpheus-data/","tags":["Morpheus"],"title":"Moving to Morpheus Data"},{"categories":["StackStorm"],"contents":"Chaos Engineering is the practice of experimenting or injecting faults into a system to test how the system responds to the failure (https://principlesofchaos.org). Chaos Engineering is still new for many organizations and can be a daunting practice to adopt at first but even baby steps into the practice can be immediately beneficial.\nIn this blog post we\u0026rsquo;ll look at how we can introduce chaos engineering into our deployment pipeline to test the resiliency of our example web application. We\u0026rsquo;ll be using StackStorm for running our deployment pipeline and integrate it with ChaosToolkit for carrying out the Chaos Engineering. ChaosToolkit is an open source framework written in Python for running chaos engineering experiments.\nThe code for this blog post can be found in the following Github repos:\nTerraform code: https://github.com/martezr/orchestration-demos/tree/master/terraform/chaosstack ChaosToolkit experiment: https://github.com/martezr/orchestration-demos/blob/master/chaos_experiments/aws.json StackStorm workflow: https://github.com/martezr/stackstorm-grtlab/blob/master/actions/workflows/chaos_demo.yaml Architecture This example will deploy a web application to AWS using Terraform and include a load balancer that serves traffic to two (2) EC2 instances in an autoscale group (ASG). This architecture should allow for the web application to be accessible even if there was an issue with one of the EC2 instances in the autoscale group. This is a common architectural pattern for deploying highly available applications but there are various configuration issues that could prevent the application from being available following a failure.\nExperiment Given the architecture in our example what we will test or evaluate is if our website can continue to be accessed if one of the EC2 instances in the autoscale group is terminated.\nConfiguration: ChaosToolkit supports passing environment variables to the experiment. In this example we are passing the name of our autoscale group along with the DNS name of the application load balancer.\nHypothesis: The hypothesis is that we expect our web application to still be reachable even if one of the instances in our AWS autoscale group is terminated.\nChaos Method: In this experiment we are terminating an EC2 instance in the autoscale group that backs our application load balancer.\nThe following is the ChaosToolkit experiment that defines what will happen during the experiment. Experiments are written in a JSON format.\n{ \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Ensure AWS resilliency\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;If an autoscale fails another node should become the leader\u0026#34;, \u0026#34;tags\u0026#34;: [\u0026#34;aws\u0026#34;], \u0026#34;contributions\u0026#34;: { \u0026#34;reliability\u0026#34;: \u0026#34;high\u0026#34;, \u0026#34;security\u0026#34;: \u0026#34;none\u0026#34;, \u0026#34;scalability\u0026#34;: \u0026#34;none\u0026#34; }, \u0026#34;configuration\u0026#34;: { \u0026#34;aws_region\u0026#34;: \u0026#34;us-east-1\u0026#34;, \u0026#34;asg_name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;env\u0026#34;, \u0026#34;key\u0026#34;: \u0026#34;asg_name\u0026#34; }, \u0026#34;alb_dns_name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;env\u0026#34;, \u0026#34;key\u0026#34;: \u0026#34;alb_dns_name\u0026#34; } }, \u0026#34;steady-state-hypothesis\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Application responds\u0026#34;, \u0026#34;probes\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;probe\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;check-web-service\u0026#34;, \u0026#34;tolerance\u0026#34;: 200, \u0026#34;provider\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;timeout\u0026#34;: [2, 2], \u0026#34;expected_status\u0026#34;: 200, \u0026#34;url\u0026#34;: \u0026#34;http://${alb_dns_name}\u0026#34; } } ] }, \u0026#34;method\u0026#34;: [ { \u0026#34;provider\u0026#34;: { \u0026#34;module\u0026#34;: \u0026#34;chaosaws.asg.actions\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;python\u0026#34;, \u0026#34;func\u0026#34;: \u0026#34;terminate_random_instances\u0026#34;, \u0026#34;arguments\u0026#34;: { \u0026#34;asg_names\u0026#34;: [\u0026#34;${asg_name}\u0026#34;], \u0026#34;instance_count\u0026#34;: 1 } }, \u0026#34;type\u0026#34;: \u0026#34;action\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;terminate-asg-instance\u0026#34;, \u0026#34;pauses\u0026#34;: { \u0026#34;after\u0026#34;: 10 } }, { \u0026#34;ref\u0026#34;: \u0026#34;check-web-service\u0026#34; } ], \u0026#34;rollbacks\u0026#34;: [] } Pipeline Now that we have our experiment we need an automated method of running the experiment. In this case we\u0026rsquo;ll be using StackStorm to deploy our application stack and perform the experiment automatically once everything has been deployed.\nClone Github Repo\nThe code used in this example is hosted on Github and we need to clone that repo to allow the code to be executed by the latter steps of the workflow.\nclone_build_repo: action: git.clone input: source: https://github.com/martezr/orchestration-demos.git destination: /stackstorm/orchestration-demos hosts: localhost next: - when: \u0026lt;% succeeded() %\u0026gt; do: generate_aws_credentials Fetch Dynamic AWS Credentials\nIn this example we\u0026rsquo;re going to utilize HashiCorp Vault to fetch short-lived credentials to use for our pipeline execution. In this case the instance of HashiCorp Vault has already been configured and integrated with StackStorm. The AWS credentials are being published to allow subsequent steps in the workflow to access the credentials.\ngenerate_aws_credentials: action: vault.read path=\u0026#34;aws/creds/stackstorm\u0026#34; next: - when: \u0026lt;% succeeded() %\u0026gt; publish: - access_key: \u0026lt;% result().result.access_key %\u0026gt; - secret_key: \u0026lt;% result().result.secret_key %\u0026gt; - stdout: \u0026lt;% result().stdout %\u0026gt; - stderr: \u0026lt;% result().stderr %\u0026gt; do: - provision_app_stack Deploy Stack\nWe\u0026rsquo;re going to build our application stack using HashiCorp Terraform. In this example our Terraform code is in the Github repository we fetched in an earlier stage of the workflow. The state is being stored locally on the system for demonstration purposes and we are passing the AWS credentials from the last step as variables.\nprovision_app_stack: delay: 15 action: terraform.pipeline input: plan_path: /stackstorm/orchestration-demos/terraform/chaosstack backend: {\u0026#34;path\u0026#34;:\u0026#34;/stackstorm/chaosstack.tfstate\u0026#34;} variable_dict: {\u0026#34;aws_access_key\u0026#34;:\u0026#34;\u0026lt;% ctx(\u0026#39;access_key\u0026#39;) %\u0026gt;\u0026#34;,\u0026#34;aws_secret_key\u0026#34;:\u0026#34;\u0026lt;% ctx(\u0026#39;secret_key\u0026#39;) %\u0026gt;\u0026#34;} next: - when: \u0026lt;% succeeded() %\u0026gt; do: output_alb_dns Output ALB DNS Name\nIn the case of this example we won\u0026rsquo;t be using a dedicated DNS address for our web application see we need an additional stage in the workflow to capture the DNS name associated with the ALB that has been provisioned. This DNS address will be used by our chaos experiment to test reachability to our web application.\noutput_alb_dns: action: terraform.output input: plan_path: /stackstorm/orchestration-demos/terraform/chaosstack next: - when: \u0026lt;% succeeded() %\u0026gt; publish: - alb_dns_name: \u0026lt;% result().result.alb_dns_name.value %\u0026gt; do: run_chaos_experiment Run Chaos Experiment\nThe final stage of the workflow is to run our chaos experiment against the application stack we just deployed. The goal of the experiment is to evaluate if our web application is able to continue serving traffic relatively uninterrupted following the termination of an instance in the AWS autoscale group. In addition to the AWS credentials being passed to the stage we\u0026rsquo;re also including the name of the autoscale group and the load balancer\u0026rsquo;s DNS name for reachability testing.\nrun_chaos_experiment: delay: 10 action: chaostoolkit.run_experiment input: path: /stackstorm/orchestration-demos/chaos_experiments/aws.json env: {\u0026#34;AWS_ACCESS_KEY_ID\u0026#34;:\u0026#34;\u0026lt;% ctx(\u0026#39;access_key\u0026#39;) %\u0026gt;\u0026#34;,\u0026#34;AWS_SECRET_ACCESS_KEY\u0026#34;:\u0026#34;\u0026lt;% ctx(\u0026#39;secret_key\u0026#39;) %\u0026gt;\u0026#34;,\u0026#34;asg_name\u0026#34;:\u0026#34;\u0026lt;% ctx(\u0026#39;asg_name\u0026#39;) %\u0026gt;\u0026#34;,\u0026#34;alb_dns_name\u0026#34;:\u0026#34;\u0026lt;% ctx(\u0026#39;alb_dns_name\u0026#39;) %\u0026gt;\u0026#34;} The screenshot below is the deployment workflow in the StackStorm web interface.\nThis example shows the core capability of adding Chaos Engineering into the pipeline but could be extended to destroy the environment if the experiment failed, it could push the report from the experiment into a CMDB to create a record of how resilient the application is along with countless other possible integrations.\nHopefully this sets you on the path of getting excited about incorporating Chaos Engineering into your deployment pipelines to actually validate the assumptions that are made about your application\u0026rsquo;s resiliency.\n","date":"24 Jun, 2020","image":"\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://s3.us-west-2.amazonaws.com/greenreedtech.com/deployment-pipeline-chaos-engineering-with-stackstorm-and-chaostoolkit/chaos_update.png\" alt=\"\" class=\"w-100 img-fluid rounded\" height=\"\" width=\"\"\u003e\n","permalink":"https://www.greenreedtech.com/deployment-pipeline-chaos-engineering-with-stackstorm-and-chaostoolkit/","tags":["Chaostoolkit","Chaos Engineering","DevOps","StackStorm"],"title":"Deployment Pipeline Chaos Engineering with StackStorm and ChaosToolkit"},{"categories":["Vault"],"contents":"Revoking the root token on a production HashiCorp Vault deployment is one of the recommended best practices for securing an instance of HashiCorp Vault. The actual process to revoke the root token is fairly straightforward by running the vault token revoke command and providing the root token at the command line. In a previous blog post we looked at how to detect when a new root token has been generated. This might be necessary to perform certain operations that require root to carry out. One thing to be aware of is that multiple root tokens can be active at a single moment in time so there is no one root token but potentially many. With the potential for multiple root tokens we need a way to determine if there are any currently active root tokens on our Vault deployment.\nIdentifying active root tokens requires us to query the Vault instance and evaluate the token accessors to determine which one is a root token. In order to do this we\u0026rsquo;ll be using Python to do this programmatically and the hvac python library to easily interact with HashiCorp Vault. The following is a list of the high level steps that our script will perform to identify any active root tokens.\nFetch the list of token accessor keys (This is a list of all the active tokens) Iterate through the list and perform a lookup on each token accessor Minimum Vault Permissions The following are the minimum Vault permissions required to perform this search on the Vault deployment.\n# List token accessors path \u0026#34;auth/token/accessors/\u0026#34; { capabilities = [\u0026#34;list\u0026#34;, \u0026#34;sudo\u0026#34;] } # Lookup individual tokens by accessor path \u0026#34;auth/token/lookup-accessor\u0026#34; { capabilities = [\u0026#34;update\u0026#34;] } Python Code The python code is fairly simple in that it first fetches all the active token accessors and then iterates through each of them performing a token lookup to identify any token that has the \u0026ldquo;root\u0026rdquo; policy associated. The script expects the address of the Vault instance and the token used to access Vault to be provided as environment variables. The python script requires the hvac and PTable python libraries to be installed.\n#!/usr/local/bin/python3 import os import time import hvac import urllib3 from prettytable import PrettyTable urllib3.disable_warnings() try: os.environ[\u0026#34;VAULT_ADDR\u0026#34;] except Exception: print(\u0026#34;The VAULT_ADDR environment must be set.\u0026#34;) os._exit(1) try: os.environ[\u0026#34;VAULT_TOKEN\u0026#34;] except Exception: print(\u0026#34;The VAULT_TOKEN environment must be set.\u0026#34;) os._exit(1) client = hvac.Client(url=os.environ[\u0026#39;VAULT_ADDR\u0026#39;], verify=False, token=os.environ[\u0026#34;VAULT_TOKEN\u0026#34;]) payload = client.list(\u0026#39;auth/token/accessors\u0026#39;) keys = payload[\u0026#39;data\u0026#39;][\u0026#39;keys\u0026#39;] x = PrettyTable() x.field_names = [\u0026#34;Display Name\u0026#34;, \u0026#34;Creation Time\u0026#34;, \u0026#34;Expiration Time\u0026#34;, \u0026#34;Policies\u0026#34;, \u0026#34;Token Accessor\u0026#34;] for key in keys: output = client.lookup_token(key, accessor=True) display_name = output[\u0026#39;data\u0026#39;][\u0026#39;display_name\u0026#39;] creation_date = time.strftime(\u0026#39;%Y-%m-%d %H:%M:%S\u0026#39;, time.localtime(output[\u0026#39;data\u0026#39;][\u0026#39;creation_time\u0026#39;])) expire_time = output[\u0026#39;data\u0026#39;][\u0026#39;expire_time\u0026#39;] policies = output[\u0026#39;data\u0026#39;][\u0026#39;policies\u0026#39;] accessor = key if \u0026#34;root\u0026#34; in policies: x.add_row([display_name, creation_date, expire_time, policies, accessor]) print(x) If there are any active root tokens a table similar to the table below will be displayed with information about the root tokens.\nRevoking Identified Tokens If there are root tokens identified at that point we likely want to revoke them and that can be done by running the vault token revoke command and adding the -accessor switch with the token accessor displayed in the script output table similar to the full command below.\nvault token revoke -accessor xooFDg520Sh3LFfL7QOERsQN\nThe root token will have been revoked and re-running the script should return a result of no active root tokens.\nReferences HashiCorp Vault API for listing token accessors\nhttps://www.vaultproject.io/api-docs/auth/token#list-accessors\nHashiCorp Vault API for token lookup by accessor\nhttps://www.vaultproject.io/api-docs/auth/token#lookup-a-token-accessor\nHVAC Python Library\nhttps://hvac.readthedocs.io/en/stable/overview.html\n","date":"25 May, 2020","image":"\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://s3.us-west-2.amazonaws.com/greenreedtech.com/identifying-active-hashicorp-vault-root-tokens/identifying_active_root.png\" alt=\"\" class=\"w-100 img-fluid rounded\" height=\"\" width=\"\"\u003e\n","permalink":"https://www.greenreedtech.com/identifying-active-hashicorp-vault-root-tokens/","tags":["HashiCorp","Vault","DevOps","Security"],"title":"Identifying Active HashiCorp Vault Root Tokens"},{"categories":["Vault"],"contents":"HashiCorp Vault generates a default root token during installation and best practice dictates that the token should be revoked once the deployment has been setup. There are certain critical operations that can only be carried out by a root token and requires that a new root token be generated. Given the immense power that the root token garners it would be ideal to identify when a root token is generated. In this example we\u0026rsquo;ll utilize the Vault audit log to determine when the process to generate a new root token is started and when it is successfully completed. Splunk will be used as our centralized logging server in this example.\nVault 1.4.0 added logging for root token generation events to the audit log to enable the easy identification of these events. Prior to Vault 1.4.0 there was no record of this operation in the Vault audit log to identify when a new root token is generated. The Vault system log output did show the execution of the root token generation commands.\nThe generation of a new root token involves multiple steps such as initializing the operation which generates an OTP, providing the unseal information and finally decoding the encode version of the generated root token.\nDetect Root Token Init\nWe want to be able to detect when someone attempts to generate a root token. The following command is used to initialize the root token generation process from the command line.\nvault operator generate-root -init The following search string returns the appropriate entry from the Vault audit log for this event.\nsource=\u0026#34;/opt/vault/logs/vault_audit.log\u0026#34; type=response request.path=\u0026#34;sys/generate-root/attempt\u0026#34; request.operation=\u0026#34;update\u0026#34; Successful Root Token Generation\nWe want to be able to detect when someone is able to successfully generate a new root token. The following search string returns the audit log entry from when the encoded token is generated.\nsource=\u0026#34;/opt/vault/logs/vault_audit.log\u0026#34; type=response request.path=\u0026#34;sys/generate-root/update\u0026#34; response.data.complete=true References The following content was used to create this blog post:\nRoot Token Generation Audit Log Pull Request\nhttps://github.com/hashicorp/vault/pull/8301\nVault Root Token Generation Lesson\nhttps://learn.hashicorp.com/vault/operations/ops-generate-root\nVault Root Token Command Reference\nhttps://www.vaultproject.io/docs/commands/operator/generate-root\n","date":"20 May, 2020","image":"\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://s3.us-west-2.amazonaws.com/greenreedtech.com/detecting-hashicorp-vault-root-token-generation/vault_root_token_gen2fixed-1024x568.png\" alt=\"\" class=\"w-100 img-fluid rounded\" height=\"\" width=\"\"\u003e\n","permalink":"https://www.greenreedtech.com/detecting-hashicorp-vault-root-token-generation/","tags":["HashiCorp","Vault","DevOps","Security"],"title":"Detecting HashiCorp Vault Root Token Generation"},{"categories":["Vault"],"contents":"Security of a HashiCorp Vault deployment is of paramount importance given the sensitive nature of the information contained within the platform. During the initial configuration process the root token is used to perform the setup and should be used to create less privileged named accounts. These accounts should be used for day to day administration of the Vault deployment and the root token should only be used in scenarios where it is absolutely necessary. The reason for this is the all-powerful privileges that the root token wields on the platform. Based upon this information it is critical to know whenever the root token is used to log into the Vault deployment and that\u0026rsquo;s what will be covered in this blog post.\nAll operations in HashiCorp Vault are audited and can be sent to an audit log or syslog that is ultimately shipped to a centralized logging server for greater security. In this example we want to utilize the audit log to find out when the root token is used for a login operation. We\u0026rsquo;ll use Splunk for our centralized logging server.\nVault Configuration The Vault instance needs to be configured to ship the log files to our logging server and in this example the logging server is a Splunk instance. This post assumes that you know how to configure Vault audit logging and ship the logs to a centralized logging server. Details on how to configure this are configured in a previous post (https://www.greenreedtech.com/vault-audit-logging/) but at a high level audit logging must be enabled in Vault, configured to write to a log file or syslog and configured to ship those logs to a centralized logging server.\nSplunk Configuration With the audit log now being shipped to our Splunk instance we can see the operations occurring in the Vault cluster. Now we need to figure out what a root login looks like in the audit log. The easiest way to do this was to perform a root login and identify what that operation was from a log perspective. The payload below is a root token login event.\n{ \u0026#34;auth\u0026#34;: { \u0026#34;accessor\u0026#34;: \u0026#34;hmac-sha256:67869871d870282745682c729d86cee81acb5346c3dbecb573b7d44ea5506d06\u0026#34;, \u0026#34;client_token\u0026#34;: \u0026#34;hmac-sha256:8fe52f85c93aad7df87c7203f864a9900d25451a1cc88c486ae0c951bd3a8936\u0026#34;, \u0026#34;display_name\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;policies\u0026#34;: [ \u0026#34;root\u0026#34; ], \u0026#34;token_policies\u0026#34;: [ \u0026#34;root\u0026#34; ], \u0026#34;token_type\u0026#34;: \u0026#34;service\u0026#34; }, \u0026#34;request\u0026#34;: { \u0026#34;client_token\u0026#34;: \u0026#34;hmac-sha256:8fe52f85c93aad7df87c7203f864a9900d25451a1cc88c486ae0c951bd3a8936\u0026#34;, \u0026#34;client_token_accessor\u0026#34;: \u0026#34;hmac-sha256:67869871d870282745682c729d86cee81acb5346c3dbecb573b7d44ea5506d06\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;3ab2651a-899b-0a98-c626-73c405d89d02\u0026#34;, \u0026#34;namespace\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;root\u0026#34; }, \u0026#34;operation\u0026#34;: \u0026#34;read\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;auth/token/lookup-self\u0026#34;, \u0026#34;remote_address\u0026#34;: \u0026#34;10.0.0.70\u0026#34; }, \u0026#34;response\u0026#34;: { \u0026#34;data\u0026#34;: { \u0026#34;accessor\u0026#34;: \u0026#34;hmac-sha256:67869871d870282745682c729d86cee81acb5346c3dbecb573b7d44ea5506d06\u0026#34;, \u0026#34;creation_time\u0026#34;: 1576768685, \u0026#34;creation_ttl\u0026#34;: 0, \u0026#34;display_name\u0026#34;: \u0026#34;hmac-sha256:6b89bf27681e54af63afe4a0b936bbf618f8d9b17bcc68df8c11470f7328d745\u0026#34;, \u0026#34;entity_id\u0026#34;: \u0026#34;hmac-sha256:de212e047ea6043f736d83549f3dae8612c688af0d5a6b4d19a262473c5b8bea\u0026#34;, \u0026#34;expire_time\u0026#34;: null, \u0026#34;explicit_max_ttl\u0026#34;: 0, \u0026#34;id\u0026#34;: \u0026#34;hmac-sha256:8fe52f85c93aad7df87c7203f864a9900d25451a1cc88c486ae0c951bd3a8936\u0026#34;, \u0026#34;meta\u0026#34;: null, \u0026#34;num_uses\u0026#34;: 0, \u0026#34;orphan\u0026#34;: true, \u0026#34;path\u0026#34;: \u0026#34;hmac-sha256:20039952cb073210bc9cb0fa1dc3dec3e49bcd8a72b5dd2a9f9ce415010c91a0\u0026#34;, \u0026#34;policies\u0026#34;: [ \u0026#34;hmac-sha256:6b89bf27681e54af63afe4a0b936bbf618f8d9b17bcc68df8c11470f7328d745\u0026#34; ], \u0026#34;ttl\u0026#34;: 0, \u0026#34;type\u0026#34;: \u0026#34;hmac-sha256:05148f41a98c981f657d9a0cb0b647e1f32a764719da2e75f27a497485eb9b7a\u0026#34; } }, \u0026#34;time\u0026#34;: \u0026#34;2020-01-31T13:14:37.132982729Z\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;response\u0026#34; } Now that we know what we want to look for we can create our Splunk search string to return only root login events.\nauth.display_name=\u0026#34;root\u0026#34; type=\u0026#34;response\u0026#34; request.path=\u0026#34;auth/token/lookup-self\u0026#34; If there has been a recent login with the root token we should see results from our search similar to those below.\nThis can be extended to send a notification or create a helpdesk ticket, most logging platforms have the ability to trigger an action when a designated event occurs. As an example, Splunk supports a handful of alert actions such as sending an email, outputting results to a lookup file or sending a webhook to an external system.\nReferences The following content was used to create this blog post.\nVault Production Guide\nhttps://www.vaultproject.io/guides/operations/production\nSplunk Alert Webhooks\nhttps://docs.splunk.com/Documentation/Splunk/8.0.1/Alert/Webhooks\n","date":"09 Feb, 2020","image":"\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://s3.us-west-2.amazonaws.com/greenreedtech.com/detecting-hashicorp-vault-root-login/vault_root_login.png\" alt=\"\" class=\"w-100 img-fluid rounded\" height=\"\" width=\"\"\u003e\n","permalink":"https://www.greenreedtech.com/detecting-hashicorp-vault-root-login/","tags":["HashiCorp","Vault","DevOps","Security"],"title":"Detecting HashiCorp Vault Root Login"},{"categories":["Career"],"contents":"In this blog post we\u0026rsquo;ll look at the HashiCorp Vault plugin for Puppet Bolt that enables authentication credentials for Bolt to be retrieved from an instance of HashiCorp Vault. HashiCorp Vault is a secrets management platform that is commonly used to store secrets such as API keys, passwords and SSH private keys. This solution helps to avoid secret sprawl where passwords and credentials are widely distributed across an environment making it difficult to track where they are.\nThe plugin was added to Puppet Bolt in version 1.28.0 and natively supports token and userpass Vault authentication methods. Documentation for the plugin can be found here - https://forge.puppet.com/puppetlabs/vault/readme.\nVault Setup We\u0026rsquo;ll configure a development instance of HashiCorp Vault to walk through the plugin\u0026rsquo;s functionality. The following assumes a basic knowledge of how to setup at least a dev instance of HashiCorp Vault. This information can be found on HashiCorp\u0026rsquo;s website if necessary (https://learn.hashicorp.com/vault).\nWrite the Windows administrator password to Vault.\nWe need to write the password for the Windows machine to the secret/credentials/windows path in Vault.\nvault kv put secret/credentials/windows password=Puppet123 Write the Linux SSH private key to Vault\nWe need to write the SSH private key for the Linux machine to the secret/credentials/linux path in Vault. The private key has been saved to a file named bolt_id_rsa which is being uploaded as a secret.\nvault kv put secret/credentials/linux privatekey=@bolt_id_rsa Create a Vault Policy\nWe need to create a Vault Policy that allows the token to read the secrets in the \u0026ldquo;credentials\u0026rdquo; secret structure but nothing else under the \u0026ldquo;secret\u0026rdquo; space. The policy should be saved to a file name bolt-policy.hcl which we\u0026rsquo;ll use in the next command to actually create the policy in Vault.\npath \u0026#34;secret/data/credentials/*\u0026#34; { capabilities = [\u0026#34;read\u0026#34;] } path \u0026#34;secret/metadata/credentials/*\u0026#34; { capabilities = [\u0026#34;list\u0026#34;,\u0026#34;read\u0026#34;] } The following command creates a policy named \u0026ldquo;bolt\u0026rdquo; using the policy file that was just created in the previous step.\nvault policy write bolt bolt-policy.hcl Token Authentication HashiCorp Vault supports token authentication that allows a token generated by an existing token or another authentication method to be used for interacting with a Vault instance. When logging into Vault via an authentication method a token is generated which is assigned privileges based upon the policies associated with the token during creation.\nGenerate Token\nLogged in with the root token we can generate a token with the bolt policy we created in a previous step associated.\nLogging in with the root token is not recommended for day to day administration in production environments\nvault token create -policy=bolt The output should be similar to that below and the \u0026ldquo;token\u0026rdquo; is what we\u0026rsquo;ll use in our Bolt inventory file to authenticate.\nKey Value — —– token s.3649w1Fh80RtwSteoDzWuDUi token_accessor Ki4onGqPfwdnMVJQFX40ddqZ token_duration 768h token_renewable true token_policies [\u0026#34;bolt\u0026#34; \u0026#34;default\u0026#34;] identity_policies [] policies [\u0026#34;bolt\u0026#34; \u0026#34;default\u0026#34;] Bolt Configuration File\nThe Bolt configuration file is used to set the global configuration for Bolt and in this example we\u0026rsquo;re adding the configuration for the Vault plugin to this file. The token has been added in plaintext to the file but we can specify the \u0026ldquo;VAULT_TOKEN\u0026rdquo; environment variable or use another plugin for encryption such as the PKCS7 to avoid the token being in plaintext in the Bolt config file.\nmodulepath: \u0026#34;~/.puppetlabs/bolt-code/modules:~/.puppetlabs/bolt-code/site-modules\u0026#34; concurrency: 10 format: human winrm: ssl: false ssh: host-key-check: false plugins: vault: server_url: http://127.0.0.1:8200 auth: method: token token: s.3649w1Fh80RtwSteoDzWuDUi In addition to generating a token using an existing token HashiCorp Vault generates a token upon login when using other authentication methods, such as those covered below.\nHuman Interaction\nHashiCorp Vault supports a number of authentication methods that are intended for a human or interactive login. The following methods are commonly utilized but is not a complete list.\nLDAP Okta Radius Github Machine Interaction\nThere is often a need to run automation as part of a pipeline or scheduled task. This means that we can\u0026rsquo;t expect a human to perform a login operation to fetch a token. In this case we need to use one of HashiCorp Vault\u0026rsquo;s authentication methods intended for non-human interaction.\nThe following authentication methods are intended for non-human authentication.\nAWS Kubernetes TLS Azure AppRole UserPass Authentication In addition to the token authentication method the plugin also supports userpass authentication. HashiCorp Vault supports a userpass authentication method that is a local user database in Vault that utilizes a username and password for authentication.\nEnable UserPass Authentication\nThe authentication engine or backend needs to be enabled before we can use that authentication method.\nvault auth enable userpass Create a user account\nThe userpass authentication method has been enabled and now we need to create a user account with a password and associate the Bolt Vault policy.\nvault write auth/userpass/users/puppetbolt password=Password123 policies=bolt We can validate that the user was successfully created and that the bolt policy is associated by running the vault login command below, a password prompt will be presented\nvault login -method=userpass username=puppetbolt Output similar to that shown below will be displayed and we can see that the \u0026ldquo;bolt\u0026rdquo; policy is associated with the credentials.\nSuccess! You are now authenticated. The token information displayed below is already stored in the token helper. You do NOT need to run \u0026#34;vault login\u0026#34; again. Future Vault requests will automatically use this token. Key Value — —– token s.Xd4v1qoCtnKEnDHjzYTRm1KC token_accessor 1ZRZUUYfWRJOGNBj8qnbRGvf token_duration 768h token_renewable true token_policies [\u0026#34;bolt\u0026#34; \u0026#34;default\u0026#34;] identity_policies [] policies [\u0026#34;bolt\u0026#34; \u0026#34;default\u0026#34;] token_meta_username puppetbolt Bolt Configuration File\nThe Bolt configuration file is used to set the global configuration for Bolt and in this example we’re adding the configuration for the Vault plugin to this file. The username and password have been added in plaintext to the file. Similar to the token authentication method we can use another plugin for encryption such as the PKCS7 to avoid the password being in plaintext in the Bolt config file.\nmodulepath: \u0026#34;~/.puppetlabs/bolt-code/modules:~/.puppetlabs/bolt-code/site-modules\u0026#34; concurrency: 10 format: human winrm: ssl: false ssh: host-key-check: false plugins: vault: server_url: http://127.0.0.1:8200 auth: method: userpass user: puppetbolt pass: Password123 Bolt Inventory File With either the authentication method configured for the Vault plugin now we just need to create an inventory file for specifying the path in Vault where Bolt will fetch the secret from.\nWindows\nThe Bolt inventory file below is an example of using the plugin to retrieve the password used by Bolt for connecting to Windows nodes via WinRM.\nversion: 2 targets: – uri: winnode1 config: transport: winrm winrm: user: administrator password: _plugin: vault path: secret/credentials/windows field: password version: 2 With the inventory file created we can run a simple command to check that the plugin is able to fetch the credentials from Vault.\nbolt plan run facts -i inventory.yaml –targets=winnode1 Linux\nThe Bolt inventory file below is an example of using the plugin to retrieve the SSH private key used by Bolt for connecting to Linux nodes via SSH.\nversion: 2 targets: – uri: linuxnode1 config: transport: ssh ssh: user: root private-key: key-data: _plugin: vault path: secret/credentials/linux field: privatekey version: 2 With the inventory file created we can run a simple command to check that the plugin is able to fetch the credentials from Vault.\nbolt plan run facts -i inventory.yaml –targets=linuxnode1 The plugin provides the ability to allow Puppet Bolt to offload a critical component of any automation process to a dedicated platform in Vault. This enables a more robust solution for managing secrets in a secure and automated manner.\nReferences Puppet Bolt HashiCorp Vault plugin documentation\nhttps://forge.puppet.com/puppetlabs/vault/readme\nPuppet Bolt Inventory File\nhttps://puppet.com/docs/bolt/latest/inventory_file_v2.html\n","date":"26 Nov, 2019","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cpicture\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_545x0_resize_q95_h2_box.webp\" media=\"(max-width: 575px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_600x0_resize_q95_h2_box.webp\" media=\"(max-width: 767px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_700x0_resize_q95_h2_box.webp\" media=\"(max-width: 991px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_h2_box.webp\"\u003e\n  \u003cimg loading=\"lazy\" decoding=\"async\" class=\"w-100 img-fluid rounded\" src=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_box.jpg\" alt=\"\" width=\"970\" height=\"500\"\u003e\n\u003c/picture\u003e\n \n \n \n\n","permalink":"https://www.greenreedtech.com/puppet-bolt-vault-inventory-plugin/","tags":["DevOps","Terraform","VMware"],"title":"Puppet Bolt Vault Inventory Plugin"},{"categories":["DevOps"],"contents":"HashiCorp Terraform is a popular Infrastructure as Code (IaC) tool that is used for provisioning virtual machines or cloud instances along with other resources. Once a virtual machine or cloud instance is provisioned typically it still needs to be configured which includes security baselines, application dependency configuration and even application deployment. The tasks are generally accomplished with a Configuration Management (CM) tool such as Puppet or Puppet Bolt.\nThe Puppet Bolt Terraform inventory plugin parses the Terraform state for resources that have been created by Terraform and it enables Bolt to be run against infrastructure created by Terraform without explicitly specifying connection information such as the IP address. This is especially useful in public cloud environments where the IP address associated with an instance is often dynamic. The Puppet Bolt Terraform plugin supports state retrieval for local state backends as well as remote backends.\nIn this blog post we\u0026rsquo;ll look at how to use the Puppet Bolt Terraform inventory plugin to dynamically retrieve the IP address of a virtual machine provisioned in a VMware vSphere environment which will allow us to run Puppet Bolt against the virtual machine using dynamic information.\nTerraform Manifest The Terraform code creates a single CentOS 7 virtual machine in a VMware vSphere environment. The Terraform state is stored in HashiCorp Consul for centralized state management.\nterraform { backend \u0026#34;consul\u0026#34; { address = \u0026#34;10.0.0.6:8500\u0026#34; scheme = \u0026#34;http\u0026#34; path = \u0026#34;terraform/bolt/boltserver/terraform.tfstate\u0026#34; datacenter = \u0026#34;puppet-bolt\u0026#34; } } resource \u0026#34;vsphere_virtual_machine\u0026#34; \u0026#34;boltserver\u0026#34; { name = \u0026#34;boltserver\u0026#34; num_cpus = 2 memory = 4096 resource_pool_id = \u0026#34;${data.vsphere_compute_cluster.cluster.resource_pool_id}\u0026#34; datastore_id = \u0026#34;${data.vsphere_datastore.datastore.id}\u0026#34; guest_id = \u0026#34;${data.vsphere_virtual_machine.template.guest_id}\u0026#34; scsi_type = \u0026#34;${data.vsphere_virtual_machine.template.scsi_type}\u0026#34; network_interface { network_id = \u0026#34;${data.vsphere_network.network.id}\u0026#34; adapter_type = \u0026#34;${data.vsphere_virtual_machine.template.network_interface_types[0]}\u0026#34; } disk { label = \u0026#34;disk0\u0026#34; size = \u0026#34;${data.vsphere_virtual_machine.template.disks.0.size}\u0026#34; eagerly_scrub = \u0026#34;${data.vsphere_virtual_machine.template.disks.0.eagerly_scrub}\u0026#34; thin_provisioned = \u0026#34;${data.vsphere_virtual_machine.template.disks.0.thin_provisioned}\u0026#34; } clone { template_uuid = \u0026#34;${data.vsphere_virtual_machine.template.id}\u0026#34; customize { linux_options { host_name = \u0026#34;boltserver\u0026#34; domain = \u0026#34;grt.local\u0026#34; } network_interface {} dns_server_list = [\u0026#34;10.0.0.200\u0026#34;] ipv4_gateway = \u0026#34;10.0.0.1\u0026#34; } } } The virtual machine can be provisioned using the standard Terraform workflow of init, plan and apply.\nPuppet Bolt Inventory The Puppet Bolt inventory file defines resource information such as how to the connect to the resource such as the IP address and credentials for the resource. With the virtual machine provisioned the Bolt inventory file needs to be created to dynamically access the virtual machine.\nDisplayed below is the Puppet Bolt inventory file used in this example.\nversion: 2 groups: – name: boltservers targets: – _plugin: terraform resource_type: vsphere_virtual_machine.boltserver uri: default_ip_address dir: . name: name backend: remote Inventory Properties The Terraform plugin accepts a number of parameters for interacting with the Terraform state.\nresource_type: The Terraform resource that should be matched with support for regular expressions (i.e. - resource_type.resource_name).\nuri: The property of the Terraform resource used to connect to the resource such as the IP address or DNS name.\ndir: The inventory plugin executes the Terraform state pull command to retrieve the state. The directory specified should be where the Terraform manifests for the resources resided.\nname: The property of the Terraform resource to use as the target name\nbackend: The type of backend to load the Terraform state from. The two supported types are local and remote.\nPuppet Bolt Plan With the resource provisioned and the inventory file configured Puppet Bolt can be run against the provisioned resource without explicitly defining the IP address or DNS name.\nThe command below runs Puppet Bolt against the virtual machine provisioned with Terraform. In this example we\u0026rsquo;re just using the facts plan that comes with Bolt to show the functionality but a much more complex plan could be used to deploy an application or configure the security baseline as an example.\nbolt plan run facts -i inventory.yaml nodes=boltservers The command generates output similar to that shown below which dynamically retrieved the IP address to access the virtual machine.\nbolt plan run facts -i inventory.yaml nodes=boltservers Starting: plan facts Starting: task facts on 10.0.0.15 Finished: task facts with 0 failures in 5.94 sec Finished: plan facts in 5.97 sec Finished on 10.0.0.15: { \u0026#34;os\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;CentOS\u0026#34;, \u0026#34;distro\u0026#34;: { \u0026#34;codename\u0026#34;: \u0026#34;Core \u0026#34; }, \u0026#34;release\u0026#34;: { \u0026#34;full\u0026#34;: \u0026#34;7.6.1810\u0026#34;, \u0026#34;major\u0026#34;: \u0026#34;7\u0026#34;, \u0026#34;minor\u0026#34;: \u0026#34;6\u0026#34; }, \u0026#34;family\u0026#34;: \u0026#34;RedHat\u0026#34; } } Successful on 1 node: boltserver Ran on 1 node This integration greatly simplifies configuring machines provisioned with Terraform and eliminates the need to specify the IP address of virtual machine before running Puppet Bolt. This plugin can be especially useful in deploying complex application stacks with dependencies between machines.\nReference Puppet Bolt Inventory Plugin\nhttps://puppet.com/docs/bolt/latest/inventory_file_v2.html#plugins-and-dynamic-inventory\n","date":"06 Oct, 2019","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cpicture\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_545x0_resize_q95_h2_box.webp\" media=\"(max-width: 575px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_600x0_resize_q95_h2_box.webp\" media=\"(max-width: 767px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_700x0_resize_q95_h2_box.webp\" media=\"(max-width: 991px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_h2_box.webp\"\u003e\n  \u003cimg loading=\"lazy\" decoding=\"async\" class=\"w-100 img-fluid rounded\" src=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_box.jpg\" alt=\"\" width=\"970\" height=\"500\"\u003e\n\u003c/picture\u003e\n \n \n \n\n","permalink":"https://www.greenreedtech.com/puppet-bolt-terraform-inventory-plugin/","tags":["DevOps","Puppet","Terraform"],"title":"Puppet Bolt Terraform Inventory Plugin"},{"categories":["Packer"],"contents":"Treating workloads as \u0026ldquo;cattle\u0026rdquo; or immutable is a popular management paradigm for stateless workloads and is especially prevalent for such workloads that are hosted in a public cloud. The concept is based upon the notion that servers are pre-baked with all of the software that is needed for the application and any stateful data is pushed to an external persistent storage mechanism such as an object storage or a message queue. There\u0026rsquo;s a number of methods for accomplishing the pre-baking that includes the configuration and installation of software on the templates that will be used for immutable servers.\nIn this post we’ll use HashiCorp Packer and Puppet Bolt to create a VMware vSphere template that contains a specific version of HashiCorp Consul that we can use for immutable upgrades or potential rollbacks. The plugin also works for other Packer builders but vSphere will be covered in this post.\nPuppet Bolt Puppet Bolt will be used in this example to enable the creation of our pre-baked template without the need for a Puppet master. To simplify the integration between Packer and Puppet Bolt we\u0026rsquo;ll use an unofficial Packer Puppet Bolt provisioner (https://github.com/martezr/packer-provisioner-puppet-bolt) that I\u0026rsquo;ve developed. We\u0026rsquo;ll take a look at the integration once we get to the HashiCorp Packer section of this blog post but first we need to set up Bolt to enable us to install Consul on the template.\nIn our example we\u0026rsquo;ll use the Consul module available on the Puppet forge (https://forge.puppet.com/KyleAnderson/consul) to install HashiCorp Consul. Since we\u0026rsquo;ll be using the module we need to add it to the Puppetfile we\u0026rsquo;re using for Puppet Bolt along with the module it requires.\nBolt Configuration The following steps will install and configure Puppet Bolt.\nInstall the Puppet Bolt repo\nAdd the Puppet repo to the system, install bolt and create the Bolt directory.\nsudo rpm -Uvh https://yum.puppet.com/puppet6/puppet6-release-el-7.noarch.rpm sudo yum install -y puppet-bolt mkdir -p ~/.puppetlabs/bolt/ Create Puppet Bolt Configuration File\nThis step creates a Puppet Bolt global configuration file that the Terraform Puppet provisioner uses for the Bolt process.\ncat \u0026lt;\u0026lt; EOF \u0026gt; ~/.puppetlabs/bolt/bolt.yaml modulepath: \u0026#34;~/.puppetlabs/bolt-code/modules:~/.puppetlabs/bolt-code/site-modules\u0026#34; inventoryfile: \u0026#34;~/.puppetlabs/bolt/inventory.yaml\u0026#34; concurrency: 10 format: human ssh: host-key-check: false user: root EOF Create Puppet Bolt Inventory File\nThe Terraform Puppet provisioner targets the Puppet master specified in the Terraform code but Bolt needs additional information for connecting to the Puppet master such as user credentials.\ncat \u0026lt;\u0026lt; EOF \u0026gt; ~/.puppetlabs/bolt/inventory.yaml --- config: ssh: host-key-check: false user: root EOF Bolt Puppetfile\nThis step creates a Bolt Puppetfile for installing the two Puppet modules required by the Terraform Puppet provisioner.\nModules from the Puppet Forge. mod \u0026lsquo;KyleAnderson-consul\u0026rsquo;, \u0026lsquo;5.1.0\u0026rsquo; mod \u0026lsquo;puppetlabs-stdlib\u0026rsquo;, \u0026lsquo;5.2.0\u0026rsquo; mod \u0026lsquo;puppet-archive\u0026rsquo;, \u0026lsquo;3.2.1\u0026rsquo; mod \u0026lsquo;camptocamp-systemd\u0026rsquo;, \u0026lsquo;2.6.0\u0026rsquo; mod \u0026lsquo;puppetlabs-powershell\u0026rsquo;, \u0026lsquo;2.3.0\u0026rsquo;\nBolt Plan\nWe\u0026rsquo;re going to create a custom plan for installing HashiCorp Consul using the module from the forge. The first thing we need to do is create the module structure in our Bolt directory.\nmkdir -p ~/.puppetlabs/bolt-code/site-modules/bolt_consul/plans With the directory structure in place we just need to create our Puppet manifest file (install.pp) in the \u0026ldquo;plans\u0026rdquo; directory with the code below.\nplan bolt_consul::install( TargetSpec $nodes, String $version, ) { # Ensure puppet tools are installed and gather facts for the apply apply_prep([$nodes]) apply($nodes) { package { \u0026#39;unzip\u0026#39;: ensure =\u0026gt; present, } class { \u0026#39;::consul\u0026#39;: version =\u0026gt; $version, config_hash =\u0026gt; { \u0026#39;bootstrap_expect\u0026#39; =\u0026gt; 1, \u0026#39;client_addr\u0026#39; =\u0026gt; \u0026#39;0.0.0.0\u0026#39;, \u0026#39;data_dir\u0026#39; =\u0026gt; \u0026#39;/opt/consul\u0026#39;, \u0026#39;datacenter\u0026#39; =\u0026gt; \u0026#39;east-aws\u0026#39;, \u0026#39;log_level\u0026#39; =\u0026gt; \u0026#39;INFO\u0026#39;, \u0026#39;node_name\u0026#39; =\u0026gt; \u0026#39;server\u0026#39;, \u0026#39;server\u0026#39; =\u0026gt; true, \u0026#39;ui\u0026#39; =\u0026gt; true, } } $puppet_packages = [ \u0026#39;puppet-agent\u0026#39;, \u0026#39;puppet-release\u0026#39; ] package { $puppet_packages: ensure =\u0026gt; absent, } } } Once the manifest has been created the Bolt directory structure should look similar to that below.\n[root@centos7base ~]# tree -L 3 ~/.puppetlabs/ /root/.puppetlabs/ ├── bolt │ ├── analytics.yaml │ ├── bolt.yaml │ ├── inventory.yaml │ └── Puppetfile └── bolt-code ├── modules │ ├── archive │ ├── consul │ ├── powershell │ ├── stdlib │ └── systemd └── site-modules └── bolt_consul 10 directories, 4 files HashiCorp Packer In this example we\u0026rsquo;ll use an existing vSphere template as our base template. The template only contains standard OS configurations and we\u0026rsquo;ll use the combination of Packer and Bolt to create our Consul specific template. The following are the tasks we\u0026rsquo;ll need to accomplish to setup Packer.\nDownload the latest version of HashiCorp Packer from releases.hashicorp.com Download the latest version of the Jetbrains vSphere Packer plugin Download the latest version of the Puppet Bolt Packer provisioner plugin Create Packer template JSON file Install HashiCorp Packer\nwget https://releases.hashicorp.com/packer/1.4.2/packer_1.4.2_linux_amd64.zip \u0026amp;\u0026amp; unzip packer_1.4.2_linux_amd64.zip \u0026amp;\u0026amp; chmod +x packer Install the Puppet Bolt Packer provisioner plugin\nwget https://github.com/martezr/packer-provisioner-puppet-bolt/releases/download/v0.1.0/packer-provisioner-puppet-bolt_0.1.0_Linux_x86_64.tar.gz \u0026amp;\u0026amp; tar -xzf packer-provisioner-puppet-bolt_0.1.0_Linux_x86_64.tar.gz Install the Jetbrains Packer vSphere builder plugin\nwget https://github.com/jetbrains-infra/packer-builder-vsphere/releases/download/v2.3/packer-builder-vsphere-clone.linux Packer Template\nThe Packer template below shows the integration provided by the Puppet Bolt plugin.\n{ \u0026#34;variables\u0026#34;: { \u0026#34;vsphere_password\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;ssh_password\u0026#34;: \u0026#34;\u0026#34; }, \u0026#34;builders\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;vsphere-clone\u0026#34;, \u0026#34;vcenter_server\u0026#34;: \u0026#34;10.0.0.205\u0026#34;, \u0026#34;username\u0026#34;: \u0026#34;administrator@vsphere.local\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;{{user `vsphere_password`}}\u0026#34;, \u0026#34;insecure_connection\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;template\u0026#34;: \u0026#34;centos7base\u0026#34;, \u0026#34;vm_name\u0026#34;: \u0026#34;boltpackdemo\u0026#34;, \u0026#34;cluster\u0026#34;: \u0026#34;GRT-Cluster\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;10.0.0.246\u0026#34;, \u0026#34;communicator\u0026#34;: \u0026#34;ssh\u0026#34;, \u0026#34;ssh_username\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;ssh_password\u0026#34;: \u0026#34;{{user `ssh_password`}}\u0026#34; } ], \u0026#34;provisioners\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;puppet-bolt\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;bolt_plan\u0026#34;: \u0026#34;bolt_consul::install\u0026#34;, \u0026#34;bolt_params\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;1.5.3\u0026#34; } } ] } With the Packer template file create we just need to run the build command to provision the vSphere template using Packer.\n./packer build -var \u0026#39;vsphere_password=password\u0026#39; -var \u0026#39;ssh_password=password\u0026#39; bolt.json The build should produce output similar to that below and shows how Bolt is being executed against the template machine.\nOnce the build process has been completed we should have a pre-baked HashiCorp Consul server. We can provision a new virtual machine based upon the template and see that consul is already installed by browsing to the Consul web interface (http://ip_address:8500) in a web browser.\nThe provisioner plugin currently only supports Linux workloads and Windows support will be added in a future release.\nReferences Consul Puppet Module\nhttps://forge.puppet.com/KyleAnderson/consul\nPacker Puppet Bolt Provisioner Plugin\nhttps://github.com/martezr/packer-provisioner-puppet-bolt\nJetbrains Packer vSphere Builder Plugin\nhttps://github.com/jetbrains-infra/packer-builder-vsphere\n","date":"14 Aug, 2019","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cpicture\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_545x0_resize_q95_h2_box.webp\" media=\"(max-width: 575px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_600x0_resize_q95_h2_box.webp\" media=\"(max-width: 767px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_700x0_resize_q95_h2_box.webp\" media=\"(max-width: 991px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_h2_box.webp\"\u003e\n  \u003cimg loading=\"lazy\" decoding=\"async\" class=\"w-100 img-fluid rounded\" src=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_box.jpg\" alt=\"\" width=\"970\" height=\"500\"\u003e\n\u003c/picture\u003e\n \n \n \n\n","permalink":"https://www.greenreedtech.com/hashicorp-packer-puppet-bolt-provisioner/","tags":["HashiCorp","Puppet","Bolt","Packer"],"title":"HashiCorp Packer Puppet Bolt Provisioner"},{"categories":["StackStorm"],"contents":"Template management is a critical facet of infrastructure management and traditionally one of the more challenging operations there is. The advent of tools like HashiCorp Packer have provided administrators with the ability declaratively automate the template creation process.\nAutomation is great but in most organizations a single piece of automation is part of a larger process that includes tickets and potentially other hand offs.\nIn this example we\u0026rsquo;ll walk through creating a process for creating VMware vSphere machine templates in a fully automated fashion with deep integrations with other tools for documenting and testing the created template.\nThe code for this example can be found in the following GitHub repositories.\nThe StackStorm pack that contains the orchestration workflow: https://github.com/martezr/stackstorm-grtlab\nThe Terraform and Packer code: https://github.com/martezr/orchestration-demos\nTools The orchestration detailed in this example contains a number of tools that are covered below.\nStackStorm StackStorm is an open source orchestration tool with numerous integrations with third-party tools for simplifying the creation of complex orchestration workflows. StackStorm will drive the orchestration in this example and handle all of the hand offs.\nGithub Github is used for storing all of the code used in this example and is where StackStorm will fetch code from.\nServiceNow ServiceNow is an ITSM or IT Service Management platform that we\u0026rsquo;ll use for incident management and as a CMDB in our example.\nJenkins Jenkins is a continuous integration (CI) server typically used for testing and building code. In our example Jenkins will be used for validating our deployed code. This shows how the workflow could incorporate tests that may already exist that are created by the applications developers or QA team. This helps to reduce potential issues created by base template updates\nServerSpec ServerSpec is a Ruby based testing framework that I being used for acceptance testing in our example. A few simple tests are being run but it shows how the integration can provide tremendous value.\nHashiCorp Packer Packer is a tool for defining a template in a declarative manner via a JSON file.\nHashiCorp Terraform Terraform is used to provision the virtual machine that is used for validating that the new template doesn\u0026rsquo;t break any downstream applications.\nStages The high level stages of the orchestration displayed in the image below are detailed below along with the StackStorm code for each stage.\nCreate Incident This first stage of the workflow is to create an incident in ServiceNow for the new template. The goal of this stage is integrating with ServiceNow to provide a record of what is being done as part of the automated template creation process. The StackStorm code below shows that we\u0026rsquo;re creating a new record in the incident table and providing some metadata about the incident such as the description and the submitter.\nOnce the record has been created in the incident table we are storing the sysid of the ServiceNow incident in the incident_id variable for use later when we resolve the incident. Additionally we\u0026rsquo;re specifying the next stage of the workflow which cleaning up any existing clones of our code git repo.\ncreate_incident: action: servicenow.create_record input: table: \u0026#34;incident\u0026#34; payload: {\u0026#34;short_description\u0026#34;:\u0026#34;CentOS 7 Monthly Image Update\u0026#34;,\u0026#34;category\u0026#34;:\u0026#34;software\u0026#34;,\u0026#34;description\u0026#34;:\u0026#34;Updating CentOS 7 template\u0026#34;,\u0026#34;caller_id\u0026#34;:\u0026#34;stackstorm\u0026#34;} next: - when: \u0026lt;% succeeded() %\u0026gt; publish: incident_id=\u0026lt;% result().result.sys_id %\u0026gt; do: cleanup_git_repo The screenshot below displays a new incident created by the orchestration to track the work performed during this process and align with change management processes.\nClone Github Repo The second stage of the workflow cleans up any existing copy of the Github repository and clones the Github repository that contains the Packer template code as well as the Terraform code used for testing the newly created template.\ncleanup_git_repo: action: linux.rm input: target: /stackstorm/orchestration-demos recursive: True sudo: True hosts: localhost next: - when: \u0026lt;% succeeded() %\u0026gt; do: clone_build_repo Once the local copy of the repo is removed then a clone operation is performed.\nclone_build_repo: action: git.clone input: source: https://github.com/martezr/orchestration-demos.git destination: /stackstorm/orchestration-demos hosts: localhost next: - when: \u0026lt;% succeeded() %\u0026gt; do: create_template Build Template The third stage of the workflow builds the VMware vSphere template from an ISO image using HashiCorp Packer and the Jetbrains vSphere Packer plugin.\nThe Packer JSON file is hosted in the cloned GitHub repository and the variable file that contains sensitive information is stored on the filesystem of the StackStorm host. This data could be retrieved in a more secure fashion such as retrieved from HashiCorp Vault or StackStorm\u0026rsquo;s encrypted data storage.\ncreate_template: action: packer.build input: packerfile: /stackstorm/orchestration-demos/packer/centos7base.json variables_file: /stackstorm/packer-vars.json next: - when: \u0026lt;% succeeded() %\u0026gt; do: provision_test_stack Provision Test Stack The fourth stage of the workflow provisions a VMware vSphere virtual machine based on the previously created template and configures it using Puppet via the Terraform Puppet Provisioner.\nIn a real world example this might be an entire application stack owned by another team that consumes the template. This provides tighter integration to help prevent breaking changes from being propogated.\nprovision_test_stack: action: terraform.pipeline input: variable_files: [\u0026#34;/stackstorm/vsphere.auto.tfvars\u0026#34;] plan_path: /stackstorm/orchestration-demos/terraform/teststack backend: {\u0026#34;path\u0026#34;:\u0026#34;/stackstorm/terraform.tfstate\u0026#34;} next: - when: \u0026lt;% succeeded() %\u0026gt; do: output_test_stack_info Validate Test Stack The fifth stage of the workflow is run some validation tests against the stack. These tests might be tests provided by teams that own applications that are built on the base template. In this example StackStorm executes a defined job in Jenkins that runs some ServerSpec tests.\nvalidate_test_stack: action: jenkins.build_job_wait input: project: \u0026#34;validate_teststack\u0026#34; parameters: {\u0026#34;target\u0026#34;:\u0026#34;\u0026lt;% ctx().ip_address %\u0026gt;\u0026#34;} next: - when: \u0026lt;% succeeded() %\u0026gt; do: destroy_test_stack - when: \u0026lt;% failed() %\u0026gt; do: destroy_test_stack The screenshot below displays the console output of the Jenkins job that was run to perform the validation of the test virtual machine provisioned by Terraform.\nDestroy Test Stack The sixth stage of the workflow is to tear down the virtual machine used for testing the updates to the base template. Since Terraform was used to provision the virtual machine we\u0026rsquo;ll just run a Terraform Destroy to tear it down.\ndestroy_test_stack: action: terraform.destroy input: variable_files: [\u0026#34;/stackstorm/vsphere.auto.tfvars\u0026#34;] plan_path: /stackstorm/orchestration-demos/terraform/teststack state_file_path: \u0026#34;/stackstorm/terraform.tfstate\u0026#34; next: - when: \u0026lt;% succeeded() %\u0026gt; do: create_cmdb_record Fetch Template Metadata The seventh stage of the workflow is to fetch the metadata for the template such as the hardware specifications to input them into the CMDB in ServiceNow. The Packer JSON is just being outputted via the cat command but a custom script for parsing file or fetching the data directly from vCenter would provide a more robust solution. The output is being stored in the template_data variable.\nget_template_data: action: core.local input: cmd: cat /stackstorm/orchestration-demos/packer/centos7base.json next: - when: \u0026lt;% succeeded() %\u0026gt; publish: template_data=\u0026lt;% result().stdout.builders[0] %\u0026gt; do: create_cmdb_record Create CMDB Entry The eighth stage of the workflow is creating a record in the CMDB for the new vSphere template. The payload contains metadata about the template that we defined in the previous stage and made available to this stage with the publish declaration.\ncreate_cmdb_record: action: servicenow.create_record input: payload: {\u0026#34;name\u0026#34;:\u0026#34;\u0026lt;% ctx().template_name %\u0026gt;\u0026#34;,\u0026#34;description\u0026#34;:\u0026#34;CentOS 7 Demo Template\u0026#34;,\u0026#34;guest_os\u0026#34;:\u0026#34;\u0026lt;% ctx().template_data.guest_os_type %\u0026gt;\u0026#34;,\u0026#34;cpus\u0026#34;:\u0026#34;\u0026lt;% ctx().template_data.CPUs %\u0026gt;\u0026#34;,\u0026#34;memory\u0026#34;:\u0026#34;\u0026lt;% ctx().template_data.RAM %\u0026gt;\u0026#34;,\u0026#34;disk_size\u0026#34;:\u0026#34;\u0026lt;% ctx().template_data.disk_size %\u0026gt;\u0026#34;,\u0026#34;object_id\u0026#34;:\u0026#34;1\u0026#34;} table: \u0026#34;cmdb_ci_vmware_template\u0026#34; next: - when: \u0026lt;% succeeded() %\u0026gt; do: close_incident The screenshot below displays the CMDB record for the template created by the orchestration.\nClose Incident The nineth and final stage of the workflow is to close or resolve the incident in ServiceNow. This stage closes the loop on the change in ServiceNow with the template successfully being created. In the example we\u0026rsquo;re passing the incident_id variable that was returned from the very first stage in the workflow along with providing the state of \u0026ldquo;6\u0026rdquo; which is resolved and the required fields of code and notes.\nclose_incident: action: servicenow.update input: table: \u0026#34;incident\u0026#34; sysid: \u0026lt;% ctx().incident_id %\u0026gt; payload: {\u0026#34;state\u0026#34;:\u0026#34;6\u0026#34;, \u0026#34;close_code\u0026#34;: \u0026#34;Solved (Permanently)\u0026#34;, \u0026#34;close_notes\u0026#34;: \u0026#34;Created \u0026lt;% ctx().template_name %\u0026gt; template\u0026#34;} The screenshot below displays the incident created by the first stage of the orchestration now in a \u0026ldquo;resolved\u0026rdquo; state due to closing out the incident as part of the orchestration.\nThis example could be extended much further to include additional systems and stages.\nThis example shows just some of the possibilities of creating advanced orchestration with a tool like StackStorm.\n","date":"06 Aug, 2019","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cpicture\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_545x0_resize_q95_h2_box.webp\" media=\"(max-width: 575px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_600x0_resize_q95_h2_box.webp\" media=\"(max-width: 767px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_700x0_resize_q95_h2_box.webp\" media=\"(max-width: 991px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_h2_box.webp\"\u003e\n  \u003cimg loading=\"lazy\" decoding=\"async\" class=\"w-100 img-fluid rounded\" src=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_box.jpg\" alt=\"\" width=\"970\" height=\"500\"\u003e\n\u003c/picture\u003e\n \n \n \n\n","permalink":"https://www.greenreedtech.com/advanced-vmware-vsphere-template-orchestration/","tags":["StackStorm","VMware","DevOps"],"title":"Advanced VMware vSphere Template Orchestration"},{"categories":["Puppet","Terraform"],"contents":"HashiCorp Terraform 0.12.2 added official support for a Puppet provisioner. One caveat is that the provisioner is only available in 0.12.x of Terraform. The provisioner provides a number of features such as adding data to the CSR for trusted facts, selecting between open source and enterprise agent versions along with autosigning the CSR.\nIn this post we\u0026rsquo;ll use the Terraform vSphere provisioner to provision a CentOS virtual machine. The provisioner autosign feature will be used for automatically signing the CSR on the Puppet master.\nWorkstation Setup The first thing we to do is setup our machine to deploy the virtual machine. Terraform and Puppet Bolt are what is needed for the setup.\nTerraform Installation HashiCorp Terraform is a Golang binary that just needs to be downloaded and added to the path on the system. The latest version of HashiCorp Terraform can be downloaded from the following link.\nhttps://www.terraform.io/downloads.html\nPuppet Bolt Installation Puppet Bolt is used by the Puppet provisioner to bootstrap the Puppet agent on the virtual machine being provisioned.\nInstall the Puppet Bolt repo\nAdd the Puppet repo to the system\nsudo rpm -Uvh https://yum.puppet.com/puppet6/puppet6-release-el-7.noarch.rpm Install Puppet Bolt\nInstall Puppet Bolt using yum\nsudo yum install -y puppet-bolt Create Bolt Directory\nCreate a directory for the Puppet Bolt configuration\nmkdir -p ~/.puppetlabs/bolt/ Create Puppet Bolt Configuration File\nThis step creates a Puppet Bolt global configuration file that the Terraform Puppet provisioner uses for the Bolt process.\ncat \u0026lt;\u0026lt; EOF \u0026gt; ~/.puppetlabs/bolt/bolt.yaml modulepath: \u0026#34;~/.puppetlabs/bolt-code/modules:~/.puppetlabs/bolt-code/site-modules\u0026#34; inventoryfile: \u0026#34;~/.puppetlabs/bolt/inventory.yaml\u0026#34; concurrency: 10 format: human ssh: host-key-check: false user: root private-key: ~/.ssh/bolt_id EOF Create Puppet Bolt Inventory File\nThe Terraform Puppet provisioner targets the Puppet master specified in the Terraform code but Bolt needs additional information for connecting to the Puppet master such as user credentials.\ncat \u0026lt;\u0026lt; EOF \u0026gt; ~/.puppetlabs/bolt/inventory.yaml --- config: ssh: host-key-check: false user: root private-key: ~/.ssh/bolt_id EOF Create Bolt Puppetfile\nThis step creates a Bolt Puppetfile for installing the two Puppet modules required by the Terraform Puppet provisioner.\ncat \u0026lt;\u0026lt; EOF \u0026gt; ~/.puppetlabs/bolt/Puppetfile # Modules from the Puppet Forge. mod \u0026#39;danieldreier/autosign\u0026#39; mod \u0026#39;puppetlabs/puppet_agent\u0026#39; EOF Install Bolt Puppet Modules\nThis step installs the Puppet modules specified in the Puppetfile from the previous step.\nbolt puppetfile install Puppet Master Configuration With the workstation configured we now need to configure the Puppet master to support the policy based autosign method supported by the Terraform Puppet provisioner.\nInstall the autosign ruby gem\nThis step installs the autosign (https://github.com/danieldreier/autosign) gem that our Puppet master will use for signing the certificate request.\n/opt/puppetlabs/puppet/bin/gem install autosign Create a new directory for autosign\nCreate a directory for the autosign installation.\nmkdir /var/autosign Update permissions on the autosign directory\nThe permissions for the autosign directory need to be updated to allow the Puppet server service to access the directory.\nchown pe-puppet:pe-puppet /var/autosign chmod 750 /var/autosign Create autosign log file\nCreate a log file for the autosign binary to log events.\ntouch /var/log/autosign.log Update permissions on the autosign log file\nThe permissions on the autosign log file need to be updated to allow it to be written to when the Puppet server attempts to autosign the CSR.\nchown pe-puppet:pe-puppet /var/log/autosign.log Setup the autosign gem\n/opt/puppetlabs/puppet/bin/autosign config setup The autosign setup creates a configuration file \u0026ldquo;/etc/autosign.conf\u0026rdquo; that needs to be modified. We need to ensure that the journalfile is configured for \u0026ldquo;/var/autosign/autosign.journal\u0026rdquo;.\n--- general: loglevel: warn logfile: \u0026#34;/var/log/autosign.log\u0026#34; jwt_token: secret: 1s79TOyqMHw3zfKhE1h+1feKMaE= validity: \u0026#39;7200\u0026#39; journalfile: \u0026#34;/var/autosign/autosign.journal\u0026#34; The CSR will contain an invalid challenge password if the loglevel is set to debug\nWith the loglevel set to debug additional debug lines will be added to the challenge password in the CSR which causes the autosign process to fail.\nSet Puppet master autosigning\nThe Puppet master configuration file needs to be updated to use the autosign gem as the executable/binary to run when validating CSRs.\npuppet config set --section master autosign /opt/puppetlabs/puppet/bin/autosign-validator Restart the Puppet Server Service\nRestart the Puppet server service for the changes to take effect.\nsystemctl restart pe-puppetserver The Puppet master is now configured for autosigning certificate requests from the Puppet provisioner.\nTerraform With all of the setup complete we can focus on the Terraform code for provisioning our CentOS 7 virtual machine and using the new Puppet provisioner to integrate it with our Puppet master.\nThe Puppet provisioner resource block includes a number of configuration options that are covered in greater detail in the documentation but we\u0026rsquo;ll look at the ones that were used in this example.\nserver: The FQDN or IP address of the Puppet master\nserver_user: The name of the user on the Puppet master that the provisioner will connect as\nautosign: The Puppet provisioner includes code that leverages Puppet Bolt as well as policy based autosigning to automatically sign the nodes SSL certificate on the Puppet master\nopen_source: Whether to use the open source version of the Puppet agent or the enterprise version\ncertname: The CN for the agent\u0026rsquo;s SSL certificate\nextension_requests: A map of extension requests to be embedded in the certificate signing request before it is sent to the Puppet master CA. In our example we\u0026rsquo;re passing data to set the pp_role trusted fact that defines the role of the virtual machine.\nconnection: Standard Terraform provisioner connection details for connecting to the virtual machine via SSH or WinRM.\nExample Code The following code is the Terraform vSphere virtual machine resource along with the Puppet provisioner used in this example.\nresource \u0026#34;vsphere_virtual_machine\u0026#34; \u0026#34;puppet-demo\u0026#34; { name = \u0026#34;puppet-demo.grt.local\u0026#34; num_cpus = 1 memory = 4096 resource_pool_id = \u0026#34;${data.vsphere_resource_pool.pool.id}\u0026#34; datastore_id = \u0026#34;${data.vsphere_datastore.datastore.id}\u0026#34; guest_id = \u0026#34;${data.vsphere_virtual_machine.template.guest_id}\u0026#34; scsi_type = \u0026#34;${data.vsphere_virtual_machine.template.scsi_type}\u0026#34; network_interface { network_id = \u0026#34;${data.vsphere_network.network.id}\u0026#34; adapter_type = \u0026#34;${data.vsphere_virtual_machine.template.network_interface_types[0]}\u0026#34; } disk { label = \u0026#34;disk0\u0026#34; size = \u0026#34;60\u0026#34; eagerly_scrub = \u0026#34;${data.vsphere_virtual_machine.template.disks.0.eagerly_scrub}\u0026#34; thin_provisioned = \u0026#34;${data.vsphere_virtual_machine.template.disks.0.thin_provisioned}\u0026#34; } clone { template_uuid = \u0026#34;${data.vsphere_virtual_machine.template.id}\u0026#34; customize { linux_options { host_name = \u0026#34;puppet-demo\u0026#34; domain = \u0026#34;grt.local\u0026#34; } network_interface { ipv4_address = \u0026#34;10.0.0.30\u0026#34; ipv4_netmask = 24 } dns_server_list = [\u0026#34;10.0.0.200\u0026#34;] ipv4_gateway = \u0026#34;10.0.0.1\u0026#34; } } provisioner \u0026#34;puppet\u0026#34; { server = \u0026#34;grtpemaster01.grt.local\u0026#34; server_user = \u0026#34;root\u0026#34; autosign = true open_source = false certname = \u0026#34;puppet-demo.grt.local\u0026#34; extension_requests = { pp_role = \u0026#34;demo\u0026#34; } connection { type = \u0026#34;ssh\u0026#34; host = \u0026#34;${self.default_ip_address}\u0026#34; user = \u0026#34;root\u0026#34; password = \u0026#34;${var.root_password}\u0026#34; } } provisioner \u0026#34;remote-exec\u0026#34; { when = \u0026#34;destroy\u0026#34; inline = [ \u0026#34;puppet node purge puppet-demo.grt.local\u0026#34;, ] connection { type = \u0026#34;ssh\u0026#34; host = \u0026#34;grtpemaster01.grt.local\u0026#34; user = \u0026#34;root\u0026#34; password = \u0026#34;${var.root_password}\u0026#34; } } } Initialize Terraform\nWith the Terraform code in place we need to initialize the current working directory to download all of the necessary plugins.\nterraform init Plan Terraform Run\nNow that the Terraform has been initialized we can plan our Terraform run to see if there are any errors.\nterraform plan Apply Terraform\nThe next step once our plan was successful is to apply our Terraform code and validate that the Puppet provisioner worked.\nterraform apply --auto-approve Once the Terraform apply has completed successfully we should be able to see the new node in the Puppet Enterprise console.\nAdditionally we can see in the facts for the node that the \u0026ldquo;demo\u0026rdquo; role has been set for the pp_role fact we designated in our Terraform code.\nReferences Terraform Puppet Provisioner\nhttps://www.terraform.io/docs/provisioners/puppet.html\nDaniel Dreier Autosign Gem\nhttps://github.com/danieldreier/autosign\n","date":"10 Jul, 2019","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cpicture\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_545x0_resize_q95_h2_box.webp\" media=\"(max-width: 575px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_600x0_resize_q95_h2_box.webp\" media=\"(max-width: 767px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_700x0_resize_q95_h2_box.webp\" media=\"(max-width: 991px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_h2_box.webp\"\u003e\n  \u003cimg loading=\"lazy\" decoding=\"async\" class=\"w-100 img-fluid rounded\" src=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_box.jpg\" alt=\"\" width=\"970\" height=\"500\"\u003e\n\u003c/picture\u003e\n \n \n \n\n","permalink":"https://www.greenreedtech.com/terraform-puppet-provisioner/","tags":["Puppet","Terraform","HashiCorp","DevOps"],"title":"Terraform Puppet Provisioner"},{"categories":["Career"],"contents":"I\u0026rsquo;m now in my second week at Puppet which is best known for its configuration management software that shares the same name as the company. I\u0026rsquo;m extremely excited about this new chapter in my professional life and this blog post covers some of the reasons why. Hopefully this will provide anyone reading this with helpful information as they come to an inflection point in their career. I decided to use a number of \u0026ldquo;P\u0026rdquo; words to describe what this transition is for me.\nPassion The first \u0026ldquo;P\u0026rdquo; is passion. The majority of my career has been focused on client facing consulting. Working for a software vendor is a first for me. While I am still very passionate about helping clients achieve their business objectives through the use of technology, I\u0026rsquo;m really looking forward to accomplishing that through the development of high quality training content. I really love to share the knowledge that I\u0026rsquo;ve gained with others.\nPosition What will I be doing at Puppet? The position I have accepted at Puppet is that of a Principal Field Solutions Developer with the education team. What that means for me is that I get to help with things related to the training and enablement Puppet provides to customers as well as internal Puppet resources. The most amazing part for me is that this includes still being very technical with solution development of the hands on technical training but also the non hands on aspects like documentation and content development.\nPortfolio Puppet is most known for its place within the configuration management space that it occupies with Chef, Ansible and a number of other products. In recent years through acquisition and a number of internally built products Puppet has added other platforms and tools such as Bolt, Continuous Delivery for Puppet Enterprise, and others to its portfolio. Trying to create something out of nothing is challenging but it can also be very rewarding to be part of that process.\nPortland Puppet is headquartered in Portland, Oregon and is where I spent my first week with the company. Since completing my orientation I\u0026rsquo;ve been working with my distributed team in a remote fashion. I truly love the ability to work remotely and with those that understand some of the challenges of a distributed team. While I\u0026rsquo;ve worked remotely for an extended period of time in the past I have a greater awareness of some of the difficulties from a professional and personal standpoint. One of the things the team and the broader organization are big on is video calls to help \u0026ldquo;connect\u0026rdquo; people.\nProfile My background is in IT operations or system administration. My skillset spans numerous technologies from networking to storage to server and desktop virtualization to security and even the gamut of \u0026ldquo;DevOps\u0026rdquo; tools. Most of my career has been spent working with a lot of different technologies on a daily basis. One of the initial thoughts about moving to a vendor or product centric company was potentially limiting future opportunities due to focusing on a narrow set of tools. I honestly believe that it actually give an amazing opportunity to learn aspects of the IT business I would never get otherwise along with a different perspective. Ultimately my most valuable skill is an ability to learn so I feel really comfortable with my decision.\nPuppet The last \u0026ldquo;P\u0026rdquo; is Puppet and more of the reason why I chose to work for Puppet. Puppet was the first tool that really propelled me down the whole \u0026ldquo;DevOps\u0026rdquo; path and is a tool that I\u0026rsquo;ve spent quite a bit of time learning and using. In a strange sort of symbolic way I feel like I\u0026rsquo;m going through a similar journey to Puppet in my professional career. Having had quite a bit of success and now to continue to become better in ways that were previously unexplored.\n","date":"26 Jun, 2019","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cpicture\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_545x0_resize_q95_h2_box.webp\" media=\"(max-width: 575px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_600x0_resize_q95_h2_box.webp\" media=\"(max-width: 767px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_700x0_resize_q95_h2_box.webp\" media=\"(max-width: 991px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_h2_box.webp\"\u003e\n  \u003cimg loading=\"lazy\" decoding=\"async\" class=\"w-100 img-fluid rounded\" src=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_box.jpg\" alt=\"\" width=\"970\" height=\"500\"\u003e\n\u003c/picture\u003e\n \n \n \n\n","permalink":"https://www.greenreedtech.com/the-next-chapter-at-puppet/","tags":["Career"],"title":"The Next Chapter at Puppet"},{"categories":["Vault"],"contents":"I gave a talk for HashiCorp\u0026rsquo;s HashiDays event earlier this year that centered around operational intelligence for HashiCorp Vault. The focus was on harnessing data and turning it into actionable insight to help drive informed decisions. One common insight that\u0026rsquo;s often required but not natively available for various reasons is a mechanism to identify what identities a policy is assigned to within Vault.\nWhile it has some limitations one way to mine such data from Vault is through scripting via the REST API. I was actually able to accomplish this using a python script that can be found at the following URL.\nhttps://github.com/martezr/vault-insights/blob/master/policy-insights.py\nAt a high level the script performs the following tasks:\nRetrieve a list of configured authentication methods (Only userpass and approle are currently supported) Retrieve a list of policies Iterate through the roles or users in the authentication methods and generate a list of assignments to policy Output the results in a JSON format for easy consumption into a tool like Splunk or ELK for dashboard visualization An example of the script output can be found below.\n{ \u0026#34;admins\u0026#34;: { \u0026#34;assignments\u0026#34;: [ \u0026#34;userpass_aboggs1\u0026#34;, \u0026#34;userpass_acortez5\u0026#34;, \u0026#34;userpass_admin\u0026#34;, \u0026#34;userpass_dsmith\u0026#34;, \u0026#34;userpass_dtaylor\u0026#34;, \u0026#34;userpass_edequattro8\u0026#34;, \u0026#34;userpass_efrazier6\u0026#34;, \u0026#34;userpass_epeterson\u0026#34;, \u0026#34;userpass_ering1\u0026#34;, \u0026#34;userpass_whadden\u0026#34; ], \u0026#34;total_assignments\u0026#34;: 10 }, \u0026#34;app-readonly\u0026#34;: { \u0026#34;assignments\u0026#34;: [ \u0026#34;approle_app8\u0026#34;, \u0026#34;approle_app9\u0026#34;, \u0026#34;userpass_appdb1\u0026#34; ], \u0026#34;total_assignments\u0026#34;: 3 }, \u0026#34;default\u0026#34;: { \u0026#34;assignments\u0026#34;: [], \u0026#34;total_assignments\u0026#34;: 0 }, \u0026#34;orphan_assignments\u0026#34;: { \u0026#34;app9\u0026#34;: \u0026#34;newerpolicy\u0026#34; }, \u0026#34;policywriter\u0026#34;: { \u0026#34;assignments\u0026#34;: [ \u0026#34;approle_app1\u0026#34;, \u0026#34;userpass_mreed\u0026#34; ], \u0026#34;total_assignments\u0026#34;: 2 }, \u0026#34;root\u0026#34;: { \u0026#34;assignments\u0026#34;: [], \u0026#34;total_assignments\u0026#34;: 0 }, \u0026#34;testpolicy\u0026#34;: { \u0026#34;assignments\u0026#34;: [ \u0026#34;approle_app4\u0026#34;, \u0026#34;approle_app8\u0026#34;, \u0026#34;approle_hello-world\u0026#34; ], \u0026#34;total_assignments\u0026#34;: 3 }, \u0026#34;vaultreporter\u0026#34;: { \u0026#34;assignments\u0026#34;: [ \u0026#34;userpass_vaultreporter\u0026#34; ], \u0026#34;total_assignments\u0026#34;: 1 } } The ultimate goal of the script is to generate metrics that could be used to help determine what level of access an identity has within Vault. It could also be used when looking to eliminate policy sprawl within a Vault cluster.\n","date":"04 Jun, 2019","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cpicture\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_545x0_resize_q95_h2_box.webp\" media=\"(max-width: 575px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_600x0_resize_q95_h2_box.webp\" media=\"(max-width: 767px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_700x0_resize_q95_h2_box.webp\" media=\"(max-width: 991px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_h2_box.webp\"\u003e\n  \u003cimg loading=\"lazy\" decoding=\"async\" class=\"w-100 img-fluid rounded\" src=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_box.jpg\" alt=\"\" width=\"970\" height=\"500\"\u003e\n\u003c/picture\u003e\n \n \n \n\n","permalink":"https://www.greenreedtech.com/hashicorp-vault-policy-metrics/","tags":["HashiCorp","Vault"],"title":"HashiCorp Vault Policy Metrics"},{"categories":["Vault"],"contents":"HashiCorp Vault is quickly becoming the defacto secrets management platform used in environments that rely on DevOps concepts for application delivery. Vault is incredibly easy and simple to get started with but takes a bit of thought and planning to operationalize it.\nOne of the challenges is ensuring that the installation of your security platform is secure. Chef InSpec is a compliance as code tool that allows us to create profiles that outline a desired security posture. In this post we\u0026rsquo;re looking at an example InSpec profile for Vault that mimics some of the common controls found in industry standards such as CIS benchmarks and DISA STIGs.\nhttps://github.com/martezr/inspec-vault\nRequirements\nThis InSpec profile assumes the following configuration.\nCentOS 7 Vault running as a SystemD service AuditD Download and install Chef InSpec (https://downloads.chef.io/inspec)\nThe following command downloads and installs Chef InSpec.\nrpm -Uvh https://packages.chef.io/files/stable/inspec/2.2.50/el/7/inspec-2.2.50-1.el7.x86_64.rpm Run InSpec Profile\nThe following command runs the Vault InSpec profile against the local machine.\ninspec exec https://github.com/martezr/inspec-vault Profile: HashiCorp Vault InSpec Profile (inspec-vault) Version: 0.0.1 Target: local:// ✔ vault-1.1: Keep Vault up to date ✔ vault_version version should cmp \u0026gt;= \u0026#34;v0.10.1\u0026#34; × vault-1.2: Audit Vault executable × Auditd Rules lines should include \u0026#34;-w /usr/local/bin/vault -p rwxa -k vault\u0026#34; expected [\u0026#34;No rules\u0026#34;] to include \u0026#34;-w /usr/local/bin/vault -p rwxa -k vault\u0026#34; × vault-1.3: Secure Vault configuration files (2 failed) × Directory /opt/vault should not be readable by others expected Directory /opt/vault not to be readable by others ✔ Directory /opt/vault should not be writable by others × Directory /opt/vault should not be executable by others expected Directory /opt/vault not to be executable by others ✔ Directory /opt/vault owner should eq \u0026#34;vault\u0026#34; × vault-1.4: Audit Vault files and directories × Auditd Rules lines should include \u0026#34;-w /opt/vault/ -p rwxa -k vault\u0026#34; expected [\u0026#34;No rules\u0026#34;] to include \u0026#34;-w /opt/vault/ -p rwxa -k vault\u0026#34; × vault-1.5: Audit Vault service configuration × Auditd Rules lines should include \u0026#34;-w /etc/systemd/system/vault.service -p rwxa -k vault\u0026#34; expected [\u0026#34;No rules\u0026#34;] to include \u0026#34;-w /etc/systemd/system/vault.service -p rwxa -k vault\u0026#34; ✔ vault-1.6: Ensure that the vault service is running ✔ Service vault should be installed ✔ Service vault should be enabled ✔ Service vault should be running ✔ vault-1.7: Ensure Vault is not running as root ✔ Processes vault users should not eq [\u0026#34;root\u0026#34;] × vault-1.8: Ensure swap is disabled on the system × Command: `swapon -s | grep -v Filename` exit_status should eq 1 expected: 1 got: 0 (compared using ==) ✔ vault-1.9: Verify that vault.service file permissions are set to 644 or more restrictive ✔ File /etc/systemd/system/vault.service should exist ✔ File /etc/systemd/system/vault.service should be file ✔ File /etc/systemd/system/vault.service should be readable by owner ✔ File /etc/systemd/system/vault.service should be writable by owner ✔ File /etc/systemd/system/vault.service should be readable by group ✔ File /etc/systemd/system/vault.service should not be writable by group ✔ File /etc/systemd/system/vault.service should be readable by other ✔ File /etc/systemd/system/vault.service should not be writable by other ✔ File /etc/systemd/system/vault.service should not be executable Profile Summary: 4 successful controls, 5 control failures, 0 controls skipped Test Summary: 16 successful, 6 failures, 0 skipped Attributes InSpec attributes allow variables to be changed at runtime such as the name of a user or the path of a directory to check. This allows the InSpec profile be flexible enough to accommodate small differences in configurations.\nRunning the InSpec Profile with attributes The example below shows how we can use an attributes file to change some of the things that the InSpec profile looks for.\nThe readme of the InSpec profile lists what attributes are available such as vault_dir or vault_user. We just need to create a yaml file such as \u0026ldquo;attr.yaml\u0026rdquo; and define the desired attributes like the example below.\nExample attributes.yaml file\nvault_user: bob vault_dir: /etc/vault Running the InSpec profile with an attributes file\ninspec exec https://github.com/martezr/inspec-vault --attributes file_path/attributes.yaml This InSpec profile is in the early stages of development and continues to evolve but it provides an example of how InSpec can be used as a tool to shift security left.\nReferences HashiCorp Vault InSpec Profile\nhttps://github.com/martezr/inspec-vault\nInSpec Profiles\nhttps://www.inspec.io/docs/reference/profiles/\n","date":"07 Aug, 2018","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cpicture\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_545x0_resize_q95_h2_box.webp\" media=\"(max-width: 575px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_600x0_resize_q95_h2_box.webp\" media=\"(max-width: 767px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_700x0_resize_q95_h2_box.webp\" media=\"(max-width: 991px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_h2_box.webp\"\u003e\n  \u003cimg loading=\"lazy\" decoding=\"async\" class=\"w-100 img-fluid rounded\" src=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_box.jpg\" alt=\"\" width=\"970\" height=\"500\"\u003e\n\u003c/picture\u003e\n \n \n \n\n","permalink":"https://www.greenreedtech.com/vault-hardening-compliance-using-chef-inspec/","tags":["DevOps","Vault","Security","InSpec"],"title":"Vault Hardening Compliance using Chef InSpec"},{"categories":["cfd3","devops"],"contents":"NetApp was one of the \u0026ldquo;legacy\u0026rdquo; companies that presented a number of compelling solutions at Cloud Field Day #3. The challenge for many similar companies is how do you continue to stay relevant given the pace at which technology moves.\nKubernetes is the defacto container orchestration platform created by Google that is one of the hottest technologies within the IT industry at this time. One of the still developing areas with containers is the management of stateful applications which for many is an anti-pattern for containers which they feel should be stateless. The challenge with stateful applications running in containers is that there must be some sort of persistent storage to allow the state to survive the destruction of the container. This is where a company like NetApp comes into the picture.\nNetApp offers a collection of products that can be leveraged to create a highly scalable, performant and cloud agnostic kubernetes persistent storage solution.\nNetApp Cloud Volumes https://cloud.netapp.com/cloud-volumes\nNetApp Cloud Volumes is a fully managed enterprise class CIFS/NFS storage solution available within the public cloud. Cloud Volumes is based upon NetApp\u0026rsquo;s ONTAP technology that is familiar to those that manage storage in existing on-prem environments.\nNetApp Cloud Volumes is an enterprise class storage solution that includes a number of features unavailable with the native cloud provider storage solutions.\nStorage Replication: Cloud Volumes enables the use of NetApp\u0026rsquo;s SnapMirror replication to replicate data to either an on-prem NetApp or a Cloud Volumes deployment in a different cloud provider. Compression: The underlying NetApp storage compresses data to save on storage costs. Deduplication: The underlying NetApp storage deduplicates common data to save on storage costs. Performance: The performance of the Cloud Volumes solution has been benchmarked to provide consistently faster speeds than cloud provider native storage. NetApp Trident https://github.com/NetApp/trident\nTrident is an open source project created by NetApp that enables NetApp\u0026rsquo;s enterprise storage to be natively leveraged as a persistent volume with Kubernetes.\nThis allows Kubernetes to dynamically provision storage at an application level as well as specify the storage performance to meet the applications I/O requirements. This enables the application to seamlessly move around the Kubernetes cluster.\nThe combination of Trident and Cloud Volumes allows for I/O hungry workloads like databases to continue to be moved into containers and still meet performance requirements.\nThe following video is a snippet of the presentation given at Cloud Field Day #3 about NetApp\u0026rsquo;s cloud services.\nReferences Kubernetes Persistent Volumes\nhttps://kubernetes.io/docs/concepts/storage/persistent-volumes/\nKubernetes Dynamic Volume Provisioning\nhttps://kubernetes.io/docs/concepts/storage/dynamic-provisioning/\nOpenShift and NetApp\nhttps://blog.openshift.com/partner-spotlight-netapp/\n","date":"01 May, 2018","image":"\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://s3.us-west-2.amazonaws.com/greenreedtech.com/netapp_highly_performant_storage_for_cloud_native_apps/netapp_highly_performant_storage_for_cloud_native_apps_1.png\" alt=\"\" class=\"w-100 img-fluid rounded\" height=\"\" width=\"\"\u003e\n","permalink":"https://www.greenreedtech.com/netapp-highly-performant-storage-for-cloud-native-apps/","tags":["cfd3"],"title":"NetApp: Highly Performant Storage for Cloud Native Apps"},{"categories":["cfd3","Cloud Field Day"],"contents":"Earlier this month I had the privilege of being a delegate for Cloud Field Day 3 (http://techfieldday.com/event/cfd3/) which is a tech event put together by GestaltIT where tech companies present to a group of IT professionals.\nOne of the most intriguing presentations of the event was the presentation given by Veritas. The CFD3 presentation was a mixed bag of good and not so good. One of the key takeaways for me was that sometimes the things you forget to highlight are some of the most intriguing to people. This post covers a number of Veritas products that weren\u0026rsquo;t covered during the presentations that really make the case for Veritas as a cloud company.\nCloud Adoption Phases Each of the products within the Veritas product portfolio aren\u0026rsquo;t game changers by themselves but when combined they form a compelling solution that enables public cloud adoption.\nBackup data to the cloud Disaster Recovery/Business Continuity to the cloud Migrate production workloads to the cloud Backup workloads in the cloud that have been migrated Operationalize the cloud investment: identify unstructured data for security and compliance Backup Exec Backup Exec is one of Veritas\u0026rsquo; foundational products that\u0026rsquo;s very familiar to many backup administrators. Support for backing up data to public cloud providers like AWS and Azure are the first sort of foray into the public cloud for their product line. The product also represents the first phase of a cloud migration for many companies by first moving data to the public cloud.\nResiliency Platform Resiliency Platform is a Disaster Recovery/Business Continuity platform that enables IT organizations to simplify DR and BC through the use of public cloud providers and automation.\nCloud Mobility Cloud Mobility is a migration tool for migrating on-premises workloads to the public cloud. The tool supports migrating workloads to AWS as well as Azure. Cloud Mobility also supports migrations between cloud providers to avoid vendor lock-in.\nCloudPoint CloudPoint provides cloud native backups for Azure, AWS and Google Cloud. This represents the next generation of backup capabilities for Veritas that enables them to address backup and recovery in a unified manner across on-premises as well as the cloud.\nInformation Map Information map is a SaaS solution provides a way to easily visualize data stored in various platforms such as AWS S3, OneDrive, NetApp filers and more. In addition to data visualization Information Map enables organizations to drill down to view detailed information about the data being produced which can be used for compliance such as GDPR or to optimize storage costs.\nUltimately the untold story of Veritas the cloud company is an all too common situation for many tech companies looking to reestablish themselves in the cloud era. The challenge is that name recognition is a great tool for sales except when its not.\nReferences Resiliency Platform Datasheet https://www.veritas.com/content/dam/Veritas/docs/data-sheets/V0481_GA_ENT_DS-VRP-3.0-EN.pdf\nCloud Mobility Datasheet https://www.veritas.com/content/dam/Veritas/docs/data-sheets/veritas-cloudmobility-en.pdf\nInformation Map Datasheet https://www.veritas.com/content/dam/Veritas/docs/data-sheets/veritas-information-map-dynamic-perspective-en.pdf\n","date":"18 Apr, 2018","image":"\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://s3.us-west-2.amazonaws.com/greenreedtech.com/veritas_the_untold_story_of_a_cloud_company/information_map.jpg\" alt=\"\" class=\"w-100 img-fluid rounded\" height=\"\" width=\"\"\u003e\n","permalink":"https://www.greenreedtech.com/veritas-the-untold-story-of-a-cloud-company/","tags":["cfd3","Cloud Field Day"],"title":"Veritas: The Untold Story of A Cloud Company"},{"categories":["Vault"],"contents":"Vault (https://www.vaultproject.io/) is a secrets management tool created by HashiCorp that is extremely popular. Given the sensitive nature of the data being stored by a Vault server it is critical that auditing be configured appropriately to provide a record of who accessed sensitive data and when it was accessed. In this blog post we\u0026rsquo;ll walk through configuring a Vault server for auditing and dump the log entries to an AWS S3 bucket for centralized storage.\nEnable Auditing The following is an example of a write event to vault.\nExample Audit entry\n{ \u0026#34;time\u0026#34;: \u0026#34;2018-02-01T14:40:03.226772711Z\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;response\u0026#34;, \u0026#34;auth\u0026#34;: { \u0026#34;client_token\u0026#34;: \u0026#34;hmac-sha256:09b0c08f04bc69bf10a0b8c1d2fa3d84ad4efc470d4f3125950cb5979e606843\u0026#34;, \u0026#34;accessor\u0026#34;: \u0026#34;hmac-sha256:34d5c7474ecac552772fbe091aee99dfc60b6461d42504a117b09928552cfad8\u0026#34;, \u0026#34;display_name\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;policies\u0026#34;: [ \u0026#34;root\u0026#34; ], \u0026#34;metadata\u0026#34;: null, \u0026#34;entity_id\u0026#34;: \u0026#34;\u0026#34; }, \u0026#34;request\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;1af553d4-f2a8-4fac-66ec-b333b392011a\u0026#34;, \u0026#34;operation\u0026#34;: \u0026#34;create\u0026#34;, \u0026#34;client_token\u0026#34;: \u0026#34;hmac-sha256:09b0c08f04bc69bf10a0b8c1d2fa3d84ad4efc470d4f3125950cb5979e606843\u0026#34;, \u0026#34;client_token_accessor\u0026#34;: \u0026#34;hmac-sha256:34d5c7474ecac552772fbe091aee99dfc60b6461d42504a117b09928552cfad8\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;secret/audittest\u0026#34;, \u0026#34;data\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;hmac-sha256:f0dbb2e3574400553d45f259391f17a2c09e7845deb310faf3151ea2527e6c51\u0026#34; }, \u0026#34;policy_override\u0026#34;: false, \u0026#34;remote_address\u0026#34;: \u0026#34;172.28.128.6\u0026#34;, \u0026#34;wrap_ttl\u0026#34;: 0, \u0026#34;headers\u0026#34;: {} }, \u0026#34;response\u0026#34;: {}, \u0026#34;error\u0026#34;: \u0026#34;\u0026#34; } Security Considerations The audit log entry contains the value of the secret in a hashed format but can be deciphered with the appropriate access to the vault server. The log files should be encrypted and access to the log files restricted.\nThe audit logs contain the full request and response objects for every interaction with Vault. The request and response can be matched utilizing a unique identifier assigned to each request. The data in the request and the data in the response (including secrets and authentication tokens) will be hashed with a salt using HMAC-SHA256.\nThe purpose of the hash is so that secrets aren\u0026rsquo;t in plaintext within your audit logs. However, you\u0026rsquo;re still able to check the value of secrets by generating HMACs yourself; this can be done with the audit device\u0026rsquo;s hash function and salt by using the /sys/audit-hash API endpoint (see the documentation for more details). Currently Vault supports three auditing methods\nAudit Methods Vault supports the following three methods for writing audit entries.\nSocket Syslog File Socket The socket audit device writes to a TCP, UDP, or UNIX socket. Due the warning in the HashiCorp documentation below we are avoiding this method for a more reliable method as the loss of a single audit entry is not desired.\nWarning: Due to the nature of the underlying protocols used in this device there exists a case when the connection to a socket is lost a single audit entry could be omitted from the logs and the request will still succeed. Using this device in conjunction with another audit device will help to improve accuracy, but the socket device should not be used if strong guarantees are needed for audit logs.\nSyslog Syslog is the de-facto standard for logging and is partially supported by Vault as the method is limited to sending only to local syslog and not a remote destination.\nFile Given the limitations of the other two methods the file audit method is the ideal audit method. To centralize the logging we\u0026rsquo;ll use Fluentd to dump the logs to an S3 bucket.\nvault audit enable file file_path=/var/log/vault_audit.log Installing Fluentd Increase Max number of File Descriptors\nPlease add following lines to your /etc/security/limits.conf file and reboot your machine.\nroot soft nofile 65536 root hard nofile 65536 * soft nofile 65536 * hard nofile 65536 Install Fluentd\ncurl -L https://toolbelt.treasuredata.com/sh/install-redhat-td-agent3.sh | sh Enable Fluentd to start on boot\nsudo systemctl enable td-agent.service Start the Fluentd service\nsudo systemctl start td-agent.service With the Fluentd agent installed and the service started we now need to create an entry for our source which is the vault audit log file and a destination which will be our S3 bucket for persistently storing the log entries.\nFluentd Configuration\nWe need to write following input and output stanzas to the Fluentd agent configuration file (/etc/td-agent/td-agent.conf).\nInput\n\u0026lt;source\u0026gt; @type tail path /var/log/vault_audit.log pos_file /var/log/td-agent/vault.audit_log.pos \u0026lt;parse\u0026gt; @type json \u0026lt;/parse\u0026gt; tag s3.vault.audit \u0026lt;/source\u0026gt; S3 Output\n\u0026lt;match s3.*.*\u0026gt; @type s3 aws_key_id YOUR_AWS_KEY_ID aws_sec_key YOUR_AWS_SECRET/KEY s3_bucket YOUR_S3_BUCKET_NAME path logs/ \u0026lt;buffer\u0026gt; @type file path /var/log/td-agent/s3 timekey_wait 1m chunk_limit_size 256m \u0026lt;/buffer\u0026gt; time_slice_format %Y%m%d%H%M \u0026lt;/match\u0026gt; Validating Auditing Now that we\u0026rsquo;ve gotten everything configured we just need to test it to make sure everything is working properly. To do this we need to perform some vault action like writing a secret or reading a secret to trigger the creation of an audit entry.\nvault write secret/audittest value=testingvaultauditing Once the command is run we should see a new entry created in the /var/log/vault_audit.log file and after a few minutes we should see a folder created in our S3 bucket for storing our Vault audit logs.\nReferences Installing Fluentd\nhttps://docs.fluentd.org/v1.0/articles/before-install\nHashiCorp Vault Auditing\nhttps://www.vaultproject.io/docs/audit/index.html\n","date":"01 Feb, 2018","image":"\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://s3.us-west-2.amazonaws.com/greenreedtech.com/vault_audit_logging/vault_audit_logging_1.png\" alt=\"\" class=\"w-100 img-fluid rounded\" height=\"\" width=\"\"\u003e\n","permalink":"https://www.greenreedtech.com/vault-audit-logging/","tags":["HashiCorp","Vault","DevOps","Security"],"title":"Vault Audit Logging"},{"categories":["terraform"],"contents":"A very popular Terraform state management configuration is to utilize AWS S3 for state management and AWS DynamoDB for state locking. The problem is that there does not appear to be a publicly available document that details the minimum privileges required by an AWS user or role to leverage AWS S3 and DynamoDB for Terraform state management.\nIdeally the concept of least privilege would be used when assigning permissions to AWS users/roles. The permissions discussed below are for administrators or users that utilize the S3 bucket and DynamoDB table and are not responsible for managing those resources.\nS3 Bucket S3 access should be restricted to the specific bucket that the user/role is using for storing state files.\nListBucket: List the S3 bucket GetObject: Read access to the Terraform state files PutObject: Write access to the Terraform state files DeleteObject: Delete an existing Terraform state file DynamoDB DynamoDB access should be restricted to the specific table that the user/role is using for state locking.\nGetItem: Read an existing Terraform state lock PutItem: Write a new Terraform state lock DeleteItem: Delete an existing Terraform state lock The example IAM policy below utilizes the permissions discussed in the previous section.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;TerraformStateLocking\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;dynamodb:GetItem\u0026#34;, \u0026#34;dynamodb:PutItem\u0026#34;, \u0026#34;dynamodb:DeleteItem\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:dynamodb:::table/${dynamodb-table}\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;AllowTerraformStateBucketAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:ListBucket\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::${s3-bucket}\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;AllowTerraformStateFileAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::${s3-bucket}/*\u0026#34; } ] } The policy could be restricted even further by adding the specific account number and AWS region to the \u0026ldquo;resource\u0026rdquo; property.\nReferences Terraform S3 Backend Configuration\nhttps://www.terraform.io/docs/backends/types/s3.html\nAWS S3 IAM Permissions\nhttp://docs.aws.amazon.com/AmazonS3/latest/dev/using-with-s3-actions.html\nAWS DynamoDB IAM Permissions\nhttp://docs.aws.amazon.com/amazondynamodb/latest/developerguide/api-permissions-reference.html\n","date":"30 Oct, 2017","image":"\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://s3.us-west-2.amazonaws.com/greenreedtech.com/vault_audit_logging/vault_audit_logging_1.png\" alt=\"\" class=\"w-100 img-fluid rounded\" height=\"\" width=\"\"\u003e\n","permalink":"https://www.greenreedtech.com/terraform-aws-s3-state-management-least-privilege/","tags":["aws","devops","terraform"],"title":"Terraform AWS S3 State Management Least Privilege"},{"categories":["Terraform"],"contents":"What is immutable infrastructure? Immutable infrastructure is the concept of utilizing an infrastructure component in an ephemeral manner. This means that the component can be destroyed and recreated at will without major impact.\nAdvantages\nTroublesome instances can easily be destroyed and recreated. System patching processes are replaced by just provisioning instances from a new template. Terraform\nTerraform provides us with the ability to create vSphere infrastructure with code. Terraform enables us to quickly tear down and provision new infrastructure which allows us to quickly transition all of our VMs to a new template within a maintenance window.\nvSphere Immutable Infrastructure Design Our example application is a node.js application with a Mongodb backend.\nWeb Server: Deployment of our web instance is fairly simple as the web server stores no data that needs to persists across iterations of the instance.\nDatabase Server: Deploying an immutable database server is particularly challenging given the requirement that the data in the database must persist across iterations of the instance.\nData persistence is achieved by decoupling the data drive or .VMDK file from the instantiation of the virtual machine. The virtual machine is created and attaches an existing hard disk that stores the data for our Mongodb database. When we destroy the database VM the .VMDK file is detached and the virtual machine is destroyed.\nFolder Layout The design uses three directories to store the Terraform code to allow us to abstract the code that manages the data disk for our database instance from the database instance.\ndbinstantiation: The code in this directory manages the database instance we\u0026rsquo;ll use to prep our data disk. instances: The code in this directory manages the web and database instances. persistentdisks: The code in this directory manages the data disk for the database instance. vsphereimmutable/ ├── dbinstantiation │ ├── main.tf │ └── mongodbinstall.sh ├── instances │ ├── main.tf │ ├── mongodbinstall.sh │ └── nodeinstall.sh └── persistentdisks └── main.tf Terraform Code Now that we\u0026rsquo;ve walked through the immutable design we\u0026rsquo;ll jump into the Terraform code and helper scripts to build out the design in our vSphere environment.\nAll code used in this example can be found on Github.\nhttps://github.com/martezr/terraform-immutable-vsphere\nDatabase Hard Disk The second hard drive or data drive for our database instance is the first resource we\u0026rsquo;ll need to create using Terraform. The .VMDK file will be stored in the root of the datastore in this example but can easily be placed into a subfolder for persistent disks.\nresource \u0026#34;vsphere_virtual_disk\u0026#34; \u0026#34;DBDisk01\u0026#34; { size = 20 vmdk_path = \u0026#34;DBDisk01.vmdk\u0026#34; datacenter = \u0026#34;Datacenter\u0026#34; datastore = \u0026#34;local\u0026#34; type = \u0026#34;thin\u0026#34; adapter_type = \u0026#34;lsiLogic\u0026#34; } Let\u0026rsquo;s go ahead and run Terraform apply to create the data drive.\nTerraform apply Database Instantiation Instance The next thing we need to do is create the instance to prep the data drive for use with our immutable database instance.\nFirst database boot script We need to partition our data drive separate of the database bootstrap script as we only want to partition the data drive only once.\n#!/bib/bash # Partition the second disk ( echo n # Add a new partition echo p # Primary partition echo 1 # Partition number echo # First sector (Accept default: 1) echo # Last sector (Accept default: varies) echo w # Write changes ) | sudo fdisk /dev/sdb echo yes | mkfs.ext4 /dev/sdb mkdir /mongodb echo \u0026#39;/dev/sdb /mongodb ext4 defaults 0 0\u0026#39; \u0026gt;\u0026gt; /etc/fstab mount -a cat \u0026lt;\u0026lt; EOF \u0026gt; /etc/yum.repos.d/mongodb-org.repo [mongodb-org-3.4] name=MongoDB Repository baseurl=https://repo.mongodb.org/yum/redhat/7Server/mongodb-org/3.4/x86_64/ gpgcheck=1 enabled=1 gpgkey=https://www.mongodb.org/static/pgp/server-3.4.asc EOF yum -y install mongodb-org cat \u0026lt;\u0026lt; EOF \u0026gt; /etc/mongod.conf # where to write logging data. systemLog: destination: file logAppend: true path: /var/log/mongodb/mongod.log # Where and how to store data. storage: dbPath: /mongo journal: enabled: true # engine: # mmapv1: # wiredTiger: # how the process runs processManagement: fork: true # fork and run in background pidFilePath: /var/run/mongodb/mongod.pid # location of pidfile # network interfaces net: port: 27017 # bindIp: 127.0.0.1 # Listen to local interface only, comment to listen on all interfaces. EOF systemctl start mongod Now we need to run Terraform apply to create the instance.\nTerraform apply Once our instance has been successfully bootstrapped using the mongodbprep script we\u0026rsquo;ll need to run the Terraform destroy command to destroy our instantiation instance.\nTerraform destroy Production Instances Now that the data drive for our Mongodb VM has been prepped we just need to provision our production instances and start adding data.\nresource \u0026#34;vsphere_virtual_machine\u0026#34; \u0026#34;Node01\u0026#34; { name = \u0026#34;terraform-node\u0026#34; vcpu = 2 memory = 4096 network_interface { label = \u0026#34;VM Network\u0026#34; ipv4_address = \u0026#34;192.168.1.4\u0026#34; ipv4_prefix_length = \u0026#34;24\u0026#34; ipv4_gateway = \u0026#34;192.168.1.254\u0026#34; } provisioner \u0026#34;file\u0026#34; { source = \u0026#34;nodeinstall.sh\u0026#34; destination = \u0026#34;nodeinstall.sh\u0026#34; connection { type = \u0026#34;ssh\u0026#34; host = \u0026#34;192.168.1.4\u0026#34; user = \u0026#34;root\u0026#34; password = \u0026#34;password\u0026#34; } } provisioner \u0026#34;remote-exec\u0026#34; { connection { type = \u0026#34;ssh\u0026#34; host = \u0026#34;192.168.1.4\u0026#34; user = \u0026#34;root\u0026#34; password = \u0026#34;password\u0026#34; } inline = [ \u0026#34;setenforce 0\u0026#34;, \u0026#34;chmod +x /root/nodeinstall.sh \u0026amp;\u0026amp; sh /root/nodeinstall.sh\u0026#34; ] } disk { template = \u0026#34;centosnodetemp\u0026#34; type = \u0026#34;thin\u0026#34; datastore = \u0026#34;Local_Storage\u0026#34; } } resource \u0026#34;vsphere_virtual_machine\u0026#34; \u0026#34;DB01\u0026#34; { name = \u0026#34;terraform-db\u0026#34; vcpu = 2 memory = 4096 detach_unknown_disks_on_delete = \u0026#34;true\u0026#34; enable_disk_uuid = \u0026#34;true\u0026#34; network_interface { label = \u0026#34;VM Network\u0026#34; ipv4_address = \u0026#34;192.168.1.5\u0026#34; ipv4_prefix_length = \u0026#34;24\u0026#34; ipv4_gateway = \u0026#34;192.168.1.254\u0026#34; } provisioner \u0026#34;file\u0026#34; { source = \u0026#34;mongodbinstall.sh\u0026#34; destination = \u0026#34;mongodbinstall.sh\u0026#34; connection { type = \u0026#34;ssh\u0026#34; host = \u0026#34;192.168.1.5\u0026#34; user = \u0026#34;root\u0026#34; password = \u0026#34;password\u0026#34; } } provisioner \u0026#34;remote-exec\u0026#34; { connection { type = \u0026#34;ssh\u0026#34; host = \u0026#34;192.168.1.5\u0026#34; user = \u0026#34;root\u0026#34; password = \u0026#34;password\u0026#34; } inline = [ \u0026#34;mkdir /mongodb\u0026#34;, \u0026#34;echo \u0026#39;/dev/sdb /mongodb ext4 defaults 0 0\u0026#39; \u0026gt;\u0026gt; /etc/fstab\u0026#34;, \u0026#34;mount -a\u0026#34;, \u0026#34;chmod +x /root/mongodbinstall.sh \u0026amp;\u0026amp; sh /root/mongodbinstall.sh\u0026#34; ] } disk { template = \u0026#34;centos7temp\u0026#34; type = \u0026#34;thin\u0026#34; } disk { vmdk = \u0026#34;DBDisk01.vmdk\u0026#34; datastore = \u0026#34;Local_Storage\u0026#34; type = \u0026#34;thin\u0026#34; keep_on_remove = \u0026#34;true\u0026#34; } } Let\u0026rsquo;s go ahead and run a Terraform apply to build our production instances.\nWith the node.js app started we should be able to access the example node.js app from our web browser.\nhttp://web_instance:8080\nLet\u0026rsquo;s add some todo items to the list so we can validate that the data persists across instantiations of the database virtual machine.\nAdd a few tasks to the list, in our example a few whimsical todo items have been added to the list.\nDatatbase can be immutable too Stateful apps don\u0026rsquo;t have to be pets The data is all that needs to persist Immutability Test The last thing we need to do now is to actually test the immutability of our design by destroying and rebuilding the infrastructure. From within the \u0026ldquo;instances\u0026rdquo; directory we need to run the terraform destroy command to destroy the node.js and database virtual machines.\nTerraform destroy Once both of our VMs are destroyed we need recreate them using terraform apply and validate that the todo items are still available in our app.\nTerraform apply If everything works as expected then we should see the todo items we created earlier still in the list. This indicates that we were able to destroy and recreate both the web and database instances without losing any data.\nThis solution enables us to decouple the underlying virtual machines from the business value provided by our application and allows us to take advantage of immutable infrastructure.\nReferences Terraform Provisioner Connection https://www.terraform.io/docs/provisioners/connection.html\nTerraform vSphere Provider https://www.terraform.io/docs/providers/vsphere/index.html\nNode.js Todo Example App https://scotch.io/tutorials/creating-a-single-page-todo-app-with-node-and-angular\nGithub example code https://github.com/martezr/terraform-immutable-vsphere\n","date":"13 Oct, 2017","image":"\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://s3.us-west-2.amazonaws.com/greenreedtech.com/vsphere_immutable_infrastructure_with_terraform/vsphere_immutable_infrastructure_with_terraform_1.png\" alt=\"\" class=\"w-100 img-fluid rounded\" height=\"\" width=\"\"\u003e\n","permalink":"https://www.greenreedtech.com/vsphere-immutable-infrastructure-with-terraform/","tags":["DevOps","Terraform","VMware"],"title":"vSphere Immutable Infrastructure with Terraform"},{"categories":["VMware","vRO"],"contents":"In a previous post we walked through configuring a PostgreSQL database server as the external database for our vRealize Orchestrator (vRO) cluster. In this post we\u0026rsquo;ll cover adding SSL support to encrypt the traffic between our vReazlie Orchestrator cluster and our PostgreSQL database server.\nPostgreSQL SSL Configuration Generate CSR The first thing we need to do is get our SSL certificate for the database server. In this case we\u0026rsquo;re going to use openssl to generate our certificate request and ultimately we\u0026rsquo;ll use openssl self-sign the certificate.\nopenssl req -newkey rsa:4096 -nodes -keyout vrodb.key -out vrodb.csr Provide the necessary information to the prompts.\nGenerating a 4096 bit RSA private key .............................................................++ ..................................................................................++ writing new private key to \u0026#39;vrodb.key\u0026#39; ----- You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter \u0026#39;.\u0026#39;, the field will be left blank. ----- Country Name (2 letter code) [XX]:US State or Province Name (full name) []:IL Locality Name (eg, city) [Default City]:Chicago Organization Name (eg, company) [Default Company Ltd]:Green Reed Technology Organizational Unit Name (eg, section) []:IT Common Name (eg, your name or your server\u0026#39;s hostname) []:grtdb01.grt.local Email Address []:admin@grt.local Please enter the following \u0026#39;extra\u0026#39; attributes to be sent with your certificate request A challenge password []: An optional company name []:Green Reed Technology Sign Certificate Request We\u0026rsquo;ll sign the certificate request with openssl since we\u0026rsquo;re using a self-signed certificate but the CSR could be signed by a trusted CA as well.\nopenssl x509 -signkey vrodb.key -in vrodb.csr -req -days 365 -out vrodb.crt Signature ok subject=/C=US/ST=IL/L=Chicago/O=Green Reed Technology/OU=IT/CN=grtdb01.grt.local/emailAddress=admin@grt.local Getting Private key Modify Permissions With the private key and certificate created we\u0026rsquo;ll secure the file permissions.\nchmod 400 vrodb.key chown postgres:postgres vrodb.key chown postgres:postgres vrodb.crt Configure SSL Support The following change must be made to the /var/lib/pgsql/9.6/data/postgresql.conf config file to enable SSL support.\nssl = on ssl_cert_file = \u0026#39;vrodb.crt\u0026#39; ssl_key_file = \u0026#39;vrodb.key\u0026#39; ssl_ca_file = \u0026#39;vrodb.crt\u0026#39; Restart the PostgreSQL Service We need to restart the PostgreSQL service to apply the changes we\u0026rsquo;ve made.\nsystemctl restart postgresql-9.6 vRO SSL Configuration With our database server configured to support SSL we just need to configure our vRealize Orchestrator (vRO) cluster to use SSL to connect to the database.\nThe first thing we need to do is import the SSL certificate for our database server into vRealize Orchestrator. From the vRealize Orchestrator (vRO) control center click \u0026ldquo;Certificates\u0026rdquo; to manage the vRealize Orchestrator (vRO) certificates.\nClick \u0026ldquo;IMPORT\u0026rdquo; and select \u0026ldquo;Import from a PEM-encoded file\u0026rdquo;.\nClick \u0026ldquo;BROWSE\u0026rdquo; and select the SSL certificate that our database server utilizes. The SSL certificate may need to be copied from the database server is not present on the local machine.\nClick \u0026ldquo;IMPORT\u0026rdquo; to import the SSL certificate into the keystore.\nVerify the SSL certificate and click \u0026ldquo;IMPORT\u0026rdquo;.\nOnce the import process has been completed the SSL certificate should show as a trusted SSL certificate.\nRepeat the SSL certificate import process on all additional vRO appliances in the cluster.\nWith the SSL certificate of our database imported we need to configure our connection to use SSL. Click on \u0026ldquo;Database\u0026rdquo; from the Control Center home page to configure the database.\nToggle the \u0026ldquo;Use SSL\u0026rdquo; slider, select \u0026ldquo;require\u0026rdquo; for the SSL mode and click \u0026ldquo;SAVE\u0026rdquo; to configure SSL support.\nIf the connection was established successfully we should see a \u0026ldquo;Configuration saved\u0026rdquo; message.\nUnfortunately even though we just configured SSL support the UI doesn\u0026rsquo;t update properly so it seems like we don\u0026rsquo;t have SSL configured if we go back to the database configuration screen.\nOne way to validate the SSL configuration is to log into one of the vRO appliances and run the following command.\ncat /var/lib/vco/app-server/conf/vmo.properties | grep sslmode The command output should be similar to that below if SSL is configured and no output if it\u0026rsquo;s not.\ndatabase.url=jdbc:postgresql://grtdb01.grt.local:5432/vrodatabase?sslmode=require References PostgreSQL SSL Configuration\nhttps://www.postgresql.org/docs/9.6/static/ssl-tcp.html\nvRealize Orchestrator Database Configuration\nhttps://docs.vmware.com/en/vRealize-Orchestrator/7.3/com.vmware.vrealize.orchestrator-install-config.doc/GUID-DB3E9B1D-59B4-40A6-8A82-E7D1605697C7.html\n","date":"30 Jun, 2017","image":"\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://s3.us-west-2.amazonaws.com/greenreedtech.com/vrealize_orchestrator_vro_and_postgresql_database_ssl/vRO_Cluster_SSL_7.png\" alt=\"\" class=\"w-100 img-fluid rounded\" height=\"\" width=\"\"\u003e\n","permalink":"https://www.greenreedtech.com/vrealize-orchestrator-vro-and-postgresql-database-ssl/","tags":["vRO","VMware"],"title":"vRealize Orchestrator (vRO) and PostgreSQL Database SSL"},{"categories":["VMware","vRO"],"contents":"In this post we\u0026rsquo;re going to build out a two node vRealize Orchestrator cluster with a PostgreSQL database. VMware is deprecating support for Microsoft SQL and Oracle databases as the vRO external database in favor of PostgreSQL.\nhttps://docs.vmware.com/en/vRealize-Orchestrator/7.3/rn/vrealize-orchestrator-73-release-notes.html\nPostgreSQL database installation and configuration The first thing we need to do is install and configure PostgreSQL on a linux server. The following steps will how to install and configure PostgreSQL 9.6.3 on a CentOS 7 machine.\nInstall PostgreSQL Repo\nyum install -y https://download.postgresql.org/pub/repos/yum/9.6/redhat/rhel-7-x86_64/pgdg-centos96-9.6-3.noarch.rpm Install PostgreSQL server\nyum install -y postgresql96-server Initialize the database\n/usr/pgsql-9.6/bin/postgresql96-setup initdb Enable the PostgreSQL service to start on boot\nsystemctl enable postgresql-9.6 Enable remote connections\nThe following change must be made to the /var/lib/pgsql/9.6/data/postgresql.conf config file to allow remote connections.\nlisten_address \u0026#39;localhost\u0026#39; to\nlisten_address \u0026#39;*\u0026#39; Authorize remote connections\nThe following entry must be added to the /var/lib/pgsql/9.6/data/pg_hba.conf to authorize remote servers to connect to the database.\nThe following three properties of each line should be changed.\ndatabase_name\ndatabase_user\nvro_appliance_1/2_IP\nhost database_name database_user vro_appliance_1_IP/32 trust\nhost database_name database_user vro_appliance_2_IP/32 trust\nStart the PostgreSQL service\nsystemctl start postgresql-9.6 Create vRO database user Now that we\u0026rsquo;ve got PostgreSQL installed and running we need to create a user for vRO to access the database.\nChange to the postgres superuser to create the vRO user\nsu - postgres Create the vRO database user\nProvide a password when prompted.\ncreateuser vrodbuser --pwprompt Create vRO database The last thing we need to do is create the database that vRO will utilize.\ncreatedb -O vrodbuser vrodatabase Logoff of the postgres account\nexit vRO configuration With the database configured we can now start the vRO configuration. The first step is to deploy two vRO 7.3 appliances to vSphere and then log into the control center of the first appliance (https://ip_address:8283/vco-controlcenter).\nSelect \u0026ldquo;Standalone Orchestrator\u0026rdquo; from the \u0026ldquo;Select deployment type:\u0026rdquo; drop-down list even though we\u0026rsquo;re configuring a cluster. Provide the hostname or IP address of the load balancer VIP to allow requests to be load balanced or leave the address of the appliance if a load balancer will not be used.\nSelect an authentication mode of either vSphere or vRealize Automation. We\u0026rsquo;ve selected vSphere authentication in this case since we\u0026rsquo;re not deploying vRealize Automation. Enter the hostname of the PSC in the Host address field and click \u0026ldquo;CONNECT\u0026rdquo;.\nClick \u0026ldquo;ACCEPT CERTIFICATE\u0026rdquo; to trust the SSL certificate for the PSC.\nEnter vSphere credentials as well as the default tenant and click \u0026ldquo;REGISTER\u0026rdquo; to register the vRO appliance with the vSphere SSO domain.\nDesignate a vSphere SSO group as the admin group for vRO appliance.\nClick \u0026ldquo;SAVE CHANGES\u0026rdquo; after an admin group has been selected.\nAfter completing the initial setup the service needs to be restarted to apply the changes we just made. Click \u0026ldquo;Startup Options\u0026rdquo; to access the restart action.\nWe should see a message indicating that a server restart is required to apply our configuration changes. Click the \u0026ldquo;RESTART\u0026rdquo; button to restart the vco-server service.\nOnce the service has restarted when see a current status of \u0026ldquo;RUNNING\u0026rdquo; and the active and pending configuration fingerprints matches.\nNow we need to configure vRO to use our external PostgreSQL database server. Click \u0026ldquo;Configure Database\u0026rdquo; to access the database configuration.\nEnter the hostname/IP address and port number for the PostgreSQL database (The default port is 5432). Enter the database name and credentials, then click \u0026ldquo;SAVE\u0026rdquo; to apply the changes.\nIf the connection was successful we should get a \u0026ldquo;Configuration saved\u0026rdquo; message indicating that the vRO appliance was able to establish a connection to the PostgreSQL database.\nAdd Second vRO Appliance Now we\u0026rsquo;ll add our second vRO appliance and create our cluster. Select \u0026ldquo;Clustered Orchestrator\u0026rdquo; from the \u0026ldquo;Select deployment type:\u0026rdquo; drop-down list. Enter the hostname/IP of the first vRO appliance along with the VAMI credentials for the appliance and click \u0026ldquo;JOIN\u0026rdquo; to create the vRO cluster.\nFollowing the join cluster operation we need to restart the vco-server service on our second vRO appliance. Click \u0026ldquo;Startup Options\u0026rdquo; to access the restart action.\nWe should see a message indicating that a server restart is required to apply our configuration changes. Click the \u0026ldquo;RESTART\u0026rdquo; button to restart the vco-server service.\nOnce the service has restarted when see a current status of \u0026ldquo;RUNNING\u0026rdquo; and the active and pending configuration fingerprints matche.\nLet\u0026rsquo;s view the current status of our cluster by clicking on \u0026ldquo;Orchestrator Cluster Management\u0026rdquo; from the main page of the control center.\nOnce the vco-server service on our second vRO appliance has been successfully restarted we should see that both nodes are synchronized and in a \u0026ldquo;RUNNING\u0026rdquo; state.\nReferences Install PostgreSQL\nhttps://www.postgresql.org/download/linux/redhat/\nCreate PostgreSQL user\nhttps://www.a2hosting.com/kb/developer-corner/postgresql/managing-postgresql-databases-and-users-from-the-command-line\nPostgreSQL Authorization\nhttp://www.thegeekstuff.com/2014/02/enable-remote-postgresql-connection/?utm_source=tuicool\nvRealize Orchestrator 7.3 Installation and Configuration\nhttp://pubs.vmware.com/orchestrator-73/index.jsp#com.vmware.vrealize.orchestrator-install-config.doc/GUID-BEB98B76-354A-438D-8BA0-53DE8E2F1DE2.html\nvRealize Orchestrator 7.3 Release Notes\nhttp://pubs.vmware.com/Release_Notes/en/orchestrator/vrealize-orchestrator-73-release-notes.html\n","date":"26 Jun, 2017","image":"\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://s3.us-west-2.amazonaws.com/greenreedtech.com/vrealize_orchestrator_vro_cluster_with_a_postgresql_database/vROCluster_14.png\" alt=\"\" class=\"w-100 img-fluid rounded\" height=\"\" width=\"\"\u003e\n","permalink":"https://www.greenreedtech.com/vrealize-orchestrator-vro-cluster-with-a-postgresql-database/","tags":["vRO","VMware"],"title":"vRealize Orchestrator (vRO) Cluster with a PostgreSQL Database"},{"categories":["VMware","vRA"],"contents":"In this post we\u0026rsquo;re going to walk through how to dynamically populate a vRA request field using values retrieved from a REST API using vRO.\nA common scenario when working with any software is wanting to test against multiple versions of the software.\nIn this example we\u0026rsquo;re going to dynamically fetch the software version of StackStorm (https://stackstorm.com/) from github so that we can select which version to install from the vRA request form.\nvRO Action The first thing we need to do is create our action in vRealize Orchestrator that will dynamically fetch the tags from the StackStorm github repository. Below is the code used for the action, the steps to create an action can be found in a previous post (http://www.greenreedtech.com/vro-building-a-dynamic-drop-down-for-vsphere-tags/).\nFrom a high level the tasks being performed by the action are as follows.\nTypically a REST Host and REST Operation would be created but I wanted to try and do it all in code.\nImport the http://api.github.com SSL certification into the vRO truststore Create a transient REST host Submit a GET request to https://api.github.com/repos/StackStorm/st2/tags for the repo tags Iterate over the JSON to return just the name field and add it to an array Return the array of tags // vRO action to retrieve StackStorm Github repository tags url = \u0026#39;https://api.github.com\u0026#39;; // Import SSL certificate var ld = Config.getKeystores().getImportCAFromUrlAction(); var model = ld.getModel(); model.value = url; error = ld.execute(); // Retrieve StackStorm repository tags var uri = \u0026#34;https://api.github.com/repos/StackStorm/st2/tags\u0026#34;; var method = \u0026#34;GET\u0026#34;; var body = \u0026#34;\u0026#34;; var httpRestHost = null; // Create a dynamic REST host: var restHost = RESTHostManager.createHost(\u0026#34;dynamicRequest\u0026#34;); restHost.operationTimeout = 900; httpRestHost = RESTHostManager.createTransientHostFrom(restHost); httpRestHost.operationTimeout = 900; // Remove the endpoint from the URI: var urlEndpointSplit = uri.split(\u0026#34;/\u0026#34;); var urlEndpoint = urlEndpointSplit[urlEndpointSplit.length - 1]; uri = uri.split(urlEndpoint)[0]; httpRestHost.url = uri; httpRestHost.hostVerification = false; // REST client only accepts method in all UPPER CASE: method = method.toUpperCase(); var request = httpRestHost.createRequest(method, urlEndpoint, body); request.contentType = \u0026#34;application/json\u0026#34;; var response = request.execute(); jsonResponse = response.contentAsString var githubTags = []; tags = JSON.parse(jsonResponse) for each (tag in tags){ githubTags.push(tag.name); } // Return array of tags return githubTags; vRA Property Definition The next task is to create a property definition in vRealize Automation that utilizes the vRO action we just created to present the requestor with a dynamic drop-down list.\nSelect the \u0026ldquo;External Value\u0026rdquo; radio button next to \u0026ldquo;Values:\u0026rdquo; and click the \u0026ldquo;Select\u0026hellip;\u0026rdquo; button next to \u0026ldquo;Script action:\u0026rdquo; to assign the vRO action.\nSelect the vRO action we created in the previous step and click \u0026ldquo;OK\u0026rdquo; to select the vRO action.\nThis step assumes that we have a blueprint already created to add our property definition to.\nWe now need to assign the property we just created to the machine vSphere machine object in our StackStorm blueprint.\nWith the blueprint published and properly entitled we can now verify that everything is working by requesting the StackStorm catalog item. If everything worked we should get a functioning drop down list of tags for the \u0026ldquo;Tag\u0026rdquo; field.\nWe can select one of the tags from the drop-down to select the particular version of StackStorm that we want to install. In this case \u0026ldquo;v2.1.0\u0026rdquo; was chosen.\nWith our tag selected we can submit our request. The \u0026ldquo;StackStormTags\u0026rdquo; property will be passed to our vRO workflow and can be used to set the particular version to install on our virtual machine.\nReferences https://get-creativetitle.com/2017/03/30/vrealize-orchestrator-http-rest-cannot-execute-the-request-read-timed-out/\nhttp://www.jaas.co/tag/vco/page/2/\nhttp://theithollow.com/2017/05/22/vra-placement-decisions-dynamic-form/\nhttps://developer.github.com/v3/repos/\n","date":"26 May, 2017","image":"\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://s3.us-west-2.amazonaws.com/greenreedtech.com/vra_7_2_dynamic_property_list_from_rest_api/vRA_Dynamic_Properties_4.png\" alt=\"\" class=\"w-100 img-fluid rounded\" height=\"\" width=\"\"\u003e\n","permalink":"https://www.greenreedtech.com/vra-7.2-dynamic-property-list-from-rest-api/","tags":["VMware","vRA"],"title":"vRA 7.2 dynamic property list from REST API"},{"categories":["VMware","vRA"],"contents":"In this post we\u0026rsquo;ll walk through how we can utilize vRealize Orchestrator and Splunk to determine how compliant our vRA appliance is with the vRealize Automation 7.2 hardening guide.\nvRA 7.2 hardening compliance script The first task is utilizing a script to check the settings specified in the hardening guide. In our case we\u0026rsquo;re going to generate JSON output from the script in order to easily ingest the data into Splunk. We\u0026rsquo;re utilizing bash to avoid any dependencies upon other packages that aren\u0026rsquo;t part of the default appliance install. The script below has been truncated for brevity and can be found in the github repo (https://github.com/martezr/vra72-hardening-automation).\nThe script is incomplete and does not cover all the items covered in the hardening guide.\n#!/bin/bash # Set host variable as the system hostname host=$(hostname) header=\u0026#34;{ \u0026#34;host\u0026#34;: \u0026#34;$host\u0026#34;, \u0026#34;checks\u0026#34;: [\u0026#34; checks=\u0026#34;\u0026#34; checknumber=\u0026#34;011\u0026#34; checkdescription=\u0026#34;Use IPv4 TCP Syncookies\u0026#34; cat /proc/sys/net/ipv4/tcp_syncookies | grep -v 1 \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 if [ $? -eq 1 ]; then check=\u0026#34;{\u0026#34;check\u0026#34;: \u0026#34;$checknumber\u0026#34;,\u0026#34;check_description\u0026#34;: \u0026#34;$checkdescription\u0026#34;,\u0026#34;status\u0026#34;: \u0026#34;pass\u0026#34;},\u0026#34; checks=\u0026#34;$checks $check\u0026#34; echo \u0026#34;pass\u0026#34; else check=\u0026#34;{\u0026#34;check\u0026#34;: \u0026#34;$checknumber\u0026#34;,\u0026#34;check_description\u0026#34;: \u0026#34;$checkdescription\u0026#34;,\u0026#34;status\u0026#34;: \u0026#34;fail\u0026#34;},\u0026#34; checks=\u0026#34;$checks $check\u0026#34; echo \u0026#34;fail\u0026#34; fi checknumber=\u0026#34;012\u0026#34; checkdescription=\u0026#34;Deny IPv6 Router Advertisements\u0026#34; grep [01] /proc/sys/net/ipv6/conf/*/accept_ra|egrep \u0026#34;default|all\u0026#34; | grep -v 0 \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 if [ $? -eq 1 ]; then check=\u0026#34;{\u0026#34;check\u0026#34;: \u0026#34;$checknumber\u0026#34;,\u0026#34;check_description\u0026#34;: \u0026#34;$checkdescription\u0026#34;,\u0026#34;status\u0026#34;: \u0026#34;pass\u0026#34;},\u0026#34; checks=\u0026#34;$checks $check\u0026#34; echo \u0026#34;pass\u0026#34; else check=\u0026#34;{\u0026#34;check\u0026#34;: \u0026#34;$checknumber\u0026#34;,\u0026#34;check_description\u0026#34;: \u0026#34;$checkdescription\u0026#34;,\u0026#34;status\u0026#34;: \u0026#34;fail\u0026#34;},\u0026#34; checks=\u0026#34;$checks $check\u0026#34; echo \u0026#34;fail\u0026#34; fi echo \u0026#34;{ \u0026#34;host\u0026#34;: \u0026#34;$host\u0026#34;, \u0026#34;checks\u0026#34;: [${checks::-1}] }\u0026#34; The script will generate JSON output similar to that below.\n{ \u0026#34;host\u0026#34;: \u0026#34;vratest.grt.local\u0026#34;, \u0026#34;checks\u0026#34;: [{ \u0026#34;check\u0026#34;: \u0026#34;011\u0026#34;, \u0026#34;check_description\u0026#34;: \u0026#34;Use IPv4 TCP Syncookies\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;pass\u0026#34; }, { \u0026#34;check\u0026#34;: \u0026#34;012\u0026#34;, \u0026#34;check_description\u0026#34;: \u0026#34;Deny IPv6 Router Advertisements\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;fail\u0026#34; }, { \u0026#34;check\u0026#34;: \u0026#34;013\u0026#34;, \u0026#34;check_description\u0026#34;: \u0026#34;Deny IPv6 Router Solicitations\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;pass\u0026#34; }, { \u0026#34;check\u0026#34;: \u0026#34;014\u0026#34;, \u0026#34;check_description\u0026#34;: \u0026#34;Deny IPv6 Router Preference in Router Solicitations\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;fail\u0026#34; }, { \u0026#34;check\u0026#34;: \u0026#34;015\u0026#34;, \u0026#34;check_description\u0026#34;: \u0026#34;Deny IPv6 Router Prefix\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;fail\u0026#34; }, { \u0026#34;check\u0026#34;: \u0026#34;016\u0026#34;, \u0026#34;check_description\u0026#34;: \u0026#34;Deny IPv6 Router Advertisement Hop Limit Settings\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;fail\u0026#34; }, { \u0026#34;check\u0026#34;: \u0026#34;017\u0026#34;, \u0026#34;check_description\u0026#34;: \u0026#34;Deny IPv6 Router Advertisement Autoconf Settings\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;fail\u0026#34; }, { \u0026#34;check\u0026#34;: \u0026#34;018\u0026#34;, \u0026#34;check_description\u0026#34;: \u0026#34;Deny IPv6 Neighbor Solicitations\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;pass\u0026#34; }, { \u0026#34;check\u0026#34;: \u0026#34;019\u0026#34;, \u0026#34;check_description\u0026#34;: \u0026#34;Restrict IPv6 Max Addresses\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;pass\u0026#34; }] } vRO workflow The first task in the workflow is to dynamically generate the command to run on the vRA appliance. In this example the script is stored on a web server and one of the workflow inputs is the url of the script.\nscript = \u0026#39;curl -so script.sh \u0026#39; + scriptUrl + \u0026#39; \u0026amp;\u0026amp; chmod +x script.sh \u0026amp;\u0026amp; sh script.sh; rm script.sh\u0026#39;; The second task is running the generated command against the vRA appliance and capturing the JSON output into an attribute that will later be sent to the Splunk server.\nThe third task in the workflow is to fetch the FQDN of the VM to be used as the source for Splunk.\nsplunkSource = vm.guest.hostName The final task in the workflow is to send the JSON output from our compliance script to our Splunk server. For this task we\u0026rsquo;re going to utilize the Splunk REST API to send data to the Splunk server. Before we can post the data we need to setup our REST API endpoint in vRO.\nLibrary \u0026gt; HTTP-REST \u0026gt; Configuration \u0026gt; Add a REST host\nSelect \u0026ldquo;Basic\u0026rdquo; host authentication\nWe\u0026rsquo;ll select \u0026ldquo;Shared Session\u0026rdquo; for the session mode and enter in a Splunk user account with the appropriate permissions. Click \u0026ldquo;Submit\u0026rdquo; to add the REST host.\nNow that we\u0026rsquo;ve added our REST host we just need to add a REST operation.\nLibrary \u0026gt; HTTP-REST \u0026gt; Configuration \u0026gt; Add a REST operation\nSelect the REST host that we just created as the \u0026ldquo;Parent Host\u0026rdquo;. Copy and past the \u0026ldquo;Template URL\u0026rdquo; below which utilizes a variable for the source name that was defined in the previous workflow step.\n/services/receivers/simple?source={sourceName}\u0026amp;sourcetype=web_event Splunk vRA 7.2 compliance dashboard Now that we\u0026rsquo;ve got our compliance data being ingested into our Splunk server let\u0026rsquo;s turn that data into a dashboard to easily make sense of the the data.\nReferences https://pubs.vmware.com/vrealize-automation-72/topic/com.vmware.ICbase/PDF/vrealize-automation-72-hardening.pdf\nhttp://theithollow.com/2015/08/27/vrealize-orchestrator-rest-hosts-and-operations-for-rubrik/\n","date":"01 May, 2017","image":"\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://s3.us-west-2.amazonaws.com/greenreedtech.com/vrealize_automation_7_2_hardening_compliance_with_vro_and_splunk/vRA72_Hardening_6.png\" alt=\"\" class=\"w-100 img-fluid rounded\" height=\"\" width=\"\"\u003e\n","permalink":"https://www.greenreedtech.com/vrealize-automation-7.2-hardening-compliance-with-vro-and-splunk/","tags":["VMware","vRA","Security","Splunk"],"title":"vRealize Automation 7.2 hardening compliance with vRO and Splunk"},{"categories":["VMware","vRO"],"contents":"A recent blog post by Rob Nelson about using vRO to provision VMs with vSphere tags (https://rnelson0.com/2017/04/06/vrealize-orchestrator-workflows-for-puppet-enterprise/) got me thinking about how to present a dynamic list of vSphere tags in vRO. So in this post we\u0026rsquo;ll walk through how to create a dynamic drop-down list of vSphere tags in vRO.\nThis post assumes that the VAPI plugin has been configured which is covered in this post.\nhttp://www.thevirtualist.org/vsphere-6-automation-vro-7-tags-vapi-part-i/\nThe first thing we need to do is create an action that retrieves the list of tags that we want. Click on the gray gear with the blue play button to change to actions section.\nCreate a folder to store the actions. I\u0026rsquo;ve used com.grt.vsphere for the folder name to align with the \u0026ldquo;com.orgname.function\u0026rdquo; naming schema.\nNow we can create the action by clicking on the gray gear with the blue play button above the \u0026ldquo;General\u0026rdquo; tab. Provide a name for the action and click \u0026ldquo;Ok\u0026rdquo;.\nNow we need to add the Javascript code needed to fetch the vSphere tags from the category that we want. The code below does just that and all we need to modify is the \u0026lsquo;var categoryname = \u0026ldquo;vROTags\u0026rdquo;\u0026rsquo; line at the second line and replace \u0026ldquo;vROTags\u0026rdquo; with the desired vSphere tag category.\n// Modify with the name of the desired category var categoryName = \u0026#34;vROTags\u0026#34; // Set the VAPI endpoint to the first endpoint returned var endpoints = VAPIManager.getAllEndpoints(); var endpoint = endpoints[0] if (endpoint == null) { throw \u0026#34;\u0026#39;endpoint\u0026#39; parameter should not be null\u0026#34;; } // Fetch tags and tag categories var client = endpoint.client(); var category = new com_vmware_cis_tagging_category(client); var categories = category.list(); var tagging = new com_vmware_cis_tagging_tag(client); var tags = tagging.list(); var outputTags = []; // Iterate through tag categories to find the category specified in categoryName for (var i in categories) { if (category.get(categories[i]).name == categoryName) { categoryId = categories[i]; System.log(\u0026#34;Name: \u0026#34; + category.get(categories[i]).name + \u0026#34; Id: \u0026#34; + categories[i]); } } // Iterate through tags to find the tags that belong to the categoryName and add them to the outputTags array for (var i in tags) { if (tagging.get(tags[i]).category_id.toString() == categoryId.toString()) { outputTags.push(tagging.get(tags[i]).name); } } System.log(outputTags); return outputTags; Copy and paste the Javascript code and modify the categoryName variable value to the desired vSphere tag category name. Then click on the blue link next to \u0026ldquo;Return Type\u0026rdquo; under \u0026ldquo;Scripting\u0026rdquo; and ensure that it is set to \u0026ldquo;Array/string\u0026rdquo;. Now that our action is configured click \u0026ldquo;Save and close\u0026rdquo;.\nWith all the hard stuff out of the way let\u0026rsquo;s create a workflow to test our action. We\u0026rsquo;ll add a scriptable task just to provide output which tag we selected.\nEdit the scriptable task and add the following code to log the tag selection.\nSystem.log(tag) Close the scriptable task and create an input parameter named \u0026ldquo;tag\u0026rdquo;.\nWe need to bind the \u0026ldquo;tag\u0026rdquo; input parameter to our scriptable task.\nClick on the \u0026ldquo;Presentation\u0026rdquo; tab and select the \u0026ldquo;tag\u0026rdquo; input. Then click the blue triangle to add a property and select \u0026ldquo;Predefined list of elements\u0026rdquo;. Click the purple piece on the property we just added to add the action to dynamically fetch tags.\nWe need to select the action that we created. We can use the filter text box to find the action we created by name and then click \u0026ldquo;Apply\u0026rdquo; to add it.\nIf all goes well we should see a dropdown list of the tags in the category we specified in the action.\nNow that we\u0026rsquo;ve run the workflow we can check in the log output which tag we selected from the drop-down.\nReferences http://www.thevirtualist.org/point-vapi-endpoint-javascript-vro/\nhttps://www.vcoteam.info/articles/learn-vco/290-dynamic-input-values-based-on-other-inputs.html\nhttp://www.thevirtualist.org/vsphere-6-automation-vro-7-tags-vapi-part-i/\nhttp://www.thevirtualist.org/vsphere-automation-vro-7-tags-vapi-part-ii/\n","date":"07 Apr, 2017","image":"\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://s3.us-west-2.amazonaws.com/greenreedtech.com/vro_building_a_dynamic_drop_down_for_vsphere_tags/vROvSphereTags_9.png\" alt=\"\" class=\"w-100 img-fluid rounded\" height=\"\" width=\"\"\u003e\n","permalink":"https://www.greenreedtech.com/vro-building-a-dynamic-drop-down-for-vsphere-tags/","tags":["vRO","VMware"],"title":"vRO - Building a dynamic drop-down for vSphere tags"},{"categories":["StackStorm","VMware","vRA"],"contents":"ChatOps is a pretty cool and still emerging technology that allows users to initiate actions on external systems from within a messaging platform such as slack. There are already bots available from vendors such as opvizor (http://www.opvizor.com/opbot/) that provide this functionality in a productized solution. In the case of opvizor, their bot interacts with VMware vSphere.\nIn our case we want to add chatops functionality to our VMware vRealize Automation 7 environment to be able to do cool things from within Slack. Things like see how many VMs exist in our tenant or see all the recent requests and their status.\nTo accomplish this we\u0026rsquo;re going to leverage StackStorm (https://stackstorm.com/) for integrating Slack and vRA7.\nConfiguring StackStorm The first thing we need to do is install StackStorm on a VM. The following specifications are recommended.\nOperating System: CentOS 7\nCPU: 2\nRAM: 2 GB\nHD: 40 GB\nDisable SELinux\nsetenforce 0 Install stackstorm\ncurl -sSL https://stackstorm.com/packages/install.sh | bash -s -- --user=st2admin --password=Ch@ngeMe Install the vRA7 StackStorm Pack\nst2 pack install https://github.com/martezr/stackstorm-vra7 Now that we\u0026rsquo;ve got the integration pack installed we need to configure the pack by setting the connection information for our vRA7 instance.\nhostname: The fqdn or IP address of the vRA7 instance\nusername: A user account with access to the vRA7 tenant\npassword: The password for the user account\ntenant: The vRA7 tenant (Defaults to vsphere.local)\nverify_ssl: Determines whether ssl certificate for the vRA7 instance is verified.\nThe password will be encrypted and stored in the stackstorm database.\nWe need to run the configuration script.\npython /opt/stackstorm/packs/vra7/vra7config.py We\u0026rsquo;ll be prompted for the connection information.\nhostname (vRA FQDN ex. cloud.company.local): vra.company.local verify vRA SSL certificate (true/false) [false]: username: administrator@vsphere.local Password: Retype password: tenant name [vsphere.local]: Successfully configured vRA7 integration pack Once the configuration has been completed the connection information is stored in a config at /opt/stackstorm/configs/vra7.yaml\nThe configuration file should not be updated manually as the password should be stored in clear text on the system.\nhostname: vra.company.local username: administrator@vsphere.local password: {{st2kv.system.vra7_password}} tenant: vsphere.local verify_ssl: true The last step is integrating StackStorm and Slack to allow us to talk to our bot. Configuring Slack and StackStorm has been covered extensively and the link below provides excellent guidance on how to do it.\nAnsible and ChatOps\nWith everything configured let\u0026rsquo;s open up slack and tell our bot what we want. For our example we\u0026rsquo;ll ask our bot how many VMs are in our vRA tenant.\n! How many vms are in vra Additional actions\nGet resource by name ! tell me about vm_name or ! get details about vm_name Get all requests ! get all vra7 requests References Stackstorm Install\nhttps://docs.stackstorm.com/install/\nGithub Repo\nhttps://github.com/martezr/stackstorm-vra7\n","date":"17 Feb, 2017","image":"\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://s3.us-west-2.amazonaws.com/greenreedtech.com/vra7_chatops_with_stackstorm/vRA7_Chatops_With_StackStorm_1.png\" alt=\"\" class=\"w-100 img-fluid rounded\" height=\"\" width=\"\"\u003e\n","permalink":"https://www.greenreedtech.com/vra7-chatops-with-stackstorm/","tags":["StackStorm","VMware","vRA"],"title":"vRA7 Chatops with StackStorm"},{"categories":["vmware","vra"],"contents":"What is Policy Based Autosigning A critical aspect of any Puppet deployment is determining how we want to allow nodes to get their certificate signed by the Puppet master. Before we delve into policy based autosigning we\u0026rsquo;ll discuss the other three methods for managing certificate signing.\nManual\nManually signing certs breaks automated deployment processes. Naive\nNaive is insecure as it allows any node access to the Puppet master. Basic Autosigning\nBasic autosigning signs requests based upon a whitelist which can be circumvented by modifying the name used in the request. Now that we\u0026rsquo;ve gone over the other three methods let\u0026rsquo;s go over policy based autosigning. Policy based autosigning utilizes a script that can be written in any language supported by the puppet master to validate some piece of information in the certificate request. This could be something as simple as a shell script to validate a challenge password against a file with a static list or as complex as reaching out to multiple external systems to validate. This adds additional security to our signing process as we\u0026rsquo;re challenging our node to provide additional information before signing a certificate.\nPuppet Policy Based Autosigning with vRA7 Now that we understand the goal and purpose of policy based autosigning we\u0026rsquo;ll jump into how we can use VMware vRealize Automation 7 as our validation source.\nOverview\nFirst we\u0026rsquo;ll cover how the entire process works from a high level.\nRequest catalog item\nTrigger vRO workflow\nPull machine uuid from the payload Create a csr_attributes.yaml file with the machine uuid Deploy the csr_attributes.yaml file onto the guest Install Puppet on the guest and trigger an agent run The agent presents it\u0026rsquo;s request for signing\nThe policy based autosiging script is run against the certificate request\nThe script contacts our vRA7 instance to validate the machine uuid provided in the request If the machine uuid exists sign the certificate If the machine uuid doesn\u0026rsquo;t exist reject the request Configure Puppet\nLet\u0026rsquo;s start by adding our autosigning script to our puppet master, we\u0026rsquo;ll place it in \u0026ldquo;/etc/puppetlabs/puppet\u0026rdquo;. The script can be downloaded from github using the link below.\nhttps://github.com/martezr/puppet-vra7-autosign/blob/master/vrapolicysign.rb\nWe need to make our script executable and ensure it is accessible by the user that Puppet is running as.\nPuppet Enterprise\nchmod +x vrapolicysign.rb \u0026amp;\u0026amp; chown pe-puppet:pe-puppet vrapolicysign.rb Puppet Open Source\nchmod +x vrapolicysign.rb \u0026amp;\u0026amp; chown puppet:puppet vrapolicysign.rb With our script in place we now need to add our connection for our vRA7 instance. We\u0026rsquo;ll add the \u0026ldquo;vrapolicyconfig.yaml\u0026rdquo; config file to the puppet master in the same location as our autosign script.\ngrtvra7: url: https://cloudportal.grt.local username: administrator@vsphere.local password: P@$$w0rd tenant: vsphere.local connection name: An arbitrary name for the connection\nurl: The url of the vRA7 instance\nusername: username an account with credentials to query the system for all VMs in the tenant\npassword: password for the account\ntenant: tenant to query\nThe script supports multiple connection entries but hasn\u0026rsquo;t not been tested to ensure properly functionality.\nNow that we\u0026rsquo;ve configured our connection information we just need to add our script to our Puppet config file and then restart the Puppet server service in order to start using policy based autosigning.\nAdd the following entry to the \u0026ldquo;[master]\u0026rdquo; section of the puppet.conf configuration file on the Puppet master.\nautosign = $confdir/vrapolicyautosign.rb Restart the puppet server service.\nPuppet Enterprise\nsystemctl restart pe-puppetserver Puppet Open Source\nsystemctl restart puppetserver With everything in place we\u0026rsquo;re ready to provision a new machine and have Puppet automatically sign the node\u0026rsquo;s certificate.\nThe script also has a logging mechanism that outputs the results of incoming requests in the \u0026ldquo;vrapolicylog.json\u0026rdquo; log file in the \u0026ldquo;/etc/puppetlabs/puppet/ssl\u0026rdquo; directory.\n{\u0026#34;status\u0026#34;:\u0026#34;success\u0026#34;,\u0026#34;certname\u0026#34;:\u0026#34;engineering0141.grt.local\u0026#34;,\u0026#34;uuid\u0026#34;:\u0026#34;afcd55a3-20ea-47cf-90cc-8bbb6c6d5324\u0026#34;} {\u0026#34;status\u0026#34;:\u0026#34;success\u0026#34;,\u0026#34;certname\u0026#34;:\u0026#34;engineering0154.grt.local\u0026#34;,\u0026#34;uuid\u0026#34;:\u0026#34;dd3b798f-d7c5-434f-8e0e-2df9cacea6e0\u0026#34;} {\u0026#34;status\u0026#34;:\u0026#34;success\u0026#34;,\u0026#34;certname\u0026#34;:\u0026#34;engineering0155.grt.local\u0026#34;,\u0026#34;uuid\u0026#34;:\u0026#34;db30bec4-858d-4f46-bb6b-b64c0670a3ac\u0026#34;} References Puppet\nhttps://docs.puppet.com/puppet/latest/ssl_autosign.html\nRDO Project\nhttps://blogs.rdoproject.org/7221/policy-based-autosigning-a-step-towards-more-secure-deployments-with-puppet\n","date":"09 Feb, 2017","image":"\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://s3.us-west-2.amazonaws.com/greenreedtech.com/vra7_chatops_with_stackstorm/vRA7_Chatops_With_StackStorm_1.png\" alt=\"\" class=\"w-100 img-fluid rounded\" height=\"\" width=\"\"\u003e\n","permalink":"https://www.greenreedtech.com/puppet-policy-based-autosigning-with-vra7/","tags":["puppet","vmware","vra"],"title":"Puppet Policy Based Autosigning with vRA7"},{"categories":["StackStorm"],"contents":"Being notified of when something happens in your environment has always been important and has evolved over time from basic emails to IM messages via tools like Slack.\nThis post will cover posting alarm information to Slack via StackStorm. In addition post alarm information to Slack we can take autoremediation actions in response to alarms which we\u0026rsquo;ll cover in later posts.\nGenerate API Key In order for our vSphere instance to communicate with StackStorm we need to provide vSphere with some way of authenticating to our StackStorm server. The solution is to create an API key that our script will use to connect to StackStorm.\nst2 apikey create -k -m \u0026#39;{\u0026#34;used_by\u0026#34;: \u0026#34;vsphere_alarms\u0026#34;}\u0026#39; MTk4ZThiZWM3ZTc0ZWI0N2UxNDMzZGExOWZjNw Create Custom StackStorm Webhook Now we\u0026rsquo;ll create our webhook to allow vSphere to communicate with StackStorm. We\u0026rsquo;ll create go ahead and rule which will also handle the creation of our webhook.\nWe\u0026rsquo;ll name our rule vsphere_alarm_slack and we\u0026rsquo;ll place it in the default pack.\n/opt/stackstorm/packs/default/rules/vsphere_alarm_slack.yaml\n--- name: \u0026#34;vsphere_alarm_slack\u0026#34; pack: \u0026#34;default\u0026#34; description: \u0026#34;Post vSphere alarms to Slack\u0026#34; enabled: true trigger: type: \u0026#34;core.st2.webhook\u0026#34; parameters: url: \u0026#34;vsphere\u0026#34; action: ref: \u0026#34;chatops.post_message\u0026#34; parameters: message: \u0026#34;Alarm Name: {{trigger.body.alarm_name}}n Alarm ID: {{trigger.body.alarm_id}}n Alarm Target: {{trigger.body.alarm_target_name}}n Alarm Description: {{trigger.body.alarm_description}}\u0026#34; channel: \u0026#34;vsphere\u0026#34; This configuration assumes that chatops has been configured to allow StackStorm to post to Slack.\nCreate vSphere Script With StackStorm configured we can now dig into creating our script to call the webhook from our VCSA when the alarm is triggered.\nSSH to the vCenter Appliance\nssh root@vcenterappliance Enable the command shell\nshell.set --enabled True Access the command shell\nshell We\u0026rsquo;ll create a scripts directory to store our scripts.\nmkdir /root/scripts Now we can create our script\n/root/scripts/alarm.sh\nThe environment variables are just a few key ones that have been picked from the entire list available that can be found here\n#!/bin/bash # environment variables alarm_name=$VMWARE_ALARM_NAME alarm_id=$VMWARE_ALARM_ID alarm_target_name=$VMWARE_ALARM_TARGET_NAME alarm_description=$VMWARE_ALARM_EVENTDESCRIPTION # Trigger StackStorm Web Hook curl -k -X POST https://stackstorm01.grt.local/api/v1/webhooks/vsphere -H \u0026#34;St2-Api-Key: MTk4ZThiZWM3ZTc0ZWI0N2UxNDMzZGExOWZjNw\u0026#34; -H \u0026#34;Content-Type: application/json\u0026#34; --data \u0026#39;{\u0026#34;alarm_name\u0026#34;: \u0026#34;value1\u0026#34;, \u0026#34;alarm_id\u0026#34;: \u0026#34;value2\u0026#34;, \u0026#34;alarm_target_name\u0026#34;: \u0026#34;value3\u0026#34;, \u0026#34;alarm_description\u0026#34;: \u0026#34;value4\u0026#34;}\u0026#39; Create vSphere Alarm The last step is for us to create the vSphere alarm that we\u0026rsquo;ll be notified of via Slack. Of course we could use an existing alarm as well.\nClick the green plus from the \u0026ldquo;Manage \u0026gt; Alarm Definitions\u0026rdquo; section of the VM we want to test with.\nWe\u0026rsquo;ll give the alarm a name of \u0026ldquo;stackstorm_notify\u0026rdquo; and select \u0026ldquo;specific event occuring on this object, for example VM Power On\u0026rdquo; from the change the \u0026ldquo;Monitor for:\u0026rdquo; section.\nClick the green plus and select \u0026ldquo;VM powered off\u0026rdquo; from the drop-down list of event options under the \u0026ldquo;Trigger if ANY of the following events occur:\u0026rdquo; section.\nClick the green plus and select \u0026ldquo;Run a command\u0026rdquo; from the action drop-down list. Add the path of our vSphere script in the \u0026ldquo;Configuration field\u0026rdquo; and select \u0026ldquo;Once\u0026rdquo; from the yellow to red section.\nNow that our rule and webhook have been created let\u0026rsquo;s go ahead and test our Alarm by powering off our VM. If everything has been configured correctly we get a fancy message in Slack regarding the alarm.\nThis is just a basic example of the value that StackStorm can bring to a vSphere environment.\nReferences vSphere Alarm Environment Variables\nhttps://pubs.vmware.com/vsphere-60/index.jsp?topic=%2Fcom.vmware.vsphere.monitoring.doc%2FGUID-5F5932FA-71FA-473E-8776-92B00742D566_copy.html\nVirtuallyGhetto - How to run a script on VCSA\nhttp://www.virtuallyghetto.com/2016/06/how-to-run-a-script-from-a-vcenter-alarm-action-in-the-vcsa.html\nStackStorm Authentication\nhttps://docs.stackstorm.com/authentication.html\nStackStorm Webhooks\nhttps://docs.stackstorm.com/webhooks.html\n","date":"03 Feb, 2017","image":"\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://s3.us-west-2.amazonaws.com/greenreedtech.com/vsphere_alarms_with_slack_and_stackstorm/stackstorm_vsphere_slack_05.png\" alt=\"\" class=\"w-100 img-fluid rounded\" height=\"\" width=\"\"\u003e\n","permalink":"https://www.greenreedtech.com/vsphere-alarms-with-slack-and-stackstorm/","tags":["DevOps","StackStorm","VMware"],"title":"vSphere Alarms with Slack and StackStorm"},{"categories":["Jenkins"],"contents":"This post covers the section listed below on the Certified Jenkins Engineer (CJE) exam.\nSection #3: Building Continuous Delivery (CD) Pipelines\nFolders\nHow to control access to items in Jenkins with folders Referencing jobs in folders What are folders? Jenkins provides the ability to organize jobs into a hierarchical manner with the CloudBees Folders Plugin. This allows us to manage the jobs much like we would files on a file system. Folders can also be used to manage permissions on a per folder basis to ease security administration.\nBelow is the config.xml file of a folder which holds the folder\u0026rsquo;s configuration much like the config.xml configuration file for a job.\n\u0026lt;?xml version=\u0026#39;1.0\u0026#39; encoding=\u0026#39;UTF-8\u0026#39;?\u0026gt; \u0026lt;com.cloudbees.hudson.plugins.folder.Folder plugin=\u0026#34;cloudbees-folder@5.12\u0026#34;\u0026gt; \u0026lt;actions/\u0026gt; \u0026lt;description\u0026gt;\u0026lt;/description\u0026gt; \u0026lt;properties/\u0026gt; \u0026lt;views\u0026gt; \u0026lt;hudson.model.AllView\u0026gt; \u0026lt;owner class=\u0026#34;com.cloudbees.hudson.plugins.folder.Folder\u0026#34; reference=\u0026#34;../../..\u0026#34;/\u0026gt; \u0026lt;name\u0026gt;All\u0026lt;/name\u0026gt; \u0026lt;filterExecutors\u0026gt;false\u0026lt;/filterExecutors\u0026gt; \u0026lt;filterQueue\u0026gt;false\u0026lt;/filterQueue\u0026gt; \u0026lt;properties class=\u0026#34;hudson.model.View$PropertyList\u0026#34;/\u0026gt; \u0026lt;/hudson.model.AllView\u0026gt; \u0026lt;/views\u0026gt; \u0026lt;viewsTabBar class=\u0026#34;hudson.views.DefaultViewsTabBar\u0026#34;/\u0026gt; \u0026lt;healthMetrics\u0026gt; \u0026lt;com.cloudbees.hudson.plugins.folder.health.WorstChildHealthMetric/\u0026gt; \u0026lt;/healthMetrics\u0026gt; \u0026lt;icon class=\u0026#34;com.cloudbees.hudson.plugins.folder.icons.StockFolderIcon\u0026#34;/\u0026gt; Creating folders Now that we\u0026rsquo;ve gone over what folders are let\u0026rsquo;s go ahead and create a folder.\nStep #1: Click \u0026ldquo;New Item\u0026rdquo; to create a new item (job/folder) Step #2: Enter a name in the textbox, select \u0026ldquo;Folder\u0026rdquo; and click \u0026ldquo;OK\u0026rdquo; to create the folder. Step #3: Configure the folder settings as desired and click \u0026ldquo;Save\u0026rdquo; to apply the changes. Creating a job in a folder This section will cover creating a new job within a folder.\nStep #1: From within the desired folder, click \u0026ldquo;New Item\u0026rdquo; to create a new item (job/folder). In this case we\u0026rsquo;re using the \u0026ldquo;Finance\u0026rdquo; folder we created in the previous task. Step #2: Enter a name in the textbox and select \u0026ldquo;Freestyle project\u0026rdquo; and click \u0026ldquo;OK\u0026rdquo; to create the job. Step #3: Configure the job settings as desired and click \u0026ldquo;Save\u0026rdquo; to apply the changes.\nIf we go back to the folder we now see that the \u0026ldquo;Monthly_Report\u0026rdquo; job we just created is listed in the folder.\nMoving an existing job to a folder In addition to creating a new job in our folder we can move our existing jobs to folder. We\u0026rsquo;ll move a job named \u0026ldquo;Test\u0026rdquo; to our Finance folder.\nStep 1: Click the arrow to the right of the job name and select \u0026ldquo;Move\u0026rdquo;. Step 2: Select the desired folder from the drop down menu and click \u0026ldquo;Move\u0026rdquo;. In our case we\u0026rsquo;ll select the Finance folder. We now see that the \u0026ldquo;Test\u0026rdquo; job has been added to our \u0026ldquo;Finance\u0026rdquo; folder.\nReferencing jobs in folders With the creation of folders we need to understand the namespace used for referencing files within the folder such as the folder config.xml and jobs in the folder.\nIn this example we\u0026rsquo;re going to use a folder named \u0026ldquo;JenkinsFolder\u0026rdquo; and a job name \u0026ldquo;JenkinsJob\u0026rdquo;.\nhttp://**JenkinsServer/job/JenkinsFolder/job/JenkinsJob\nServer Name: JenkinsServer\nFolder Name: JenkinsFolder\nJob Name: JenkinsJob\nWe can see that a \u0026ldquo;job\u0026rdquo; section is prepended to both the folder name as well as the job name. Now that we have an idea of how to navigate folders, let\u0026rsquo;s try an example. We need to create a link for the following job nested within a folder.\nServer Name: JenkinsServer\nFolder Name: Finance\nJob Name: MonthlyReport\nSolution: http://**JenkinsServer/job/Finance/job/MonthlyReport\nNow that we\u0026rsquo;re comfortable with accessing jobs within let\u0026rsquo;s try accessing a file within the folder.\nServer Name: JenkinsServer\nFolder Name: HR\nFile Name: config.xml\nSolution: http://**JenkinsServer/job/HR/config.xml\nHow to control access to items in Jenkins with folders We can utilize our folders for managing user access to jobs by providing users with global read privileges and then assigning the user additional rights at the folder level to allow them to manage the jobs within the folder but not access any other folders on the Jenkins server.\nA security realm needs to be configured\nStep #1: Click \u0026ldquo;Manage Jenkins\u0026rdquo; from the sidebar. Step #2: Click \u0026ldquo;Configure Global Security\u0026rdquo;. Step #3: Select the \u0026ldquo;Project-based Matrix Authorization Strategy\u0026rdquo; under the \u0026ldquo;Authorization\u0026rdquo; section and select the appropriate permissions. Step #4: From the folder configuration section select the \u0026ldquo;Enable project-based security\u0026rdquo; checkbox and assign the desired permissions to the user or group. With access control being managed at the folder level we are able to segregate jobs on our Jenkins server to different departments or groups.\nReference\nhttps://www.cloudbees.com/products/cloudbees-jenkins-platform/team-edition/features/folders-plugin\nhttps://go.cloudbees.com/docs/cloudbees-documentation/cje-user-guide/chapter-folder.html#\n","date":"14 Jul, 2016","image":"\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://s3.us-west-2.amazonaws.com/greenreedtech.com/vsphere_alarms_with_slack_and_stackstorm/stackstorm_vsphere_slack_05.png\" alt=\"\" class=\"w-100 img-fluid rounded\" height=\"\" width=\"\"\u003e\n","permalink":"https://www.greenreedtech.com/jenkins-certified-engineer-folders/","tags":["DevOps","Jenkins"],"title":"Jenkins Certified Engineer: Folders"},{"categories":["DevOps"],"contents":"Docker (https://www.docker.com/) is arguably the most popular container platform and this post covers a number of useful Docker commands from basic commands about viewing system information to viewing logs of docker containers. Additional commands can be found at the Docker docs website (https://docs.docker.com/engine/reference/commandline/cli/) or via the command line help.\nDocker Version View the version of docker installed on the system. Both the client and server version are listed.\ncore@coreos03 ~ $ docker version Client: Version: 1.11.1 API version: 1.23 Go version: go1.5.4 Git commit: 5604cbe Built: Wed Apr 27 00:34:42 2016 OS/Arch: linux/amd64 Server: Version: 1.11.1 API version: 1.23 Go version: go1.5.4 Git commit: 5604cbe Built: Wed Apr 27 00:34:42 2016 OS/Arch: linux/amd64 Docker Info The docker info command provides information about the configuration of the docker installation and the underlying host system.\nContainers: 5 Running: 0 Paused: 0 Stopped: 5 Images: 4 Server Version: 1.11.1 Storage Driver: devicemapper Pool Name: docker-253:0-201329853-pool Pool Blocksize: 65.54 kB Base Device Size: 10.74 GB Backing Filesystem: xfs Data file: /dev/loop0 Metadata file: /dev/loop1 Data Space Used: 4.111 GB Data Space Total: 107.4 GB Data Space Available: 31.4 GB Metadata Space Used: 3.453 MB Metadata Space Total: 2.147 GB Metadata Space Available: 2.144 GB Udev Sync Supported: true Deferred Removal Enabled: false Deferred Deletion Enabled: false Deferred Deleted Device Count: 0 Data loop file: /var/lib/docker/devicemapper/devicemapper/data Metadata loop file: /var/lib/docker/devicemapper/devicemapper/metadata Library Version: 1.02.107-RHEL7 (2015-12-01) Logging Driver: json-file Cgroup Driver: cgroupfs Plugins: Volume: local Network: bridge null host Kernel Version: 3.10.0-327.18.2.el7.x86_64 Operating System: CentOS Linux 7 (Core) OSType: linux Architecture: x86_64 CPUs: 4 Total Memory: 7.625 GiB Name: grtdocker01.grt.local ID: AUCR:OSG7:L55S:OUSY:RIC2:5Z3N:GUHO:6FY5:KY5D:4XMW:XVGE:OS4W Docker Root Dir: /var/lib/docker Debug mode (client): false Debug mode (server): false Registry: https://index.docker.io/v1/ WARNING: bridge-nf-call-iptables is disabled WARNING: bridge-nf-call-ip6tables is disabled Docker Search The docker search command is used to search Docker Hub for images.\ncore@coreos03 ~ $ docker search tomcat NAME DESCRIPTION STARS OFFICIAL AUTOMATED tomcat Apache Tomcat is an open source implementa... 170 [OK] tutum/tomcat Tomcat image - listens in port 8080. For t... 39 [OK] consol/tomcat-7.0 Tomcat 7.0.57, 8080, \u0026#34;admin/admin\u0026#34; 12 [OK] consol/tomcat-8.0 Tomcat 8.0.15, 8080, \u0026#34;admin/admin\u0026#34; 10 [OK] consol/tomcat-6.0 Tomcat 6.0.43, 8080, \u0026#34;admin/admin\u0026#34; 6 [OK] dordoka/tomcat Ubuntu 14.04, Oracle JDK 8 and Tomcat 8 ba... 4 [OK] cloudesire/tomcat Tomcat server 6/7/8 with oracle java 7/8 o... 1 [OK] ericogr/tomcat Tomcat 8.0.23, 8080, \u0026#34;docker/docker\u0026#34; 1 [OK] liferay/tomcat Tomcat version used by Portal bundles (onl... 0 [OK] learninglayers/tomcat 0 [OK] Docker Pull The docker pull command is used to pull/download a docker image from Docker hub or another registry.\ncore@coreos03 ~ $ docker pull tomcat latest: Pulling from tomcat 64e5325c0d9d: Pull complete bf84c1d84a8f: Pull complete 87de57de6955: Pull complete e96a900a1f99: Pull complete cea65f1f366a: Pull complete 4302b8c85571: Pull complete 18915de4c9b5: Pull complete 82abb7bc23ec: Pull complete 6416577ab1f9: Pull complete 30d3aa5cd1ab: Pull complete c38e3be299d7: Pull complete 5b52f9536b25: Pull complete 20a4f100dbfe: Pull complete 78385a28b88c: Pull complete a8fa88caa27d: Pull complete 70f5cd177182: Pull complete 46e30d8fbae8: Pull complete f7ad3bbb1c50: Pull complete cb8603fe47ec: Already exists tomcat:latest: The image you are pulling has been verified. Important: image verification is a tech preview feature and should not be relied on to provide security. Digest: sha256:848f3114260a02bacbd2495f30fb55264cc8271dc4ac4e3a8f2e7a052088ea07 Status: Downloaded newer image for tomcat:latest Docker Images The docker images command is used to view the docker images that are on the system.\ncore@coreos03 ~ $ docker images REPOSITORY TAG IMAGE ID CREATED VIRTUAL SIZE tomcat latest cb8603fe47ec 10 days ago 347.8 MB Docker Inspect The docker inspect command is used to view detailed information about a container.\ncore@coreos03 ~ $ docker inspect tomcat [{ \u0026#34;Architecture\u0026#34;: \u0026#34;amd64\u0026#34;, \u0026#34;Author\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;Comment\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;Config\u0026#34;: { \u0026#34;AttachStderr\u0026#34;: false, \u0026#34;AttachStdin\u0026#34;: false, \u0026#34;AttachStdout\u0026#34;: false, \u0026#34;Cmd\u0026#34;: [ \u0026#34;catalina.sh\u0026#34;, \u0026#34;run\u0026#34; ], \u0026#34;CpuShares\u0026#34;: 0, \u0026#34;Cpuset\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;Domainname\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;Entrypoint\u0026#34;: null, \u0026#34;Env\u0026#34;: [ \u0026#34;PATH=/usr/local/tomcat/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\u0026#34;, \u0026#34;LANG=C.UTF-8\u0026#34;, \u0026#34;JAVA_VERSION=7u79\u0026#34;, \u0026#34;JAVA_DEBIAN_VERSION=7u79-2.5.5-1~deb8u1\u0026#34;, \u0026#34;CATALINA_HOME=/usr/local/tomcat\u0026#34;, \u0026#34;TOMCAT_MAJOR=8\u0026#34;, \u0026#34;TOMCAT_VERSION=8.0.23\u0026#34;, \u0026#34;TOMCAT_TGZ_URL=https://www.apache.org/dist/tomcat/tomcat-8/v8.0.23/bin/ apache-tomcat-8.0.23.tar.gz\u0026#34; ], \u0026#34;ExposedPorts\u0026#34;: { \u0026#34;8080/tcp\u0026#34;: {} }, \u0026#34;Hostname\u0026#34;: \u0026#34;6c732c6044b7\u0026#34;, \u0026#34;Image\u0026#34;: \u0026#34;f7ad3bbb1c50434bb0c84c53492f342205b65c65d06945d063e5b17121d0d4c5\u0026#34;, \u0026#34;Labels\u0026#34;: {}, \u0026#34;MacAddress\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;Memory\u0026#34;: 0, \u0026#34;MemorySwap\u0026#34;: 0, \u0026#34;NetworkDisabled\u0026#34;: false, \u0026#34;OnBuild\u0026#34;: [], \u0026#34;OpenStdin\u0026#34;: false, \u0026#34;PortSpecs\u0026#34;: null, \u0026#34;StdinOnce\u0026#34;: false, \u0026#34;Tty\u0026#34;: false, \u0026#34;User\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;Volumes\u0026#34;: null, \u0026#34;WorkingDir\u0026#34;: \u0026#34;/usr/local/tomcat\u0026#34; }, \u0026#34;Container\u0026#34;: \u0026#34;be30c700ecba2c2b79ff74e285fed908c4d7999439dbd2930520d43192ddd194\u0026#34;, \u0026#34;ContainerConfig\u0026#34;: { \u0026#34;AttachStderr\u0026#34;: false, \u0026#34;AttachStdin\u0026#34;: false, \u0026#34;AttachStdout\u0026#34;: false, \u0026#34;Cmd\u0026#34;: [ \u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;#(nop) CMD [\u0026#34;catalina.sh\u0026#34; \u0026#34;run\u0026#34;]\u0026#34; ], \u0026#34;CpuShares\u0026#34;: 0, \u0026#34;Cpuset\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;Domainname\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;Entrypoint\u0026#34;: null, \u0026#34;Env\u0026#34;: [ \u0026#34;PATH=/usr/local/tomcat/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\u0026#34;, \u0026#34;LANG=C.UTF-8\u0026#34;, \u0026#34;JAVA_VERSION=7u79\u0026#34;, \u0026#34;JAVA_DEBIAN_VERSION=7u79-2.5.5-1~deb8u1\u0026#34;, \u0026#34;CATALINA_HOME=/usr/local/tomcat\u0026#34;, \u0026#34;TOMCAT_MAJOR=8\u0026#34;, \u0026#34;TOMCAT_VERSION=8.0.23\u0026#34;, \u0026#34;TOMCAT_TGZ_URL=https://www.apache.org/dist/tomcat/tomcat-8/v8.0.23/bin/ apache-tomcat-8.0.23.tar.gz\u0026#34; ], \u0026#34;ExposedPorts\u0026#34;: { \u0026#34;8080/tcp\u0026#34;: {} }, \u0026#34;Hostname\u0026#34;: \u0026#34;6c732c6044b7\u0026#34;, \u0026#34;Image\u0026#34;: \u0026#34;f7ad3bbb1c50434bb0c84c53492f342205b65c65d06945d063e5b17121d0d4c5\u0026#34;, \u0026#34;Labels\u0026#34;: {}, \u0026#34;MacAddress\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;Memory\u0026#34;: 0, \u0026#34;MemorySwap\u0026#34;: 0, \u0026#34;NetworkDisabled\u0026#34;: false, \u0026#34;OnBuild\u0026#34;: [], \u0026#34;OpenStdin\u0026#34;: false, \u0026#34;PortSpecs\u0026#34;: null, \u0026#34;StdinOnce\u0026#34;: false, \u0026#34;Tty\u0026#34;: false, \u0026#34;User\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;Volumes\u0026#34;: null, \u0026#34;WorkingDir\u0026#34;: \u0026#34;/usr/local/tomcat\u0026#34; }, \u0026#34;Created\u0026#34;: \u0026#34;2015-06-17T09:43:56.815855333Z\u0026#34;, \u0026#34;DockerVersion\u0026#34;: \u0026#34;1.6.2\u0026#34;, \u0026#34;Id\u0026#34;: \u0026#34;cb8603fe47ec8fb30fe333e5bbf3cf17ddb945343713a5fc377ad8f85ab29b9d\u0026#34;, \u0026#34;Os\u0026#34;: \u0026#34;linux\u0026#34;, \u0026#34;Parent\u0026#34;: \u0026#34;f7ad3bbb1c50434bb0c84c53492f342205b65c65d06945d063e5b17121d0d4c5\u0026#34;, \u0026#34;Size\u0026#34;: 0, \u0026#34;VirtualSize\u0026#34;: 347787355 } ] Docker Run The docker run command is used to start a docker container.\ncore@coreos03 ~ $ docker run -d --name=tomcatserver -p 8080:8080 tomcat 7ed9431710611b301ee9a50e3afc3f3fd8ce3486e9d00cbae06821d3bffb1007 Docker PS The docker ps command is used to view running containers.\ncore@coreos03 ~ $ docker ps CONTAINER ID IMAGE COMMAND 7ed943171061 tomcat:latest \u0026#34;catalina.sh run\u0026#34; CREATED STATUS PORTS NAMES 33 seconds ago Up 32 seconds 0.0.0.0:8080-\u0026gt;8080/tcp tomcatserver Docker Stop The docker stop command is used to stop a running container.\ncore@coreos03 ~ $ docker stop tomcatserver Docker Logs The docker logs command is used to view log data from a container.\ncore@coreos03 ~ $ docker logs tomcatserver Server version: Apache Tomcat/8.0.23 Server built: May 19 2015 14:58:38 UTC Server number: 8.0.23.0 OS Name: Linux OS Version: 4.0.3 Architecture: amd64 Java Home: /usr/lib/jvm/java-7-openjdk-amd64/jre JVM Version: 1.7.0_79-b14 JVM Vendor: Oracle Corporation CATALINA_BASE: /usr/local/tomcat CATALINA_HOME: /usr/local/tomcat Command line argument: -Dcatalina.base=/usr/local/tomcat Command line argument: -Dcatalina.home=/usr/local/tomcat Command line argument: -Djava.io.tmpdir=/usr/local/tomcat/temp Server startup in 598 ms Docker stats The docker stats command is an interactive tool for viewing performance data about running containers.\nCONTAINER CPU % MEM USAGE/LIMIT MEM % NET I/O tomcatserver 0.15% 124.3 MiB/1.943 GiB 6.25% 1.512 KiB/648 B References: https://docs.docker.com/engine/reference/commandline/cli/\n","date":"12 Jun, 2016","image":"\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://s3.us-west-2.amazonaws.com/greenreedtech.com/vsphere_alarms_with_slack_and_stackstorm/stackstorm_vsphere_slack_05.png\" alt=\"\" class=\"w-100 img-fluid rounded\" height=\"\" width=\"\"\u003e\n","permalink":"https://www.greenreedtech.com/useful-docker-commands/","tags":["Docker"],"title":"Useful Docker Commands"},{"categories":["Jenkins"],"contents":"This post covers the section listed below on the Certified Jenkins Engineer (CJE) exam.\nSection #1: Key CI/CD/Jenkins Concepts\nFingerprints\nWhat are fingerprints? How do fingerprints work? What are fingerprints? Jenkins utilizes fingerprints for tracking a specific instance of a file. This is critically important when attempting to determine which particular version of a file was used during a build. The fingerprint of a file is simply a MD5 checksum that can be used for comparing files.\nAn example fingerprint (MD5 checksum) is provided below for reference.\nMD5: d41d8cd98f00b204e9800998ecf8427e\nHow do fingerprints work? The fingerprints are stored as xml files in the $JENKINS_HOME/fingerprints directory on the Jenkins master. The xml file contains the Jenkins job name, build number, MD5 checksum, and fingerprinted file name.\n\u0026lt;?xml version=\u0026#39;1.0\u0026#39; encoding=\u0026#39;UTF-8\u0026#39;?\u0026gt; \u0026lt;fingerprint\u0026gt; \u0026lt;timestamp\u0026gt;2016-03-15 19:26:54.777 UTC\u0026lt;/timestamp\u0026gt; \u0026lt;original\u0026gt; \u0026lt;name\u0026gt;Test\u0026lt;/name\u0026gt; \u0026lt;number\u0026gt;1\u0026lt;/number\u0026gt; \u0026lt;/original\u0026gt; \u0026lt;md5sum\u0026gt;d41d8cd98f00b204e9800998ecf8427e\u0026lt;/md5sum\u0026gt; \u0026lt;fileName\u0026gt;testfile.txt\u0026lt;/fileName\u0026gt; \u0026lt;usages\u0026gt; \u0026lt;entry\u0026gt; \u0026lt;string\u0026gt;Test\u0026lt;/string\u0026gt; \u0026lt;ranges\u0026gt;1\u0026lt;/ranges\u0026gt; \u0026lt;/entry\u0026gt; \u0026lt;/usages\u0026gt; \u0026lt;facets/\u0026gt; \u0026lt;/fingerprint\u0026gt; References https://wiki.jenkins-ci.org/display/JENKINS/Fingerprint\n","date":"15 Mar, 2016","image":"\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://s3.us-west-2.amazonaws.com/greenreedtech.com/vsphere_alarms_with_slack_and_stackstorm/stackstorm_vsphere_slack_05.png\" alt=\"\" class=\"w-100 img-fluid rounded\" height=\"\" width=\"\"\u003e\n","permalink":"https://www.greenreedtech.com/jenkins-certified-engineer-fingerprints/","tags":["DevOps","Jenkins"],"title":"Jenkins Certified Engineer: Fingerprints"},{"categories":["Terraform"],"contents":"Recently a co-worker of mine (Thanks Ken Erwin) introduced me to Jenkins Job Builder (http://docs.openstack.org/infra/jenkins-job-builder/) which is a project created by the OpenStack infrastructure team that aims to automate the creation of Jenkins Jobs. The software is written in python and utilizes either yaml or json files as the framework for creating Jenkins jobs. A list of some of the primary features is provided below.\nFeatures: Supports job creation, modification, and deletion Supports templates to speed up the creation of similar jobs Supports all major plugins as well as extending the software to incorporate additional plugins The software has built-in test functionality to test changes before being deployed into production Many additional features Example: The following section covers an example job in yaml format.\n- job: name: puppet-module-profiles project-type: matrix execution-strategy: sequential: true axes: - axis: type: user-defined name: PUPPET_VERSION values: - 3.8.2 - 3.8.4 - 4.0.0 - 4.1.0 - axis: type: slave name: nodes values: - puppet description: \u0026#39;Jenkins job to perform syntax and style checking on {name} puppet module.\u0026#39; disabled: false node: puppet scm: - git: url: ssh://git@stash.grt.local:7999/infrastructure/puppet-module-profiles.git branches: - origin/pr/{branch} credentials-id: \u0026#39;f46gdb08-b25e-7h9e-ngef-nar52j732j98\u0026#39; browser: auto triggers: - pollscm: cron: \u0026#34;\u0026#34; wrappers: - ansicolor: colormap: xterm - workspace-cleanup: include: - \u0026#34;*\u0026#34; - rvm-env: implementation: 1.9.3@pupet-module-profiles builders: - system-groovy: command: | def currentBuild = Thread.currentThread().executable def PULL_REQUEST_URL = build.buildVariableResolver.resolve(\u0026#39;PULL_REQUEST_URL\u0026#39;) def PULL_REQUEST_ID = build.buildVariableResolver.resolve(\u0026#39;PULL_REQUEST_ID\u0026#39;) def description = \u0026#34;\u0026lt;a href=\u0026#39;$PULL_REQUEST_URL\u0026#39;\u0026gt;PR #$PULL_REQUEST_ID\u0026lt;/a\u0026gt;\u0026#34; currentBuild.setDescription(description) - conditional-step: condition-kind: regex-match regex: \u0026#39;^(BUTTON_TRIGGER|OPENED|REOPENED|UPDATED)$\u0026#39; label: ${{ENV,var=\u0026#34;ACTION\u0026#34;}} steps: - shell: | #!/bin/bash echo \u0026#34;Build Trigger $ACTION\u0026#34; export PUPPET_VERSION=\u0026#34;$PUPPET_VERSION\u0026#34; gem install bundler bundle install - shell: | find . -iname *.pp -exec puppet-lint --no-80chars-check --log-format \u0026#34;%{{path}}:%{{line}}:%{{check}}:%{{KIND}}:%{{message}}\u0026#34; {{}} ; - shell: | for file in $(find . -iname \u0026#39;*.pp\u0026#39;); do puppet parser validate --render-as s --modulepath=modules \u0026#34;$file\u0026#34; || exit 1; done - conditional-step: condition-kind: regex-match regex: \u0026#39;^(MERGED)$\u0026#39; label: ${{ENV,var=\u0026#34;ACTION\u0026#34;}} steps: - shell: | #!/bin/bash echo \u0026#34;Build Trigger $ACTION\u0026#34; publishers: - warnings: console-log-parsers: - Puppet-Lint run-always: true - stash: url: \u0026#39;https://stash.grt.local:8443\u0026#39; ignore-ssl: true References: http://docslide.us/software/continuous-delivery-with-docker-and-jenkins-job-builder.html\n","date":"13 Jan, 2016","image":"\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://s3.us-west-2.amazonaws.com/greenreedtech.com/vsphere_alarms_with_slack_and_stackstorm/stackstorm_vsphere_slack_05.png\" alt=\"\" class=\"w-100 img-fluid rounded\" height=\"\" width=\"\"\u003e\n","permalink":"https://www.greenreedtech.com/jenkins-job-builder/","tags":["DevOps","Terraform","VMware"],"title":"Jenkins Job Builder"},{"categories":["DevOps"],"contents":"This post covers integrating Jenkins CI server with Microsoft Active Directory to provide centralized authentication.\nStep #1 - Install he Active Directory plugin Click \u0026ldquo;Manage Jenkins\u0026rdquo; from the sidebar\nClick \u0026ldquo;Manage Plugins\u0026rdquo; to install the Active Directory plugin\nClick on the \u0026ldquo;Available\u0026rdquo; tab, enter \u0026ldquo;Active Directory\u0026rdquo; in the \u0026ldquo;Filter:\u0026rdquo; search box, click the checkbox next to \u0026ldquo;Active Directory plugin\u0026rdquo; and finally click \u0026ldquo;Download now and install after restart\u0026rdquo;\nClick the \u0026ldquo;Restart Jenkins when install is complete and no jobs are running\u0026rdquo; checkbox to complete the plugin installation.\nClick \u0026ldquo;Configure Global Security\u0026rdquo;\nSelect \u0026ldquo;Active Directory\u0026rdquo; and enter the domain and domain accounts information.\nSelect \u0026ldquo;Matrix-based security\u0026rdquo; and add the desired users and groups. user names and group names are case sensitive\nLog into the web interface with AD credentials\nReferences: Jenkins AD Security http://justinramel.com/2013/01/15/active-directory-security/\nJenkins AD Plugin https://wiki.jenkins-ci.org/display/JENKINS/Active+Directory+Plugin\n","date":"20 Nov, 2015","image":"\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://s3.us-west-2.amazonaws.com/greenreedtech.com/jenkins_active_directory/Jenkins_AD_Cover.png\" alt=\"\" class=\"w-100 img-fluid rounded\" height=\"\" width=\"\"\u003e\n","permalink":"https://www.greenreedtech.com/jenkins-active-directory/","tags":["DevOps","Jenkins"],"title":"Jenkins Active Directory"},{"categories":["Puppet"],"contents":"Puppet supports various hiera backends to pull in external data from various sources. This post will cover integrating open source puppet with a couchdb database using the hiera-http backend.\nCouchDB Setup and Configuration The following steps will cover the setup and configuration of the CouchDB database server that will be used as the data store. The following steps assume that docker has been installed\nStart CouchDB Docker container\nThe container is started with a name of \u0026ldquo;couchdb\u0026rdquo; and the CouchDB admin username and password are set during container creation. Port 5984 is mapped to port 5984 on the host machine.\ndocker run -d --name couchdb -p 5984:5984 -e COUCHDB_USERNAME=couchadmin -e COUCHDB_PASSWORD=Password couchdb Database configuration\nCouchDB provides a web interface for managing the databases as well as REST API.\nStep #1 - Log into the CouchDB web interface using the credentials created during the container provisioning.\nClick login at the bottom right corner of the web page\nhttp://ip_address:5984/_utils Enter the login credentials Step #2 - Create a new database In our example configuration we\u0026rsquo;ll use \u0026ldquo;hiera\u0026rdquo; as the database to store all the puppet related documents.\nClick \u0026ldquo;Create Database\u0026rdquo; to create the new database Enter the database name and click \u0026ldquo;Create\u0026rdquo; to create the database. Step #3 - Create a new document The documents will act like the individual .yaml files in the yaml backend to provide a customized hierarchy.\nClick \u0026ldquo;New Document\u0026rdquo; to create the new document Enter the appropriate value for the \u0026ldquo;_id\u0026rdquo; field and Click \u0026ldquo;Add Field\u0026rdquo; to add a new field. In our example we use common which replicates the common.yaml file in the yaml backend structure.\nEnter the desired hiera data and click \u0026ldquo;Save Document\u0026rdquo; when done. The additional fields are used to store the actual data such as classes, and class variables. In our example we\u0026rsquo;ll add a couchdbtest value for testing.\nThe example below shows what the code would look like in a yaml file.\n--- couchdbtest: \u0026#39;Does it really work\u0026#39; The database has now been configured so we can move on to the puppet configuration. REST API The previous steps for configuring the database and fields can be performed utilizing the REST API provided by CouchDB. Basic authentication is used to manage the database and documents.\nThe following command creates the \u0026ldquo;hiera\u0026rdquo; database\ncurl -X PUT http://couchadmin:Password@10.0.0.148:5984/hiera The following command creates the \u0026ldquo;common\u0026rdquo; document\ncurl -X PUT http://couchadmin:Password@10.0.0.148:5984/hiera/common -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39;{\u0026#34;couchdbtest\u0026#34;:\u0026#34;Does it really work\u0026#34;}\u0026#39; Puppet Master Configuration The following steps will cover configuring Puppet to communicate with the CouchDB database.\nInstall hiera-http The hiera-http is installed via ruby-gems\ngem install hiera-http Update hiera.yaml config file Use a text editor to modify the hiera.yaml file.\n--- :backends: [\u0026#39;http\u0026#39;,\u0026#39;yaml\u0026#39;] :hierarchy: - defaults - \u0026#34;%{clientcert}\u0026#34; - \u0026#34;%{environment}\u0026#34; - global :yaml: # datadir is empty here, so hiera uses its defaults: # - /var/lib/hiera on *nix # - %CommonAppData%PuppetLabshieravar on Windows # When specifying a datadir, make sure the directory exists. :datadir: :http: :host: 10.0.0.148 :port: 5984 :output: json :failure: graceful :use_auth: true :auth_user: \u0026#39;couchadmin\u0026#39; :auth_pass: \u0026#39;Password\u0026#39; :paths: - /hiera/%{clientcert} - /hiera/%{environment} - /hiera/common Perform hiera lookup We’ll perform a hiera lookup to verify that everything is working.\nhiera couchdbtest -d References: CouchDB Docker Image https://hub.docker.com/r/frodenas/couchdb/\nHiera-http configuration http://www.craigdunn.org/2012/11/puppet-data-from-couchdb-using-hiera-http/\nOpen source puppet master install http://blog.fnaard.com/2015/04/build-puppet-master-on-centos-7-hella.html\n","date":"19 Nov, 2015","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cpicture\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_545x0_resize_q95_h2_box.webp\" media=\"(max-width: 575px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_600x0_resize_q95_h2_box.webp\" media=\"(max-width: 767px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_700x0_resize_q95_h2_box.webp\" media=\"(max-width: 991px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_h2_box.webp\"\u003e\n  \u003cimg loading=\"lazy\" decoding=\"async\" class=\"w-100 img-fluid rounded\" src=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_box.jpg\" alt=\"\" width=\"970\" height=\"500\"\u003e\n\u003c/picture\u003e\n \n \n \n\n","permalink":"https://www.greenreedtech.com/puppet-hiera-http-using-couchdb/","tags":["Puppet","Hiera"],"title":"Puppet Hiera HTTP Using CouchDB"}]