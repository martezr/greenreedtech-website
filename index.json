[{"categories":["HashiCorp"],"contents":"HashiCorp Vault supports several authentication methods for human and non-human access. Several of the non-human authentication methods are tied to specific platforms or clouds such as AWS, Kubernetes, Azure, and others. For workloads that are running on non-supported platforms, the AppRole authentication method is typically recommended for authentication. The AppRole method uses a role as the core construct as the name implies. In order to authenticate a role ID and a secret ID are required.\nAppRole Conundrum The role is typically used in a non-unique or shared manner in that it is used by multiple systems for authentication purposes. It is possible to create a role that is unique to an individual workload but can greatly increase the management overhead. This poses the classic challenge of correctly identifying which system accessed a given path in Vault. The audit log does include the remote IP address of the system that makes the request, but this method of identification relies upon a point in time mapping of the IP address to the system.\nSolution The Vault API supports the ability to add custom metadata to a generated AppRole secret ID that is displayed in the Vault audit logs. This enables the system that is trusted to generate secret IDs for a given role to associate a unique identity with the secret ID. The custom metadata appears in the requests and responses of access attempts by the token associated with the generated secret ID. The screenshot below displays an example that specifies virtual_machine_name as the identifying piece of metadata associated with the secret ID.\nWithout the addition of the additional metadata all that we would know was that the potentially shared role was used to access a given path within HashiCorp Vault.\nAdding Identity Metadata Now that we know how the solution works let’s walk through adding the custom metadata during an API call to generate a secret ID for an example role. The API expects the metadata field to be a JSON formatted string (https://www.vaultproject.io/api/auth/approle#generate-new-secret-id).\ncurl -k \\  --header \u0026#34;X-Vault-Token: s.Zdd50L8a35S6FKxC1UNzvhgU\u0026#34; \\  --request POST --data \u0026#39;{\u0026#34;metadata\u0026#34;: \u0026#34;{ \\\u0026#34;tag1\\\u0026#34;: \\\u0026#34;production\\\u0026#34; }\u0026#34;}\u0026#39; \\  https://127.0.0.1:8200/v1/auth/approle/role/vsphereapp/secret-id Conclusion In this blog post we took a quick look at how we can utilize the AppRole authentication method in HashiCorp Vault without sacrificing the benefit of named or unique identities from an auditing perspective.\n","date":"01 Dec, 2021","image":"\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://www.datocms-assets.com/2885/1620159869-brandvaultprimaryattributedcolor.svg\" alt=\"\" class=\"w-100 img-fluid rounded\" height=\"\" width=\"\"\u003e\n","permalink":"https://www.greenreedtech.com/hashicorp-vault-unique-approle-identity-logging/","tags":["DevOps","HashiCorp","Security","Vault"],"title":"HashiCorp Vault Unique AppRole Identity Logging"},{"categories":["VMware"],"contents":"Network booting operating systems isn’t a new concept and has been around for years. Bare metal deployments are typically where network booting is commonly used. Most network cards are equipped with a network boot rom that enables the server to boot from the network using the PXE protocol. The PXE protocol is an old protocol that offers limited functionality. iPXE (https://ipxe.org/) is an open source network boot firmware that extends PXE with additional functionality. Network booting comes with a number of challenges, primarily configuring and managing the associated infrastructure required (DHCP, TFTP, etc.). TFTP isn’t typically used in most IT environments and DHCP isn’t commonly available on networks or VLANs used for hosting servers.\nNetwork booting vSphere VMs\nSimilar to bare metal servers a VMware vSphere virtual machine supports network booting via the PXE protocol embedded in the virtual network card boot rom. As we mention earlier, the PXE protocol is limited in terms of functionality and requires a DHCP and TFTP server on the network. iPXE enables offers more functionality than PXE but we need a way to use the iPXE protocol instead of PXE. This is sometimes done through a process known as chain loading in which we boot iPXE with PXE but we still need DHCP and TFTP. We could also burn an ISO image to eliminate the DHCP and TFTP requirement but that means every VM needs to have an ISO mounted.\nThe solution to our problem is to use a custom network boot ROM that includes iPXE. This means that we can quasi natively use iPXE instead of PXE to network boot without DHCP or TFTP. iPXE supports interacting with VMware guest info to set the network information to boot an OS image from the network.\nWhy network boot vSphere VMs?\nWe now know that network booting is a pain but we have a fairly elegant solution but why even consider it for virtual machines when a template can easily be used. I view this as the next wave of workload management in which virtual machines are stateless shells that can be used for hosting containers that offers a simple one to one mapping with persistent storage handle further up in the proverbial stack. In my opinion this decouples the underlying VM guest operating system from the application and unlocks the ability to patch or update the guest os on the fly. Hosting the OS in memory and not writing it to disk means that every boot is an opportunity to swap out the underlying operating system. Projects like LinuxKit make this all the more viable when you think about hosting a MySQL database container on a minimal container host that only host the database container. This means I can potentially swap out the host OS at a moment’s notice while maintaining continuity at the database layer. This starts to enable a huge shift in how IT workloads are managed.\nBuilding a network boot ROM Now that we know that we can replace the network boot rom of a virtual machine we need to create an iPXE boot rom to replace the VMware shipped network boot rom.\nReduce ROM Size\nIn order to boot iPXE from the custom network boot rom we need to disable some of the iPXE features that are enabled by default. This is due to a size limitation documented in this Github issue (https://github.com/ipxe/ipxe/issues/168). After some testing the one big component that would get us under the 64kb limit on the network boot rom was unfortunately HTTPS support. We could easily chainload a version of iPXE via HTTP to then use an image with HTTPS support that ultimately loads the operating system that we want.\nEnable VMware Settings\nThe VMware guest info support is easily added by uncommented one of the VMware iPXE setting in the config/settings.h file before building the rom (https://ipxe.org/buildcfg/vmware_settings).\nBuild the ROM\nIn order to build the network boot rom for our vSphere VM requires a number of tools but we can use Docker to avoid installing a bunch of tools on our local machine. I’ve simplified this process by creating automation to handle the end to end process in this Github repo – https://github.com/martezr/vsphere-network-boot-rom-builder.\nMove the boot ROM\nWith the network boot rom built we now need to move it somewhere that can be accessed by our ESXi host. There are two common options of where to place the rom, in the virtual machine directory on the ESXi datastore or in a global location that isn’t tied to the lifecycle of an individual VM. In this case the global location made sense to avoid needing to keep track of rom build versions across potentially dozens or hundreds of VMs.\nThe custom network boot rom can be uploaded to vSphere in a number of ways but I often use SCP to simply copy it to a datastore of a single ESXi host. The rom can be uploaded to a shared datastore hosted on any of the supported storage backends such as NFS or iSCSI. Here’s an example scp command to copy the boot rom to an ESXi host.\nscp bins/15ad07b0.rom root@grtesxi02.grt.local:/vmfs/volumes/5826d416-0504a9f8-cf5f-0026b954baa6 Creating a Virtual Machine With the iPXE network boot rom created we now need to create a new virtual machine that will act as our shell for booting our operating system. All of the iPXE configuration will be handled using guest information presented to the virtual machine. We just need to provide the information during the provisioning process. The network boot rom related values (nx3bios.filename and ethernet0.opromsize) are covered in this blog post (https://thewayeye.net/2012/april/1/pxe-booting-virtual-machines-using-vmware-fusionworkstation-and-gpxe-or-ipxe/) along with the appropriate values for other network adapter types such as e1000 and e1000e.\nNameDescriptionValueguestinfo.ipxe.filenameThehttp://boot.ipxe.org/demo/boot.phpguestinfo.ipxe.scriptletAn iPXE script to run on bootifopen net0 \u0026amp;\u0026amp; chain ${filename}guestinfo.ipxe.net0.ipThe IP address used by the first network interface during the iPXE boot phase10.0.0.10guestinfo.ipxe.net0.gatewayThe network gateway used by the first network interface during the iPXE boot phase10.0.0.1guestinfo.ipxe.dnsThe DNS server used during the iPXE boot phase10.0.0.200nx3bios.filenameThe path of the network boot rom relative to the vSphere environment./vmfs/volumes/5826d416-0504a9f8-cf5f-0026b954baa6/15ad07b0.romethernet0.opromsizeThe size in kilobytes of the network rom file58880 There’s several ways to create a virtual machine in vSphere but we’ll be using Terraform in this example to simplify the creation process.\ndata \u0026#34;vsphere_datacenter\u0026#34; \u0026#34;dc\u0026#34; {  name = \u0026#34;GRT\u0026#34; }  data \u0026#34;vsphere_datastore\u0026#34; \u0026#34;datastore\u0026#34; {  name = \u0026#34;Local_Storage_2\u0026#34;  datacenter_id = data.vsphere_datacenter.dc.id }  data \u0026#34;vsphere_compute_cluster\u0026#34; \u0026#34;cluster\u0026#34; {  name = \u0026#34;GRT-Cluster\u0026#34;  datacenter_id = data.vsphere_datacenter.dc.id }  data \u0026#34;vsphere_network\u0026#34; \u0026#34;network\u0026#34; {  name = \u0026#34;VM Network\u0026#34;  datacenter_id = data.vsphere_datacenter.dc.id }  data \u0026#34;vsphere_host\u0026#34; \u0026#34;host\u0026#34; {  name = \u0026#34;grtesxi02.grt.local\u0026#34;  datacenter_id = data.vsphere_datacenter.dc.id }  resource \u0026#34;vsphere_virtual_machine\u0026#34; \u0026#34;vm\u0026#34; {  name = \u0026#34;ipxedemo01\u0026#34;  resource_pool_id = data.vsphere_compute_cluster.cluster.resource_pool_id  datastore_id = data.vsphere_datastore.datastore.id  host_system_id = data.vsphere_host.host.id   num_cpus = 2  memory = 2048  guest_id = \u0026#34;other3xLinux64Guest\u0026#34;  wait_for_guest_net_timeout = 0   network_interface {  network_id = data.vsphere_network.network.id  }   disk {  label = \u0026#34;disk0\u0026#34;  size = 10  }   extra_config = {  \u0026#34;guestinfo.ipxe.scriptlet\u0026#34; = \u0026#34;ifopen net0 \u0026amp;\u0026amp; chain $${filename}\u0026#34;  \u0026#34;guestinfo.ipxe.filename\u0026#34; = \u0026#34;http://boot.ipxe.org/demo/boot.php\u0026#34;  \u0026#34;guestinfo.ipxe.net0.ip\u0026#34; = \u0026#34;10.0.0.11\u0026#34;  \u0026#34;guestinfo.ipxe.net0.gateway\u0026#34; = \u0026#34;10.0.0.1\u0026#34;  \u0026#34;guestinfo.ipxe.dns\u0026#34; = \u0026#34;10.0.0.200\u0026#34;  \u0026#34;nx3bios.filename\u0026#34; = \u0026#34;/vmfs/volumes/60eaabc3-b06bcfb0-091a-e4d3f1d05084/15ad07b0.rom\u0026#34;  \u0026#34;ethernet0.opromsize\u0026#34; = \u0026#34;58880\u0026#34;  }  } Booting the Virtual Machine Now that everything has been configured we’re ready to boot the virtual machine using the network settings associated with the virtual machine. We’re using the iPXE demo linux image for testing the boot process, the image should boot pretty quickly if it everything works as expected.\nReferences The following sites were referenced during the creation of this blog post.\nhttps://ipxe.org/buildcfg/vmware_settings\nhttps://thewayeye.net/2012/april/1/pxe-booting-virtual-machines-using-vmware-fusionworkstation-and-gpxe-or-ipxe/\n","date":"27 Aug, 2021","image":"\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://s3.us-west-2.amazonaws.com/greenreedtech.com/vmware-vsphere-vm-ipxe-boot-without-dhcp/vSphere_ipxe_title.png\" alt=\"\" class=\"w-100 img-fluid rounded\" height=\"\" width=\"\"\u003e\n","permalink":"https://www.greenreedtech.com/vmware-vsphere-vm-ipxe-boot-without-dhcp/","tags":["DevOps","VMware"],"title":"VMware vSphere VM iPXE Boot without DHCP"},{"categories":["HashiCorp"],"contents":"HashiCorp Vault supports a number of authentication methods including methods that utilize what HashiCorp refers to as a \u0026ldquo;trusted platform\u0026rdquo;. These include public clouds such as AWS, Azure and GCP along with platforms like Kubernetes. This method of authentication simplifies the introduction of the initial credential or secret that a workload must present to Vault by making use of information about itself that it already knows. The information that is provided to the instance or Kubernetes pod by the platform is metadata typically in the form of cryptographic data. This metadata is presented to HashiCorp Vault for authentication and verified by an API call to the underlying platform.\nThe question is why isn\u0026rsquo;t this same method used in a VMware vSphere environment to enable this same functionality. All of the other platforms natively present metadata that is difficult to guess and accessible only to that entity from a workload context. VMware vSphere doesn\u0026rsquo;t have a native virtual machine metadata service to enable this functionality. At one point there was a metadata service that was part of the work done with vSphere Integrated OpenStack (VIO) to provide this virtual machine metadata. Ultimately, OpenStack never gained the traction that many anticipated and the technical implementation of the metadata service was difficult in my opinion given the networking requirements for presenting the service to workloads.\nTo overcome the challenge of the platform lacking a native metadata service we can take advantage of a native capability that vSphere has used for years for appliances that are deployed to vSphere. vSphere guest info is used by OVA/OVF appliances to effectively pass information from the admin to the guest operating system via VMware tools. The guest operating system uses the vmtoolsd command to perform an RPC operation that allows the virtual machine to query guest info that has been populated by the administrator.\nSolution Overview Now that we have some background and context on the problem let\u0026rsquo;s take a look into the solution. We need to present the virtual machine with a unique piece of data that no other virtual can fetch or easily guess. HashiCorp Vault provides an AppRole authentication method that is ideally used for machine authentication. The AppRole requires a role ID and a secret ID to be presented to Vault to authenticate. The solution is to use an external system such as VMware Event Broker Appliance to populate a virtual machine\u0026rsquo;s guest info with the role ID and Secret ID from Vault. This is only accessible by the virtual machine itself and vSphere accounts with the appropriate permissions. VMware tools allows the guest OS to query the credentials and ultimately use them to authenticate to Vault. Now we\u0026rsquo;re ready to walk through an example of configuring Vault and VEBA to enable this authentication method.\nVault Configuration The first thing we need to is configure HashiCorp Vault to enable and configure AppRole authentication. We also need to create a Vault policy and identity/token for our VEBA function to interact with Vault.\nEnable Vault AppRole authentication method\nvault auth enable approle Create a Vault policy to associate with the virtual machine. Write the following policy to a file named vsphereapp-policy.hcl.\n# Read-only permission on \u0026#39;secret/vspheresecret\u0026#39; path path \u0026#34;secret/data/vspheresecret\u0026#34; {  capabilities = [ \u0026#34;read\u0026#34; ] } Create the Vault policy using the policy file that was just created.\nvault policy write vsphereapp vsphereapp-policy.hcl\nCreate an AppRole role with the policy that was just created assigned.\nvault write auth/approle/role/vsphereapp token_policies=\u0026#34;vsphereapp\u0026#34; secret_id_ttl=10m token_ttl=20m token_max_ttl=180m Create the secret that the policy grants access to.\nvault kv put secret/vspheresecret password=SuperSecurePassword VEBA Function Now that Vault is configured, we\u0026rsquo;re ready to deploy the VEBA function but there are a few things that need to be done first.\nClone the example github repository\ngit clone https://github.com/martezr/vauth Change the working directory to the vebavauth directory\ncd vauth/vebavauth Update the vebaconfig.toml file with the appropriate vCenter and Vault details.\n[vcenter] server = \u0026#34;grtvcenter01.grt.local\u0026#34; user = \u0026#34;administrator@vsphere.local\u0026#34; password = \u0026#34;password\u0026#34; insecure = true  [vault] server = \u0026#34;http://grtmanage01.grt.local:8200\u0026#34; token = \u0026#34;vaultpassword\u0026#34; Update the stack.yml file with the correct gateway for the VMware Event Broker Appliance in your environment.\nversion: 1.0 provider:  name: openfaas  gateway: https://grtveba01.grt.local functions:  vebavauth:  lang: golang-http  handler: ./handler  image: public.ecr.aws/i4r5n0t9/vebavauth:latest  environment:  write_debug: true  read_debug: true  function_debug: false  secrets:  - vebaconfig  annotations:  topic: VmPoweredOnEvent Log into OpenFaaS\nVEBA_GATEWAY=https://grtveba01.grt.local export OPENFAAS_URL=${VEBA_GATEWAY} cat ~/faas_pass.txt | faas-cli login -g https://grtveba01.grt.local -u admin --password-stdin --tls-no-verify Create the OpenFaas secret\nfaas-cli secret create vebaconfig --from-file=vebaconfig.toml --tls-no-verify Deploy the function\nfaas-cli deploy -f stack.yml --tls-no-verify Virtual Machine Configuration The VEBA function will inject the Vault AppRole role ID and secret ID into the VM\u0026rsquo;s advanced settings when the virtual machine is powered on but it first must be assigned a role in the custom attributes. The attribute name must be vauth-role as that is what the VEBA function uses when interacting the HashiCorp Vault instance.\nThe guest operating system now has access to the approle role name, role id and secret id to authenticate to HashiCorp Vault. A configuration management tool such as Ansible, Chef or Puppet could be used to run the command to query the guest info and subsequently generate a HashiCorp Vault agent configuration with that information. We won\u0026rsquo;t delve into the specifics of how to automate that end to end process in this post but we\u0026rsquo;ll perform the authentication operation using a script. On the guest operating system (CentOS 7 in this example) create a file name vaultlogin.sh that handle interacting with the VMware guest info and fetching the secret from Vault.\n#!/bin/bash  VAULTADDR=$1 ROLE=$(/usr/bin/vmtoolsd --cmd \u0026#34;info-get guestinfo.vault.role\u0026#34;) ROLEID=$(/usr/bin/vmtoolsd --cmd \u0026#34;info-get guestinfo.vault.roleid\u0026#34;) SECRETID=$(/usr/bin/vmtoolsd --cmd \u0026#34;info-get guestinfo.vault.secretid\u0026#34;)  # Create JSON payload with role and secret IDs cat \u0026lt;\u0026lt; EOF \u0026gt; payload.json { \u0026#34;role_id\u0026#34;: \u0026#34;$ROLEID\u0026#34;, \u0026#34;secret_id\u0026#34;: \u0026#34;$SECRETID\u0026#34; } EOF # Fetch Vault token  TOKENDATA=$(curl -s --request POST --data @payload.json $VAULTADDR/v1/auth/approle/login)  # Extract Vault token  TOKEN=$(echo $TOKENDATA | python -c \\  \u0026#39;import json,sys;print json.load(sys.stdin)[\u0026#34;auth\u0026#34;][\u0026#34;client_token\u0026#34;]\u0026#39;)  # Output Vault token  echo $TOKEN  # Fetch example secret  curl -s --header \u0026#34;X-Vault-Token: $TOKEN\u0026#34; $VAULTADDR/v1/secret/data/vspheresecret | python -m json.tool With the script created we are ready to run the script, the vault address must be passed to the script.\n./vaultlogin.sh http://grtvault01.grt.local:8200 The script should return the following output which is the token used for authentication and the JSON payload for the Vault secret. The authentication token output is for testing purposes only and should be removed for production environments.\ns.XBfeDjLCamnxpTik5zMpkNa8 {  \u0026#34;auth\u0026#34;: null,  \u0026#34;data\u0026#34;: {  \u0026#34;data\u0026#34;: {  \u0026#34;password\u0026#34;: \u0026#34;SuperSecurePassword\u0026#34;  },  \u0026#34;metadata\u0026#34;: {  \u0026#34;created_time\u0026#34;: \u0026#34;2021-02-16T16:09:47.867249231Z\u0026#34;,  \u0026#34;deletion_time\u0026#34;: \u0026#34;\u0026#34;,  \u0026#34;destroyed\u0026#34;: false,  \u0026#34;version\u0026#34;: 2  }  },  \u0026#34;lease_duration\u0026#34;: 0,  \u0026#34;lease_id\u0026#34;: \u0026#34;\u0026#34;,  \u0026#34;renewable\u0026#34;: false,  \u0026#34;request_id\u0026#34;: \u0026#34;dd2e325a-09f5-d416-6fb8-7b50041cd204\u0026#34;,  \u0026#34;warnings\u0026#34;: null,  \u0026#34;wrap_info\u0026#34;: null } There are a number of updates that can be made to the function to enable it to be used in a real environment but this blog post is to showcase how the solution can be used to simplify the authentication of VMware vSphere workloads to HashiCorp Vault.\nReferences https://www.vaultproject.io/docs/auth/approle https://www.vaultproject.io/api/secret/kv/kv-v2\n","date":"16 Feb, 2021","image":"\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://s3.us-west-2.amazonaws.com/greenreedtech.com/hashicorp-vault-vsphere-authentication-with-vmware-event-broker-appliance-veba/vsphere-vault-auth-veba-title.png\" alt=\"\" class=\"w-100 img-fluid rounded\" height=\"\" width=\"\"\u003e\n","permalink":"https://www.greenreedtech.com/hashicorp-vault-vsphere-authentication-with-vmware-event-broker-appliance-veba/","tags":["DevOps","HashiCorp","Security","Vault"],"title":"HashiCorp Vault vSphere Authentication with VMware Event Broker Appliance (VEBA)"},{"categories":["VMware"],"contents":"The VMware Event Broker Appliance (VEBA) fling is a really interesting project that enables administrators to take advantage of event driven automation in a VMware vSphere environment. I\u0026rsquo;ve been meaning to kick the tires on using the appliance as I\u0026rsquo;ve thought a lot about vSphere event driven security and what that looks like from a technical implementation perspective. In this blog post we\u0026rsquo;ll take a look at how to quickly get started with developing a Golang function for the VMware Event Broker Appliance (VEBA) that executes when a virtual machine is powered on.\nThe following requirements must be in place:\n Deployed VMware Event Broker Appliance (VEBA) with the OpenFaas event processor (https://vmweventbroker.io/kb/install-openfaas) Docker image registry (VMware Harbor, Docker Hub, AWS ECR, etc.) for hosting the created image Golang installed OpenFaas CLI installed (https://docs.openfaas.com/cli/install/)  Create a directory named vebago to start creating our function.\nmkdir vebago Change the working directory to the vebago directory we just created.\ncd vebago Create a file named stack.yml with the following contents. The gateway should be replaced with the VEBA\u0026rsquo;s URL (\u0026ldquo;https://appliance_fqdn\u0026rdquo;) and the image should reflect the location of where the Docker image will be stored. The name of the function is vebago and the topic parameter underneath annotations is a comma separated list of the vSphere events, in this case we are using a VM power on event.\nversion: 1.0 provider:  name: openfaas  gateway: https://grtveba01.grt.local functions:  vebago:  lang: golang-http  handler: ./handler  image: grtharbor01.grt.local/library/vebago:latest  environment:  write_debug: true  read_debug: true  function_debug: false  annotations:  topic: VmPoweredOnEvent The next thing we need to do is create a directory for our Golang code. The name of the directory that hosts Golang code is the value specified for the handler parameter in our stack.yml file that we just created.\nmkdir handler Change the working directory to the handler directory that we just created.\ncd handler Create a file named handler.go with the code snippet displayed below. The Golang code parses the event payload and prints the payload to STDOUT.\npackage function  import (  \u0026#34;encoding/json\u0026#34;  \u0026#34;fmt\u0026#34;  \u0026#34;log\u0026#34;  \u0026#34;net/http\u0026#34;   handler \u0026#34;github.com/openfaas/templates-sdk/go-http\u0026#34;  \u0026#34;github.com/vmware/govmomi/vim25/types\u0026#34; )  // cloudEvent is a subsection of a Cloud Event. type cloudEvent struct {  Data types.Event `json:\u0026#34;data\u0026#34;`  Source string `json:\u0026#34;source\u0026#34;`  Subject string `json:\u0026#34;subject\u0026#34;` }  // Handle a function invocation func Handle(req handler.Request) (handler.Response, error) {  cloudEvt, err := parseCloudEvent(req.Body)  if err != nil {  return errRespondAndLog(fmt.Errorf(\u0026#34;parsing cloud event data: %w\u0026#34;, err))  }   log.Println(cloudEvt)   return handler.Response{  Body: req.Body,  StatusCode: http.StatusOK,  }, nil }  func errRespondAndLog(err error) (handler.Response, error) {  log.Println(err.Error())   return handler.Response{  Body: []byte(err.Error()),  StatusCode: http.StatusInternalServerError,  }, err }  func parseCloudEvent(req []byte) (cloudEvent, error) {  var event cloudEvent   err := json.Unmarshal(req, \u0026amp;event)  if err != nil {  return cloudEvent{}, fmt.Errorf(\u0026#34;unmarshalling json: %w\u0026#34;, err)  }   return event, nil } Change the working directory back to the vebago directory.\ncd .. Now that we have our function components we need to download the OpenFaas template for Golang HTTP in order to properly create the function.\nfaas-cli template store pull golang-http\nWe now need to log into OpenFaas in order to deploy the function to OpenFaas.\ncat ~/faas_pass.txt | faas-cli login -g https://grtveba01.grt.local -u admin --password-stdin --tls-no-verify Once the template has been downloaded the function needs to be built, pushed to our docker registry and deployed. Fortunately we can do all of this with a single command to help save us some typing.\nfaas-cli up -f stack.yml --build-arg GO111MODULE=on --tls-no-verify The output of the command should be similar to that shown below.\nImage: grtharbor01.grt.local/library/vebago:latest built. [0] \u0026lt; Building vebago done in 1.23s. [0] Worker done. Total build time: 1.23s [0] \u0026gt; Pushing vebago [grtharbor01.grt.local/library/vebago:latest]. The push refers to repository [grtharbor01.grt.local/library/vebago] 03536ee2d42d: Layer already exists e8b6d1455770: Layer already exists 6ad006678b05: Layer already exists 5159f12913cf: Layer already exists 5f70bf18a086: Layer already exists e872e0b26f6c: Layer already exists 777b2c648970: Layer already exists latest: digest: sha256:755e3dbcdcaa490d069ecd2b547093b37f8abb37f1d2b7ed39d7402ce8fc08e1 size: 1784 [0] \u0026lt; Pushing vebago [grtharbor01.grt.local/library/vebago:latest] done. [0] Worker done.  Deploying: vebago.  Deployed. 202 Accepted. URL: https://grtveba01.grt.local/function/vebago.openfaas-fn With the function successfully deployed we are now ready to validate that the function is actually being called when a virtual machine is powered on and that the function outputs the event details. Since we aren\u0026rsquo;t sending the functions output via email or slack we have to view the STDOUT of the kubernetes pod. An easy way to view the output of the pod is to SSH into the VEBA virtual machine. The first thing to do is\nkubectl get pods -n openfaas-fn The name of the pods that back the OpenFaas functions should be displayed similar to the output displayed below. In this case we want to use the name of the pod that starts with vebago.\nNAME READY STATUS RESTARTS AGE template-compliance-899b5dbd8-qkbf9 1/1 Running 0 30h vebago-665f4c4f56-mwnjr 1/1 Running 0 9m18s The name of the pods start with the name of the OpenFaas function name specified in the stack.yml file.\nkubectl logs vebago-665f4c4f56-mwnjr -n openfaas-fn --follow At this point we can power on a virtual machine to validate if the function is working properly. The payload should include information about the virtual machine that was just powered on.\nForking - ./handler [] 2021/02/11 22:18:18 Started logging stderr from function. 2021/02/11 22:18:18 Started logging stdout from function. 2021/02/11 22:18:18 OperationalMode: http 2021/02/11 22:18:18 Timeouts: read: 10s, write: 10s hard: 10s. 2021/02/11 22:18:18 Listening on port: 8080 2021/02/11 22:18:18 Writing lock-file to: /tmp/.lock 2021/02/11 22:18:18 Metrics listening on port: 8081 2021/02/11 22:25:43 stderr: 2021/02/11 22:25:43 {{{} 18191 18189 2021-02-11 22:25:39.472999 +0000 UTC VSPHERE.LOCAL\\Administrator 0xc00005bc20 0xc00005bc50 0xc00005bc80 0xc00005bcb0 centos8base on grtesxi01.grt.local in GRT is powered on } https://grtvcenter01.grt.local/sdk VmPoweredOnEvent} 2021/02/11 22:25:43 POST / - 200 OK - ContentLength: 884 While this is a very simple example we can start to see the power being able trigger code in response to a vSphere event.\nReferences: http://www.patrickkremer.com/vmware-event-broker-appliance-part-xiii-deploying-go-functions/\n","date":"12 Feb, 2021","image":"\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://s3.us-west-2.amazonaws.com/greenreedtech.com/creating-a-vmware-event-broker-appliance-veba-golang-function/VEBA-Golang-OpenFaas-Function-2.png\" alt=\"\" class=\"w-100 img-fluid rounded\" height=\"\" width=\"\"\u003e\n","permalink":"https://www.greenreedtech.com/creating-a-vmware-event-broker-appliance-veba-golang-function/","tags":["VMware","VEBA"],"title":"Creating a VMware Event Broker Appliance (VEBA) Golang Function"},{"categories":["Career"],"contents":"I have made the decision to become the director of technical marketing at Morpheus Data and I start that new position today. Morpheus Data is a company that I became aware of during Cloud Field Day 3 (CFD3) while serving as a delegate for the event. Cloud Field Day is one of several events hosted by Gestalt IT in which several vendors present one at a time to a group of IT professionals that ask questions and provide feedback about the presentations. I was intrigued by what was covered during the presentation as it is a product that is right in my technical wheelhouse.\nWhat is Morpheus Data? Let\u0026rsquo;s get the first question out of the way for those that may have never heard of Morpheus Data. Morpheus Data is a software platform that fits in the Cloud Management Platform (CMP) space and can be be used for a lot more. One of the fundamental challenges it helps address is the complexity of provisioning workloads, especially on-premises workloads that often require multiple manual hand offs between teams to fully provision a server. The Morpheus platform has over 80 integrations with common tools and platforms such as Ansible, VMware NSX, InfoBlox and more to tie together the various parts of an end to end provisioning workflow. In addition to provisioning the platform has a number of other features that provide organizations with a centralized platform to enable IT agility along with governance at an enterprise scale.\nWhy Morpheus Data? One of the key value propositions of Morpheus Data is tying together disparate tools to help create business value. An aspect of technology that I enjoy most is creating complex orchestration workflows to solve complex problems. The broad number of Morpheus integrations allows me to tap into my varied experiences in IT and make use of the breadth of knowledge that I\u0026rsquo;ve gained during my career. In addition to the technical fit I like the opportunity that smaller companies provide to be able to wear a few different hats to directly impact the success of the organization. Something I took note of as I followed Morpheus Data from a far was the pace at which they were adding new features and capabilities to the platform and the ability to be able to speak to new and interesting things on a regular cadence is very appealing.\nWhy Technical Marketing? The majority of my career has been spent as an IT practitioner in various roles from desktop support to architecting large automation projects. The last several years I have focused on sharing the knowledge that I\u0026rsquo;ve gained over my years in IT. I\u0026rsquo;ve blogged for a number of years and began speaking at tech conferences a couple of years ago so I\u0026rsquo;ve had platforms through which I was able to fulfill that desire. In my role at Morpheus Data I\u0026rsquo;ll be creating marketing content with a focus on the technical aspects of the product and partner integrations. This is a dream role for me as it forces me to continue to grow non hands on keyboard skills such as storytelling and written communication. This role also allows me to to focus on the strategic aspects of helping to grow a business.\nI\u0026rsquo;m extremely excited about the opportunity to help grow Morpheus Data and the new challenge that I am about to begin.\n","date":"01 Feb, 2021","image":"\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://s3.us-west-2.amazonaws.com/greenreedtech.com/moving-to-morpheus-data/Morpheus-horizontal-v2.png\" alt=\"\" class=\"w-100 img-fluid rounded\" height=\"\" width=\"\"\u003e\n","permalink":"https://www.greenreedtech.com/moving-to-morpheus-data/","tags":["Morpheus"],"title":"Moving to Morpheus Data"},{"categories":["StackStorm"],"contents":"Chaos Engineering is the practice of experimenting or injecting faults into a system to test how the system responds to the failure (https://principlesofchaos.org). Chaos Engineering is still new for many organizations and can be a daunting practice to adopt at first but even baby steps into the practice can be immediately beneficial.\nIn this blog post we\u0026rsquo;ll look at how we can introduce chaos engineering into our deployment pipeline to test the resiliency of our example web application. We\u0026rsquo;ll be using StackStorm for running our deployment pipeline and integrate it with ChaosToolkit for carrying out the Chaos Engineering. ChaosToolkit is an open source framework written in Python for running chaos engineering experiments.\nThe code for this blog post can be found in the following Github repos:\n Terraform code: https://github.com/martezr/orchestration-demos/tree/master/terraform/chaosstack ChaosToolkit experiment: https://github.com/martezr/orchestration-demos/blob/master/chaos_experiments/aws.json StackStorm workflow: https://github.com/martezr/stackstorm-grtlab/blob/master/actions/workflows/chaos_demo.yaml  Architecture This example will deploy a web application to AWS using Terraform and include a load balancer that serves traffic to two (2) EC2 instances in an autoscale group (ASG). This architecture should allow for the web application to be accessible even if there was an issue with one of the EC2 instances in the autoscale group. This is a common architectural pattern for deploying highly available applications but there are various configuration issues that could prevent the application from being available following a failure.\nExperiment Given the architecture in our example what we will test or evaluate is if our website can continue to be accessed if one of the EC2 instances in the autoscale group is terminated.\nConfiguration: ChaosToolkit supports passing environment variables to the experiment. In this example we are passing the name of our autoscale group along with the DNS name of the application load balancer.\nHypothesis: The hypothesis is that we expect our web application to still be reachable even if one of the instances in our AWS autoscale group is terminated.\nChaos Method: In this experiment we are terminating an EC2 instance in the autoscale group that backs our application load balancer.\nThe following is the ChaosToolkit experiment that defines what will happen during the experiment. Experiments are written in a JSON format.\n{  \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34;,  \u0026#34;title\u0026#34;: \u0026#34;Ensure AWS resilliency\u0026#34;,  \u0026#34;description\u0026#34;: \u0026#34;If an autoscale fails another node should become the leader\u0026#34;,  \u0026#34;tags\u0026#34;: [\u0026#34;aws\u0026#34;],  \u0026#34;contributions\u0026#34;: {  \u0026#34;reliability\u0026#34;: \u0026#34;high\u0026#34;,  \u0026#34;security\u0026#34;: \u0026#34;none\u0026#34;,  \u0026#34;scalability\u0026#34;: \u0026#34;none\u0026#34;  },  \u0026#34;configuration\u0026#34;: {  \u0026#34;aws_region\u0026#34;: \u0026#34;us-east-1\u0026#34;,  \u0026#34;asg_name\u0026#34;: {  \u0026#34;type\u0026#34;: \u0026#34;env\u0026#34;,  \u0026#34;key\u0026#34;: \u0026#34;asg_name\u0026#34;  },  \u0026#34;alb_dns_name\u0026#34;: {  \u0026#34;type\u0026#34;: \u0026#34;env\u0026#34;,  \u0026#34;key\u0026#34;: \u0026#34;alb_dns_name\u0026#34;  }  },  \u0026#34;steady-state-hypothesis\u0026#34;: {  \u0026#34;title\u0026#34;: \u0026#34;Application responds\u0026#34;,  \u0026#34;probes\u0026#34;: [  {  \u0026#34;type\u0026#34;: \u0026#34;probe\u0026#34;,  \u0026#34;name\u0026#34;: \u0026#34;check-web-service\u0026#34;,  \u0026#34;tolerance\u0026#34;: 200,  \u0026#34;provider\u0026#34;: {  \u0026#34;type\u0026#34;: \u0026#34;http\u0026#34;,  \u0026#34;timeout\u0026#34;: [2, 2],  \u0026#34;expected_status\u0026#34;: 200,  \u0026#34;url\u0026#34;: \u0026#34;http://${alb_dns_name}\u0026#34;  }  } ]  },  \u0026#34;method\u0026#34;: [  {  \u0026#34;provider\u0026#34;: {  \u0026#34;module\u0026#34;: \u0026#34;chaosaws.asg.actions\u0026#34;,  \u0026#34;type\u0026#34;: \u0026#34;python\u0026#34;,  \u0026#34;func\u0026#34;: \u0026#34;terminate_random_instances\u0026#34;,  \u0026#34;arguments\u0026#34;: {  \u0026#34;asg_names\u0026#34;: [\u0026#34;${asg_name}\u0026#34;],  \u0026#34;instance_count\u0026#34;: 1  }  },  \u0026#34;type\u0026#34;: \u0026#34;action\u0026#34;,  \u0026#34;name\u0026#34;: \u0026#34;terminate-asg-instance\u0026#34;,  \u0026#34;pauses\u0026#34;: {  \u0026#34;after\u0026#34;: 10  }  },  {  \u0026#34;ref\u0026#34;: \u0026#34;check-web-service\u0026#34;  }  ],  \u0026#34;rollbacks\u0026#34;: [] } Pipeline Now that we have our experiment we need an automated method of running the experiment. In this case we\u0026rsquo;ll be using StackStorm to deploy our application stack and perform the experiment automatically once everything has been deployed.\nClone Github Repo\nThe code used in this example is hosted on Github and we need to clone that repo to allow the code to be executed by the latter steps of the workflow.\nclone_build_repo:  action: git.clone  input:  source: https://github.com/martezr/orchestration-demos.git  destination: /stackstorm/orchestration-demos  hosts: localhost  next:  - when: \u0026lt;% succeeded() %\u0026gt;  do: generate_aws_credentials Fetch Dynamic AWS Credentials\nIn this example we\u0026rsquo;re going to utilize HashiCorp Vault to fetch short-lived credentials to use for our pipeline execution. In this case the instance of HashiCorp Vault has already been configured and integrated with StackStorm. The AWS credentials are being published to allow subsequent steps in the workflow to access the credentials.\ngenerate_aws_credentials:  action: vault.read path=\u0026#34;aws/creds/stackstorm\u0026#34;  next:  - when: \u0026lt;% succeeded() %\u0026gt;  publish:  - access_key: \u0026lt;% result().result.access_key %\u0026gt;  - secret_key: \u0026lt;% result().result.secret_key %\u0026gt;  - stdout: \u0026lt;% result().stdout %\u0026gt;  - stderr: \u0026lt;% result().stderr %\u0026gt;  do:  - provision_app_stack Deploy Stack\nWe\u0026rsquo;re going to build our application stack using HashiCorp Terraform. In this example our Terraform code is in the Github repository we fetched in an earlier stage of the workflow. The state is being stored locally on the system for demonstration purposes and we are passing the AWS credentials from the last step as variables.\nprovision_app_stack:  delay: 15  action: terraform.pipeline  input:  plan_path: /stackstorm/orchestration-demos/terraform/chaosstack  backend: {\u0026#34;path\u0026#34;:\u0026#34;/stackstorm/chaosstack.tfstate\u0026#34;}  variable_dict: {\u0026#34;aws_access_key\u0026#34;:\u0026#34;\u0026lt;% ctx(\u0026#39;access_key\u0026#39;) %\u0026gt;\u0026#34;,\u0026#34;aws_secret_key\u0026#34;:\u0026#34;\u0026lt;% ctx(\u0026#39;secret_key\u0026#39;) %\u0026gt;\u0026#34;}  next:  - when: \u0026lt;% succeeded() %\u0026gt;  do: output_alb_dns Output ALB DNS Name\nIn the case of this example we won\u0026rsquo;t be using a dedicated DNS address for our web application see we need an additional stage in the workflow to capture the DNS name associated with the ALB that has been provisioned. This DNS address will be used by our chaos experiment to test reachability to our web application.\noutput_alb_dns:  action: terraform.output  input:  plan_path: /stackstorm/orchestration-demos/terraform/chaosstack  next:  - when: \u0026lt;% succeeded() %\u0026gt;  publish:  - alb_dns_name: \u0026lt;% result().result.alb_dns_name.value %\u0026gt;  do: run_chaos_experiment Run Chaos Experiment\nThe final stage of the workflow is to run our chaos experiment against the application stack we just deployed. The goal of the experiment is to evaluate if our web application is able to continue serving traffic relatively uninterrupted following the termination of an instance in the AWS autoscale group. In addition to the AWS credentials being passed to the stage we\u0026rsquo;re also including the name of the autoscale group and the load balancer\u0026rsquo;s DNS name for reachability testing.\nrun_chaos_experiment:  delay: 10  action: chaostoolkit.run_experiment  input:  path: /stackstorm/orchestration-demos/chaos_experiments/aws.json  env: {\u0026#34;AWS_ACCESS_KEY_ID\u0026#34;:\u0026#34;\u0026lt;% ctx(\u0026#39;access_key\u0026#39;) %\u0026gt;\u0026#34;,\u0026#34;AWS_SECRET_ACCESS_KEY\u0026#34;:\u0026#34;\u0026lt;% ctx(\u0026#39;secret_key\u0026#39;) %\u0026gt;\u0026#34;,\u0026#34;asg_name\u0026#34;:\u0026#34;\u0026lt;% ctx(\u0026#39;asg_name\u0026#39;) %\u0026gt;\u0026#34;,\u0026#34;alb_dns_name\u0026#34;:\u0026#34;\u0026lt;% ctx(\u0026#39;alb_dns_name\u0026#39;) %\u0026gt;\u0026#34;} The screenshot below is the deployment workflow in the StackStorm web interface.\nThis example shows the core capability of adding Chaos Engineering into the pipeline but could be extended to destroy the environment if the experiment failed, it could push the report from the experiment into a CMDB to create a record of how resilient the application is along with countless other possible integrations.\nHopefully this sets you on the path of getting excited about incorporating Chaos Engineering into your deployment pipelines to actually validate the assumptions that are made about your application\u0026rsquo;s resiliency.\n","date":"24 Jun, 2020","image":"\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://s3.us-west-2.amazonaws.com/greenreedtech.com/deployment-pipeline-chaos-engineering-with-stackstorm-and-chaostoolkit/chaos_update.png\" alt=\"\" class=\"w-100 img-fluid rounded\" height=\"\" width=\"\"\u003e\n","permalink":"https://www.greenreedtech.com/deployment-pipeline-chaos-engineering-with-stackstorm-and-chaostoolkit/","tags":["Chaostoolkit","Chaos Engineering","DevOps","StackStorm"],"title":"Deployment Pipeline Chaos Engineering with StackStorm and ChaosToolkit"},{"categories":["Vault"],"contents":"Revoking the root token on a production HashiCorp Vault deployment is one of the recommended best practices for securing an instance of HashiCorp Vault. The actual process to revoke the root token is fairly straightforward by running the vault token revoke command and providing the root token at the command line. In a previous blog post we looked at how to detect when a new root token has been generated. This might be necessary to perform certain operations that require root to carry out. One thing to be aware of is that multiple root tokens can be active at a single moment in time so there is no one root token but potentially many. With the potential for multiple root tokens we need a way to determine if there are any currently active root tokens on our Vault deployment.\nIdentifying active root tokens requires us to query the Vault instance and evaluate the token accessors to determine which one is a root token. In order to do this we\u0026rsquo;ll be using Python to do this programmatically and the hvac python library to easily interact with HashiCorp Vault. The following is a list of the high level steps that our script will perform to identify any active root tokens.\n Fetch the list of token accessor keys (This is a list of all the active tokens) Iterate through the list and perform a lookup on each token accessor  Minimum Vault Permissions The following are the minimum Vault permissions required to perform this search on the Vault deployment.\nhttps://gist.github.com/martezr/378a8f6ce22ab839bee0eb79cd43491a#file-reporter-hcl\nPython Code The python code is fairly simple in that it first fetches all the active token accessors and then iterates through each of them performing a token lookup to identify any token that has the \u0026ldquo;root\u0026rdquo; policy associated. The script expects the address of the Vault instance and the token used to access Vault to be provided as environment variables. The python script requires the hvac and PTable python libraries to be installed.\nhttps://gist.github.com/martezr/378a8f6ce22ab839bee0eb79cd43491a#file-active-root-py\nIf there are any active root tokens a table similar to the table below will be displayed with information about the root tokens.\nRevoking Identified Tokens If there are root tokens identified at that point we likely want to revoke them and that can be done by running the vault token revoke command and adding the -accessor switch with the token accessor displayed in the script output table similar to the full command below.\nvault token revoke -accessor xooFDg520Sh3LFfL7QOERsQN\nThe root token will have been revoked and re-running the script should return a result of no active root tokens.\nReferences HashiCorp Vault API for listing token accessors\nhttps://www.vaultproject.io/api-docs/auth/token#list-accessors\nHashiCorp Vault API for token lookup by accessor\nhttps://www.vaultproject.io/api-docs/auth/token#lookup-a-token-accessor\nHVAC Python Library\nhttps://hvac.readthedocs.io/en/stable/overview.html\n","date":"25 May, 2020","image":"\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://s3.us-west-2.amazonaws.com/greenreedtech.com/deployment-pipeline-chaos-engineering-with-stackstorm-and-chaostoolkit/chaos_update.png\" alt=\"\" class=\"w-100 img-fluid rounded\" height=\"\" width=\"\"\u003e\n","permalink":"https://www.greenreedtech.com/identifying-active-hashicorp-vault-root-tokens/","tags":["HashiCorp","Vault","DevOps","Security"],"title":"Identifying Active HashiCorp Vault Root Tokens"},{"categories":["Career"],"contents":"HashiCorp Vault generates a default root token during installation and best practice dictates that the token should be revoked once the deployment has been setup. There are certain critical operations that can only be carried out by a root token and requires that a new root token be generated. Given the immense power that the root token garners it would be ideal to identify when a root token is generated. In this example we\u0026rsquo;ll utilize the Vault audit log to determine when the process to generate a new root token is started and when it is successfully completed. Splunk will be used as our centralized logging server in this example.\nVault 1.4.0 added logging for root token generation events to the audit log to enable the easy identification of these events. Prior to Vault 1.4.0 there was no record of this operation in the Vault audit log to identify when a new root token is generated. The Vault system log output did show the execution of the root token generation commands.\nThe generation of a new root token involves multiple steps such as initializing the operation which generates an OTP, providing the unseal information and finally decoding the encode version of the generated root token.\nDetect Root Token Init\nWe want to be able to detect when someone attempts to generate a root token. The following command is used to initialize the root token generation process from the command line.\nvault operator generate-root -init The following search string returns the appropriate entry from the Vault audit log for this event.\nsource=\u0026#34;/opt/vault/logs/vault_audit.log\u0026#34; type=response request.path=\u0026#34;sys/generate-root/attempt\u0026#34; request.operation=\u0026#34;update\u0026#34; Successful Root Token Generation\nWe want to be able to detect when someone is able to successfully generate a new root token. The following search string returns the audit log entry from when the encoded token is generated.\nsource=\u0026#34;/opt/vault/logs/vault_audit.log\u0026#34; type=response request.path=\u0026#34;sys/generate-root/update\u0026#34; response.data.complete=true References The following content was used to create this blog post:\nRoot Token Generation Audit Log Pull Request\nhttps://github.com/hashicorp/vault/pull/8301\nVault Root Token Generation Lesson\nhttps://learn.hashicorp.com/vault/operations/ops-generate-root\nVault Root Token Command Reference\nhttps://www.vaultproject.io/docs/commands/operator/generate-root\n","date":"20 May, 2020","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cpicture\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_545x0_resize_q95_h2_box.webp\" media=\"(max-width: 575px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_600x0_resize_q95_h2_box.webp\" media=\"(max-width: 767px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_700x0_resize_q95_h2_box.webp\" media=\"(max-width: 991px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_h2_box.webp\"\u003e\n  \u003cimg loading=\"lazy\" decoding=\"async\" class=\"w-100 img-fluid rounded\" src=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_box.jpg\" alt=\"\" width=\"970\" height=\"500\"\u003e\n\u003c/picture\u003e\n \n \n \n\n","permalink":"https://www.greenreedtech.com/detecting-hashicorp-vault-root-token-generation/","tags":["DevOps","Terraform","VMware"],"title":"Detecting HashiCorp Vault Root Token Generation"},{"categories":["Career"],"contents":"Security of a HashiCorp Vault deployment is of paramount importance given the sensitive nature of the information contained within the platform. During the initial configuration process the root token is used to perform the setup and should be used to create less privileged named accounts. These accounts should be used for day to day administration of the Vault deployment and the root token should only be used in scenarios where it is absolutely necessary. The reason for this is the all-powerful privileges that the root token wields on the platform. Based upon this information it is critical to know whenever the root token is used to log into the Vault deployment and that\u0026rsquo;s what will be covered in this blog post.\nAll operations in HashiCorp Vault are audited and can be sent to an audit log or syslog that is ultimately shipped to a centralized logging server for greater security. In this example we want to utilize the audit log to find out when the root token is used for a login operation. We\u0026rsquo;ll use Splunk for our centralized logging server.\nVault Configuration The Vault instance needs to be configured to ship the log files to our logging server and in this example the logging server is a Splunk instance. This post assumes that you know how to configure Vault audit logging and ship the logs to a centralized logging server. Details on how to configure this are configured in a previous post (https://www.greenreedtech.com/vault-audit-logging/) but at a high level audit logging must be enabled in Vault, configured to write to a log file or syslog and configured to ship those logs to a centralized logging server.\nSplunk Configuration With the audit log now being shipped to our Splunk instance we can see the operations occurring in the Vault cluster. Now we need to figure out what a root login looks like in the audit log. The easiest way to do this was to perform a root login and identify what that operation was from a log perspective. The payload below is a root token login event.\n {  \u0026#34;auth\u0026#34;: {  \u0026#34;accessor\u0026#34;: \u0026#34;hmac-sha256:67869871d870282745682c729d86cee81acb5346c3dbecb573b7d44ea5506d06\u0026#34;,  \u0026#34;client_token\u0026#34;: \u0026#34;hmac-sha256:8fe52f85c93aad7df87c7203f864a9900d25451a1cc88c486ae0c951bd3a8936\u0026#34;,  \u0026#34;display_name\u0026#34;: \u0026#34;root\u0026#34;,  \u0026#34;policies\u0026#34;: [  \u0026#34;root\u0026#34;  ],  \u0026#34;token_policies\u0026#34;: [  \u0026#34;root\u0026#34;  ],  \u0026#34;token_type\u0026#34;: \u0026#34;service\u0026#34;  },  \u0026#34;request\u0026#34;: {  \u0026#34;client_token\u0026#34;: \u0026#34;hmac-sha256:8fe52f85c93aad7df87c7203f864a9900d25451a1cc88c486ae0c951bd3a8936\u0026#34;,  \u0026#34;client_token_accessor\u0026#34;: \u0026#34;hmac-sha256:67869871d870282745682c729d86cee81acb5346c3dbecb573b7d44ea5506d06\u0026#34;,  \u0026#34;id\u0026#34;: \u0026#34;3ab2651a-899b-0a98-c626-73c405d89d02\u0026#34;,  \u0026#34;namespace\u0026#34;: {  \u0026#34;id\u0026#34;: \u0026#34;root\u0026#34;  },  \u0026#34;operation\u0026#34;: \u0026#34;read\u0026#34;,  \u0026#34;path\u0026#34;: \u0026#34;auth/token/lookup-self\u0026#34;,  \u0026#34;remote_address\u0026#34;: \u0026#34;10.0.0.70\u0026#34;  },  \u0026#34;response\u0026#34;: {  \u0026#34;data\u0026#34;: {  \u0026#34;accessor\u0026#34;: \u0026#34;hmac-sha256:67869871d870282745682c729d86cee81acb5346c3dbecb573b7d44ea5506d06\u0026#34;,  \u0026#34;creation_time\u0026#34;: 1576768685,  \u0026#34;creation_ttl\u0026#34;: 0,  \u0026#34;display_name\u0026#34;: \u0026#34;hmac-sha256:6b89bf27681e54af63afe4a0b936bbf618f8d9b17bcc68df8c11470f7328d745\u0026#34;,  \u0026#34;entity_id\u0026#34;: \u0026#34;hmac-sha256:de212e047ea6043f736d83549f3dae8612c688af0d5a6b4d19a262473c5b8bea\u0026#34;,  \u0026#34;expire_time\u0026#34;: null,  \u0026#34;explicit_max_ttl\u0026#34;: 0,  \u0026#34;id\u0026#34;: \u0026#34;hmac-sha256:8fe52f85c93aad7df87c7203f864a9900d25451a1cc88c486ae0c951bd3a8936\u0026#34;,  \u0026#34;meta\u0026#34;: null,  \u0026#34;num_uses\u0026#34;: 0,  \u0026#34;orphan\u0026#34;: true,  \u0026#34;path\u0026#34;: \u0026#34;hmac-sha256:20039952cb073210bc9cb0fa1dc3dec3e49bcd8a72b5dd2a9f9ce415010c91a0\u0026#34;,  \u0026#34;policies\u0026#34;: [  \u0026#34;hmac-sha256:6b89bf27681e54af63afe4a0b936bbf618f8d9b17bcc68df8c11470f7328d745\u0026#34;  ],  \u0026#34;ttl\u0026#34;: 0,  \u0026#34;type\u0026#34;: \u0026#34;hmac-sha256:05148f41a98c981f657d9a0cb0b647e1f32a764719da2e75f27a497485eb9b7a\u0026#34;  }  },  \u0026#34;time\u0026#34;: \u0026#34;2020-01-31T13:14:37.132982729Z\u0026#34;,  \u0026#34;type\u0026#34;: \u0026#34;response\u0026#34; } Now that we know what we want to look for we can create our Splunk search string to return only root login events.\nauth.display_name=\u0026#34;root\u0026#34; type=\u0026#34;response\u0026#34; request.path=\u0026#34;auth/token/lookup-self\u0026#34; If there has been a recent login with the root token we should see results from our search similar to those below.\nThis can be extended to send a notification or create a helpdesk ticket, most logging platforms have the ability to trigger an action when a designated event occurs. As an example, Splunk supports a handful of alert actions such as sending an email, outputting results to a lookup file or sending a webhook to an external system.\nReferences The following content was used to create this blog post.\nVault Production Guide\nhttps://www.vaultproject.io/guides/operations/production\nSplunk Alert Webhooks\nhttps://docs.splunk.com/Documentation/Splunk/8.0.1/Alert/Webhooks\n","date":"09 Feb, 2020","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cpicture\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_545x0_resize_q95_h2_box.webp\" media=\"(max-width: 575px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_600x0_resize_q95_h2_box.webp\" media=\"(max-width: 767px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_700x0_resize_q95_h2_box.webp\" media=\"(max-width: 991px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_h2_box.webp\"\u003e\n  \u003cimg loading=\"lazy\" decoding=\"async\" class=\"w-100 img-fluid rounded\" src=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_box.jpg\" alt=\"\" width=\"970\" height=\"500\"\u003e\n\u003c/picture\u003e\n \n \n \n\n","permalink":"https://www.greenreedtech.com/detecting-hashicorp-vault-root-login/","tags":["DevOps","Terraform","VMware"],"title":"Detecting HashiCorp Vault Root Login"},{"categories":["Career"],"contents":"In this blog post we\u0026rsquo;ll look at the HashiCorp Vault plugin for Puppet Bolt that enables authentication credentials for Bolt to be retrieved from an instance of HashiCorp Vault. HashiCorp Vault is a secrets management platform that is commonly used to store secrets such as API keys, passwords and SSH private keys. This solution helps to avoid secret sprawl where passwords and credentials are widely distributed across an environment making it difficult to track where they are.\nThe plugin was added to Puppet Bolt in version 1.28.0 and natively supports token and userpass Vault authentication methods. Documentation for the plugin can be found here - https://forge.puppet.com/puppetlabs/vault/readme.\nVault Setup We\u0026rsquo;ll configure a development instance of HashiCorp Vault to walk through the plugin\u0026rsquo;s functionality. The following assumes a basic knowledge of how to setup at least a dev instance of HashiCorp Vault. This information can be found on HashiCorp\u0026rsquo;s website if necessary (https://learn.hashicorp.com/vault).\nWrite the Windows administrator password to Vault.\nWe need to write the password for the Windows machine to the secret/credentials/windows path in Vault.\nvault kv put secret/credentials/windows password=Puppet123 Write the Linux SSH private key to Vault\nWe need to write the SSH private key for the Linux machine to the secret/credentials/linux path in Vault. The private key has been saved to a file named bolt_id_rsa which is being uploaded as a secret.\nvault kv put secret/credentials/linux privatekey=@bolt_id_rsa Create a Vault Policy\nWe need to create a Vault Policy that allows the token to read the secrets in the \u0026ldquo;credentials\u0026rdquo; secret structure but nothing else under the \u0026ldquo;secret\u0026rdquo; space. The policy should be saved to a file name bolt-policy.hcl which we\u0026rsquo;ll use in the next command to actually create the policy in Vault.\npath \u0026#34;secret/data/credentials/*\u0026#34; {  capabilities = [\u0026#34;read\u0026#34;] } path \u0026#34;secret/metadata/credentials/*\u0026#34; {  capabilities = [\u0026#34;list\u0026#34;,\u0026#34;read\u0026#34;] } The following command creates a policy named \u0026ldquo;bolt\u0026rdquo; using the policy file that was just created in the previous step.\nvault policy write bolt bolt-policy.hcl Token Authentication HashiCorp Vault supports token authentication that allows a token generated by an existing token or another authentication method to be used for interacting with a Vault instance. When logging into Vault via an authentication method a token is generated which is assigned privileges based upon the policies associated with the token during creation.\nGenerate Token\nLogged in with the root token we can generate a token with the bolt policy we created in a previous step associated.\nLogging in with the root token is not recommended for day to day administration in production environments\nvault token create -policy=bolt The output should be similar to that below and the \u0026ldquo;token\u0026rdquo; is what we\u0026rsquo;ll use in our Bolt inventory file to authenticate.\nKey Value — —– token s.3649w1Fh80RtwSteoDzWuDUi token_accessor Ki4onGqPfwdnMVJQFX40ddqZ token_duration 768h token_renewable true token_policies [\u0026#34;bolt\u0026#34; \u0026#34;default\u0026#34;] identity_policies [] policies [\u0026#34;bolt\u0026#34; \u0026#34;default\u0026#34;] Bolt Configuration File\nThe Bolt configuration file is used to set the global configuration for Bolt and in this example we\u0026rsquo;re adding the configuration for the Vault plugin to this file. The token has been added in plaintext to the file but we can specify the \u0026ldquo;VAULT_TOKEN\u0026rdquo; environment variable or use another plugin for encryption such as the PKCS7 to avoid the token being in plaintext in the Bolt config file.\nmodulepath: \u0026#34;~/.puppetlabs/bolt-code/modules:~/.puppetlabs/bolt-code/site-modules\u0026#34; concurrency: 10 format: human winrm:  ssl: false ssh:  host-key-check: false plugins:  vault:  server_url: http://127.0.0.1:8200  auth:  method: token  token: s.3649w1Fh80RtwSteoDzWuDUi In addition to generating a token using an existing token HashiCorp Vault generates a token upon login when using other authentication methods, such as those covered below.\nHuman Interaction\nHashiCorp Vault supports a number of authentication methods that are intended for a human or interactive login. The following methods are commonly utilized but is not a complete list.\n LDAP Okta Radius Github  Machine Interaction\nThere is often a need to run automation as part of a pipeline or scheduled task. This means that we can\u0026rsquo;t expect a human to perform a login operation to fetch a token. In this case we need to use one of HashiCorp Vault\u0026rsquo;s authentication methods intended for non-human interaction.\nThe following authentication methods are intended for non-human authentication.\n AWS Kubernetes TLS Azure AppRole  UserPass Authentication In addition to the token authentication method the plugin also supports userpass authentication. HashiCorp Vault supports a userpass authentication method that is a local user database in Vault that utilizes a username and password for authentication.\nEnable UserPass Authentication\nThe authentication engine or backend needs to be enabled before we can use that authentication method.\nvault auth enable userpass Create a user account\nThe userpass authentication method has been enabled and now we need to create a user account with a password and associate the Bolt Vault policy.\nvault write auth/userpass/users/puppetbolt password=Password123 policies=bolt We can validate that the user was successfully created and that the bolt policy is associated by running the vault login command below, a password prompt will be presented\nvault login -method=userpass username=puppetbolt Output similar to that shown below will be displayed and we can see that the \u0026ldquo;bolt\u0026rdquo; policy is associated with the credentials.\nSuccess! You are now authenticated. The token information displayed below is already stored in the token helper. You do NOT need to run \u0026#34;vault login\u0026#34; again. Future Vault requests will automatically use this token.  Key Value — —– token s.Xd4v1qoCtnKEnDHjzYTRm1KC token_accessor 1ZRZUUYfWRJOGNBj8qnbRGvf token_duration 768h token_renewable true token_policies [\u0026#34;bolt\u0026#34; \u0026#34;default\u0026#34;] identity_policies [] policies [\u0026#34;bolt\u0026#34; \u0026#34;default\u0026#34;] token_meta_username puppetbolt Bolt Configuration File\nThe Bolt configuration file is used to set the global configuration for Bolt and in this example we’re adding the configuration for the Vault plugin to this file. The username and password have been added in plaintext to the file. Similar to the token authentication method we can use another plugin for encryption such as the PKCS7 to avoid the password being in plaintext in the Bolt config file.\nmodulepath: \u0026#34;~/.puppetlabs/bolt-code/modules:~/.puppetlabs/bolt-code/site-modules\u0026#34; concurrency: 10 format: human winrm:  ssl: false ssh:  host-key-check: false plugins:  vault:  server_url: http://127.0.0.1:8200  auth:  method: userpass  user: puppetbolt  pass: Password123 Bolt Inventory File With either the authentication method configured for the Vault plugin now we just need to create an inventory file for specifying the path in Vault where Bolt will fetch the secret from.\nWindows\nThe Bolt inventory file below is an example of using the plugin to retrieve the password used by Bolt for connecting to Windows nodes via WinRM.\nversion: 2 targets:  – uri: winnode1  config:  transport: winrm  winrm:  user: administrator  password:  _plugin: vault  path: secret/credentials/windows  field: password  version: 2 With the inventory file created we can run a simple command to check that the plugin is able to fetch the credentials from Vault.\nbolt plan run facts -i inventory.yaml –targets=winnode1 Linux\nThe Bolt inventory file below is an example of using the plugin to retrieve the SSH private key used by Bolt for connecting to Linux nodes via SSH.\nversion: 2 targets:  – uri: linuxnode1  config:  transport: ssh  ssh:  user: root  private-key:  key-data:  _plugin: vault  path: secret/credentials/linux  field: privatekey  version: 2 With the inventory file created we can run a simple command to check that the plugin is able to fetch the credentials from Vault.\nbolt plan run facts -i inventory.yaml –targets=linuxnode1 The plugin provides the ability to allow Puppet Bolt to offload a critical component of any automation process to a dedicated platform in Vault. This enables a more robust solution for managing secrets in a secure and automated manner.\nReferences Puppet Bolt HashiCorp Vault plugin documentation\nhttps://forge.puppet.com/puppetlabs/vault/readme\nPuppet Bolt Inventory File\nhttps://puppet.com/docs/bolt/latest/inventory_file_v2.html\n","date":"26 Nov, 2019","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cpicture\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_545x0_resize_q95_h2_box.webp\" media=\"(max-width: 575px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_600x0_resize_q95_h2_box.webp\" media=\"(max-width: 767px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_700x0_resize_q95_h2_box.webp\" media=\"(max-width: 991px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_h2_box.webp\"\u003e\n  \u003cimg loading=\"lazy\" decoding=\"async\" class=\"w-100 img-fluid rounded\" src=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_box.jpg\" alt=\"\" width=\"970\" height=\"500\"\u003e\n\u003c/picture\u003e\n \n \n \n\n","permalink":"https://www.greenreedtech.com/puppet-bolt-vault-inventory-plugin/","tags":["DevOps","Terraform","VMware"],"title":"Puppet Bolt Vault Inventory Plugin"},{"categories":["DevOps"],"contents":"HashiCorp Terraform is a popular Infrastructure as Code (IaC) tool that is used for provisioning virtual machines or cloud instances along with other resources. Once a virtual machine or cloud instance is provisioned typically it still needs to be configured which includes security baselines, application dependency configuration and even application deployment. The tasks are generally accomplished with a Configuration Management (CM) tool such as Puppet or Puppet Bolt.\nThe Puppet Bolt Terraform inventory plugin parses the Terraform state for resources that have been created by Terraform and it enables Bolt to be run against infrastructure created by Terraform without explicitly specifying connection information such as the IP address. This is especially useful in public cloud environments where the IP address associated with an instance is often dynamic. The Puppet Bolt Terraform plugin supports state retrieval for local state backends as well as remote backends.\nIn this blog post we\u0026rsquo;ll look at how to use the Puppet Bolt Terraform inventory plugin to dynamically retrieve the IP address of a virtual machine provisioned in a VMware vSphere environment which will allow us to run Puppet Bolt against the virtual machine using dynamic information.\nTerraform Manifest The Terraform code creates a single CentOS 7 virtual machine in a VMware vSphere environment. The Terraform state is stored in HashiCorp Consul for centralized state management.\nterraform {  backend \u0026#34;consul\u0026#34; {  address = \u0026#34;10.0.0.6:8500\u0026#34;  scheme = \u0026#34;http\u0026#34;  path = \u0026#34;terraform/bolt/boltserver/terraform.tfstate\u0026#34;  datacenter = \u0026#34;puppet-bolt\u0026#34;  } }  resource \u0026#34;vsphere_virtual_machine\u0026#34; \u0026#34;boltserver\u0026#34; {  name = \u0026#34;boltserver\u0026#34;  num_cpus = 2  memory = 4096   resource_pool_id = \u0026#34;${data.vsphere_compute_cluster.cluster.resource_pool_id}\u0026#34;  datastore_id = \u0026#34;${data.vsphere_datastore.datastore.id}\u0026#34;   guest_id = \u0026#34;${data.vsphere_virtual_machine.template.guest_id}\u0026#34;  scsi_type = \u0026#34;${data.vsphere_virtual_machine.template.scsi_type}\u0026#34;   network_interface {  network_id = \u0026#34;${data.vsphere_network.network.id}\u0026#34;  adapter_type = \u0026#34;${data.vsphere_virtual_machine.template.network_interface_types[0]}\u0026#34;  }   disk {  label = \u0026#34;disk0\u0026#34;  size = \u0026#34;${data.vsphere_virtual_machine.template.disks.0.size}\u0026#34;  eagerly_scrub = \u0026#34;${data.vsphere_virtual_machine.template.disks.0.eagerly_scrub}\u0026#34;  thin_provisioned = \u0026#34;${data.vsphere_virtual_machine.template.disks.0.thin_provisioned}\u0026#34;  }   clone {  template_uuid = \u0026#34;${data.vsphere_virtual_machine.template.id}\u0026#34;   customize {  linux_options {  host_name = \u0026#34;boltserver\u0026#34;  domain = \u0026#34;grt.local\u0026#34;  }   network_interface {}  dns_server_list = [\u0026#34;10.0.0.200\u0026#34;]  ipv4_gateway = \u0026#34;10.0.0.1\u0026#34;  }  } } The virtual machine can be provisioned using the standard Terraform workflow of init, plan and apply.\nPuppet Bolt Inventory The Puppet Bolt inventory file defines resource information such as how to the connect to the resource such as the IP address and credentials for the resource. With the virtual machine provisioned the Bolt inventory file needs to be created to dynamically access the virtual machine.\nDisplayed below is the Puppet Bolt inventory file used in this example.\nversion: 2 groups:  – name: boltservers  targets:  – _plugin: terraform  resource_type: vsphere_virtual_machine.boltserver  uri: default_ip_address  dir: .  name: name  backend: remote Inventory Properties The Terraform plugin accepts a number of parameters for interacting with the Terraform state.\nresource_type: The Terraform resource that should be matched with support for regular expressions (i.e. - resource_type.resource_name).\nuri: The property of the Terraform resource used to connect to the resource such as the IP address or DNS name.\ndir: The inventory plugin executes the Terraform state pull command to retrieve the state. The directory specified should be where the Terraform manifests for the resources resided.\nname: The property of the Terraform resource to use as the target name\nbackend: The type of backend to load the Terraform state from. The two supported types are local and remote.\nPuppet Bolt Plan With the resource provisioned and the inventory file configured Puppet Bolt can be run against the provisioned resource without explicitly defining the IP address or DNS name.\nThe command below runs Puppet Bolt against the virtual machine provisioned with Terraform. In this example we\u0026rsquo;re just using the facts plan that comes with Bolt to show the functionality but a much more complex plan could be used to deploy an application or configure the security baseline as an example.\nbolt plan run facts -i inventory.yaml nodes=boltservers The command generates output similar to that shown below which dynamically retrieved the IP address to access the virtual machine.\nbolt plan run facts -i inventory.yaml nodes=boltservers Starting: plan facts Starting: task facts on 10.0.0.15 Finished: task facts with 0 failures in 5.94 sec Finished: plan facts in 5.97 sec Finished on 10.0.0.15:  {  \u0026#34;os\u0026#34;: {  \u0026#34;name\u0026#34;: \u0026#34;CentOS\u0026#34;,  \u0026#34;distro\u0026#34;: {  \u0026#34;codename\u0026#34;: \u0026#34;Core \u0026#34;  },  \u0026#34;release\u0026#34;: {  \u0026#34;full\u0026#34;: \u0026#34;7.6.1810\u0026#34;,  \u0026#34;major\u0026#34;: \u0026#34;7\u0026#34;,  \u0026#34;minor\u0026#34;: \u0026#34;6\u0026#34;  },  \u0026#34;family\u0026#34;: \u0026#34;RedHat\u0026#34;  }  } Successful on 1 node: boltserver Ran on 1 node This integration greatly simplifies configuring machines provisioned with Terraform and eliminates the need to specify the IP address of virtual machine before running Puppet Bolt. This plugin can be especially useful in deploying complex application stacks with dependencies between machines.\nReference Puppet Bolt Inventory Plugin\nhttps://puppet.com/docs/bolt/latest/inventory_file_v2.html#plugins-and-dynamic-inventory\n","date":"06 Oct, 2019","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cpicture\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_545x0_resize_q95_h2_box.webp\" media=\"(max-width: 575px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_600x0_resize_q95_h2_box.webp\" media=\"(max-width: 767px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_700x0_resize_q95_h2_box.webp\" media=\"(max-width: 991px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_h2_box.webp\"\u003e\n  \u003cimg loading=\"lazy\" decoding=\"async\" class=\"w-100 img-fluid rounded\" src=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_box.jpg\" alt=\"\" width=\"970\" height=\"500\"\u003e\n\u003c/picture\u003e\n \n \n \n\n","permalink":"https://www.greenreedtech.com/puppet-bolt-terraform-inventory-plugin/","tags":["DevOps","Puppet","Terraform"],"title":"Puppet Bolt Terraform Inventory Plugin"},{"categories":["Packer"],"contents":"Treating workloads as \u0026ldquo;cattle\u0026rdquo; or immutable is a popular management paradigm for stateless workloads and is especially prevalent for such workloads that are hosted in a public cloud. The concept is based upon the notion that servers are pre-baked with all of the software that is needed for the application and any stateful data is pushed to an external persistent storage mechanism such as an object storage or a message queue. There\u0026rsquo;s a number of methods for accomplishing the pre-baking that includes the configuration and installation of software on the templates that will be used for immutable servers.\nIn this post we’ll use HashiCorp Packer and Puppet Bolt to create a VMware vSphere template that contains a specific version of HashiCorp Consul that we can use for immutable upgrades or potential rollbacks. The plugin also works for other Packer builders but vSphere will be covered in this post.\nPuppet Bolt Puppet Bolt will be used in this example to enable the creation of our pre-baked template without the need for a Puppet master. To simplify the integration between Packer and Puppet Bolt we\u0026rsquo;ll use an unofficial Packer Puppet Bolt provisioner (https://github.com/martezr/packer-provisioner-puppet-bolt) that I\u0026rsquo;ve developed. We\u0026rsquo;ll take a look at the integration once we get to the HashiCorp Packer section of this blog post but first we need to set up Bolt to enable us to install Consul on the template.\nIn our example we\u0026rsquo;ll use the Consul module available on the Puppet forge (https://forge.puppet.com/KyleAnderson/consul) to install HashiCorp Consul. Since we\u0026rsquo;ll be using the module we need to add it to the Puppetfile we\u0026rsquo;re using for Puppet Bolt along with the module it requires.\nBolt Configuration The following steps will install and configure Puppet Bolt.\nInstall the Puppet Bolt repo\nAdd the Puppet repo to the system, install bolt and create the Bolt directory.\nsudo rpm -Uvh https://yum.puppet.com/puppet6/puppet6-release-el-7.noarch.rpm sudo yum install -y puppet-bolt mkdir -p ~/.puppetlabs/bolt/ Create Puppet Bolt Configuration File\nThis step creates a Puppet Bolt global configuration file that the Terraform Puppet provisioner uses for the Bolt process.\ncat \u0026lt;\u0026lt; EOF \u0026gt; ~/.puppetlabs/bolt/bolt.yaml modulepath: \u0026#34;~/.puppetlabs/bolt-code/modules:~/.puppetlabs/bolt-code/site-modules\u0026#34; inventoryfile: \u0026#34;~/.puppetlabs/bolt/inventory.yaml\u0026#34; concurrency: 10 format: human ssh: host-key-check: false user: root EOF Create Puppet Bolt Inventory File\nThe Terraform Puppet provisioner targets the Puppet master specified in the Terraform code but Bolt needs additional information for connecting to the Puppet master such as user credentials.\ncat \u0026lt;\u0026lt; EOF \u0026gt; ~/.puppetlabs/bolt/inventory.yaml --- config: ssh: host-key-check: false user: root EOF Bolt Puppetfile\nThis step creates a Bolt Puppetfile for installing the two Puppet modules required by the Terraform Puppet provisioner.\nModules from the Puppet Forge. mod \u0026lsquo;KyleAnderson-consul\u0026rsquo;, \u0026lsquo;5.1.0\u0026rsquo; mod \u0026lsquo;puppetlabs-stdlib\u0026rsquo;, \u0026lsquo;5.2.0\u0026rsquo; mod \u0026lsquo;puppet-archive\u0026rsquo;, \u0026lsquo;3.2.1\u0026rsquo; mod \u0026lsquo;camptocamp-systemd\u0026rsquo;, \u0026lsquo;2.6.0\u0026rsquo; mod \u0026lsquo;puppetlabs-powershell\u0026rsquo;, \u0026lsquo;2.3.0\u0026rsquo;\nBolt Plan\nWe\u0026rsquo;re going to create a custom plan for installing HashiCorp Consul using the module from the forge. The first thing we need to do is create the module structure in our Bolt directory.\nmkdir -p ~/.puppetlabs/bolt-code/site-modules/bolt_consul/plans With the directory structure in place we just need to create our Puppet manifest file (install.pp) in the \u0026ldquo;plans\u0026rdquo; directory with the code below.\nplan bolt_consul::install( TargetSpec $nodes, String $version,) {# Ensure puppet tools are installed and gather facts for the apply apply_prep([$nodes]) apply($nodes) { package { \u0026#39;unzip\u0026#39;: ensure =\u0026gt; present, } class { \u0026#39;::consul\u0026#39;: version =\u0026gt; $version, config_hash =\u0026gt; { \u0026#39;bootstrap_expect\u0026#39; =\u0026gt; 1, \u0026#39;client_addr\u0026#39; =\u0026gt; \u0026#39;0.0.0.0\u0026#39;, \u0026#39;data_dir\u0026#39; =\u0026gt; \u0026#39;/opt/consul\u0026#39;, \u0026#39;datacenter\u0026#39; =\u0026gt; \u0026#39;east-aws\u0026#39;, \u0026#39;log_level\u0026#39; =\u0026gt; \u0026#39;INFO\u0026#39;, \u0026#39;node_name\u0026#39; =\u0026gt; \u0026#39;server\u0026#39;, \u0026#39;server\u0026#39; =\u0026gt; true, \u0026#39;ui\u0026#39; =\u0026gt; true, } } $puppet_packages = [ \u0026#39;puppet-agent\u0026#39;, \u0026#39;puppet-release\u0026#39; ] package { $puppet_packages: ensure =\u0026gt; absent, } }}Once the manifest has been created the Bolt directory structure should look similar to that below.\n[root@centos7base ~]# tree -L 3 ~/.puppetlabs/ /root/.puppetlabs/ ├── bolt │ ├── analytics.yaml │ ├── bolt.yaml │ ├── inventory.yaml │ └── Puppetfile └── bolt-code  ├── modules  │ ├── archive  │ ├── consul  │ ├── powershell  │ ├── stdlib  │ └── systemd  └── site-modules  └── bolt_consul  10 directories, 4 files HashiCorp Packer In this example we\u0026rsquo;ll use an existing vSphere template as our base template. The template only contains standard OS configurations and we\u0026rsquo;ll use the combination of Packer and Bolt to create our Consul specific template. The following are the tasks we\u0026rsquo;ll need to accomplish to setup Packer.\n Download the latest version of HashiCorp Packer from releases.hashicorp.com Download the latest version of the Jetbrains vSphere Packer plugin Download the latest version of the Puppet Bolt Packer provisioner plugin Create Packer template JSON file  Install HashiCorp Packer\nwget https://releases.hashicorp.com/packer/1.4.2/packer_1.4.2_linux_amd64.zip \u0026amp;\u0026amp; unzip packer_1.4.2_linux_amd64.zip \u0026amp;\u0026amp; chmod +x packer Install the Puppet Bolt Packer provisioner plugin\nwget https://github.com/martezr/packer-provisioner-puppet-bolt/releases/download/v0.1.0/packer-provisioner-puppet-bolt_0.1.0_Linux_x86_64.tar.gz \u0026amp;\u0026amp; tar -xzf packer-provisioner-puppet-bolt_0.1.0_Linux_x86_64.tar.gz Install the Jetbrains Packer vSphere builder plugin\nwget https://github.com/jetbrains-infra/packer-builder-vsphere/releases/download/v2.3/packer-builder-vsphere-clone.linux Packer Template\nThe Packer template below shows the integration provided by the Puppet Bolt plugin.\n{  \u0026#34;variables\u0026#34;: {  \u0026#34;vsphere_password\u0026#34;: \u0026#34;\u0026#34;,  \u0026#34;ssh_password\u0026#34;: \u0026#34;\u0026#34;  },  \u0026#34;builders\u0026#34;: [  {  \u0026#34;type\u0026#34;: \u0026#34;vsphere-clone\u0026#34;,  \u0026#34;vcenter_server\u0026#34;: \u0026#34;10.0.0.205\u0026#34;,  \u0026#34;username\u0026#34;: \u0026#34;administrator@vsphere.local\u0026#34;,  \u0026#34;password\u0026#34;: \u0026#34;{{user `vsphere_password`}}\u0026#34;,  \u0026#34;insecure_connection\u0026#34;: \u0026#34;true\u0026#34;,  \u0026#34;template\u0026#34;: \u0026#34;centos7base\u0026#34;,  \u0026#34;vm_name\u0026#34;: \u0026#34;boltpackdemo\u0026#34;,  \u0026#34;cluster\u0026#34;: \u0026#34;GRT-Cluster\u0026#34;,  \u0026#34;host\u0026#34;: \u0026#34;10.0.0.246\u0026#34;,  \u0026#34;communicator\u0026#34;: \u0026#34;ssh\u0026#34;,  \u0026#34;ssh_username\u0026#34;: \u0026#34;root\u0026#34;,  \u0026#34;ssh_password\u0026#34;: \u0026#34;{{user `ssh_password`}}\u0026#34;  }  ],  \u0026#34;provisioners\u0026#34;: [  {  \u0026#34;type\u0026#34;: \u0026#34;puppet-bolt\u0026#34;,  \u0026#34;user\u0026#34;: \u0026#34;root\u0026#34;,  \u0026#34;bolt_plan\u0026#34;: \u0026#34;bolt_consul::install\u0026#34;,  \u0026#34;bolt_params\u0026#34;: {  \u0026#34;version\u0026#34;: \u0026#34;1.5.3\u0026#34;  }  }  ] } With the Packer template file create we just need to run the build command to provision the vSphere template using Packer.\n./packer build -var \u0026#39;vsphere_password=password\u0026#39; -var \u0026#39;ssh_password=password\u0026#39; bolt.json The build should produce output similar to that below and shows how Bolt is being executed against the template machine.\nOnce the build process has been completed we should have a pre-baked HashiCorp Consul server. We can provision a new virtual machine based upon the template and see that consul is already installed by browsing to the Consul web interface (http://ip_address:8500) in a web browser.\nThe provisioner plugin currently only supports Linux workloads and Windows support will be added in a future release.\nReferences Consul Puppet Module\nhttps://forge.puppet.com/KyleAnderson/consul\nPacker Puppet Bolt Provisioner Plugin\nhttps://github.com/martezr/packer-provisioner-puppet-bolt\nJetbrains Packer vSphere Builder Plugin\nhttps://github.com/jetbrains-infra/packer-builder-vsphere\n","date":"14 Aug, 2019","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cpicture\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_545x0_resize_q95_h2_box.webp\" media=\"(max-width: 575px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_600x0_resize_q95_h2_box.webp\" media=\"(max-width: 767px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_700x0_resize_q95_h2_box.webp\" media=\"(max-width: 991px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_h2_box.webp\"\u003e\n  \u003cimg loading=\"lazy\" decoding=\"async\" class=\"w-100 img-fluid rounded\" src=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_box.jpg\" alt=\"\" width=\"970\" height=\"500\"\u003e\n\u003c/picture\u003e\n \n \n \n\n","permalink":"https://www.greenreedtech.com/hashicorp-packer-puppet-bolt-provisioner/","tags":["HashiCorp","Puppet","Bolt","Packer"],"title":"HashiCorp Packer Puppet Bolt Provisioner"},{"categories":["StackStorm"],"contents":"Template management is a critical facet of infrastructure management and traditionally one of the more challenging operations there is. The advent of tools like HashiCorp Packer have provided administrators with the ability declaratively automate the template creation process.\nAutomation is great but in most organizations a single piece of automation is part of a larger process that includes tickets and potentially other hand offs.\nIn this example we\u0026rsquo;ll walk through creating a process for creating VMware vSphere machine templates in a fully automated fashion with deep integrations with other tools for documenting and testing the created template.\nThe code for this example can be found in the following GitHub repositories.\nThe StackStorm pack that contains the orchestration workflow: https://github.com/martezr/stackstorm-grtlab\nThe Terraform and Packer code: https://github.com/martezr/orchestration-demos\nTools The orchestration detailed in this example contains a number of tools that are covered below.\nStackStorm StackStorm is an open source orchestration tool with numerous integrations with third-party tools for simplifying the creation of complex orchestration workflows. StackStorm will drive the orchestration in this example and handle all of the hand offs.\nGithub Github is used for storing all of the code used in this example and is where StackStorm will fetch code from.\nServiceNow ServiceNow is an ITSM or IT Service Management platform that we\u0026rsquo;ll use for incident management and as a CMDB in our example.\nJenkins Jenkins is a continuous integration (CI) server typically used for testing and building code. In our example Jenkins will be used for validating our deployed code. This shows how the workflow could incorporate tests that may already exist that are created by the applications developers or QA team. This helps to reduce potential issues created by base template updates\nServerSpec ServerSpec is a Ruby based testing framework that I being used for acceptance testing in our example. A few simple tests are being run but it shows how the integration can provide tremendous value.\nHashiCorp Packer Packer is a tool for defining a template in a declarative manner via a JSON file.\nHashiCorp Terraform Terraform is used to provision the virtual machine that is used for validating that the new template doesn\u0026rsquo;t break any downstream applications.\nStages The high level stages of the orchestration displayed in the image below are detailed below along with the StackStorm code for each stage.\nCreate Incident This first stage of the workflow is to create an incident in ServiceNow for the new template. The goal of this stage is integrating with ServiceNow to provide a record of what is being done as part of the automated template creation process. The StackStorm code below shows that we\u0026rsquo;re creating a new record in the incident table and providing some metadata about the incident such as the description and the submitter.\nOnce the record has been created in the incident table we are storing the sysid of the ServiceNow incident in the incident_id variable for use later when we resolve the incident. Additionally we\u0026rsquo;re specifying the next stage of the workflow which cleaning up any existing clones of our code git repo.\ncreate_incident:  action: servicenow.create_record  input:  table: \u0026#34;incident\u0026#34;  payload: {\u0026#34;short_description\u0026#34;:\u0026#34;CentOS 7 Monthly Image Update\u0026#34;,\u0026#34;category\u0026#34;:\u0026#34;software\u0026#34;,\u0026#34;description\u0026#34;:\u0026#34;Updating CentOS 7 template\u0026#34;,\u0026#34;caller_id\u0026#34;:\u0026#34;stackstorm\u0026#34;}  next:  - when: \u0026lt;% succeeded() %\u0026gt;  publish: incident_id=\u0026lt;% result().result.sys_id %\u0026gt;  do: cleanup_git_repo The screenshot below displays a new incident created by the orchestration to track the work performed during this process and align with change management processes.\nClone Github Repo The second stage of the workflow cleans up any existing copy of the Github repository and clones the Github repository that contains the Packer template code as well as the Terraform code used for testing the newly created template.\ncleanup_git_repo:  action: linux.rm  input:  target: /stackstorm/orchestration-demos  recursive: True  sudo: True  hosts: localhost  next:  - when: \u0026lt;% succeeded() %\u0026gt;  do: clone_build_repo Once the local copy of the repo is removed then a clone operation is performed.\nclone_build_repo:  action: git.clone  input:  source: https://github.com/martezr/orchestration-demos.git  destination: /stackstorm/orchestration-demos  hosts: localhost  next:  - when: \u0026lt;% succeeded() %\u0026gt;  do: create_template Build Template The third stage of the workflow builds the VMware vSphere template from an ISO image using HashiCorp Packer and the Jetbrains vSphere Packer plugin.\nThe Packer JSON file is hosted in the cloned GitHub repository and the variable file that contains sensitive information is stored on the filesystem of the StackStorm host. This data could be retrieved in a more secure fashion such as retrieved from HashiCorp Vault or StackStorm\u0026rsquo;s encrypted data storage.\ncreate_template:  action: packer.build  input:  packerfile: /stackstorm/orchestration-demos/packer/centos7base.json  variables_file: /stackstorm/packer-vars.json  next:  - when: \u0026lt;% succeeded() %\u0026gt;  do: provision_test_stack Provision Test Stack The fourth stage of the workflow provisions a VMware vSphere virtual machine based on the previously created template and configures it using Puppet via the Terraform Puppet Provisioner.\nIn a real world example this might be an entire application stack owned by another team that consumes the template. This provides tighter integration to help prevent breaking changes from being propogated.\nprovision_test_stack:  action: terraform.pipeline  input:  variable_files: [\u0026#34;/stackstorm/vsphere.auto.tfvars\u0026#34;]  plan_path: /stackstorm/orchestration-demos/terraform/teststack  backend: {\u0026#34;path\u0026#34;:\u0026#34;/stackstorm/terraform.tfstate\u0026#34;}  next:  - when: \u0026lt;% succeeded() %\u0026gt;  do: output_test_stack_info Validate Test Stack The fifth stage of the workflow is run some validation tests against the stack. These tests might be tests provided by teams that own applications that are built on the base template. In this example StackStorm executes a defined job in Jenkins that runs some ServerSpec tests.\nvalidate_test_stack:  action: jenkins.build_job_wait  input:  project: \u0026#34;validate_teststack\u0026#34;  parameters: {\u0026#34;target\u0026#34;:\u0026#34;\u0026lt;% ctx().ip_address %\u0026gt;\u0026#34;}  next:  - when: \u0026lt;% succeeded() %\u0026gt;  do: destroy_test_stack  - when: \u0026lt;% failed() %\u0026gt;  do: destroy_test_stack The screenshot below displays the console output of the Jenkins job that was run to perform the validation of the test virtual machine provisioned by Terraform.\nDestroy Test Stack The sixth stage of the workflow is to tear down the virtual machine used for testing the updates to the base template. Since Terraform was used to provision the virtual machine we\u0026rsquo;ll just run a Terraform Destroy to tear it down.\ndestroy_test_stack:  action: terraform.destroy  input:  variable_files: [\u0026#34;/stackstorm/vsphere.auto.tfvars\u0026#34;]  plan_path: /stackstorm/orchestration-demos/terraform/teststack  state_file_path: \u0026#34;/stackstorm/terraform.tfstate\u0026#34;  next:  - when: \u0026lt;% succeeded() %\u0026gt;  do: create_cmdb_record Fetch Template Metadata The seventh stage of the workflow is to fetch the metadata for the template such as the hardware specifications to input them into the CMDB in ServiceNow. The Packer JSON is just being outputted via the cat command but a custom script for parsing file or fetching the data directly from vCenter would provide a more robust solution. The output is being stored in the template_data variable.\nget_template_data:  action: core.local  input:  cmd: cat /stackstorm/orchestration-demos/packer/centos7base.json  next:  - when: \u0026lt;% succeeded() %\u0026gt;  publish: template_data=\u0026lt;% result().stdout.builders[0] %\u0026gt;  do: create_cmdb_record Create CMDB Entry The eighth stage of the workflow is creating a record in the CMDB for the new vSphere template. The payload contains metadata about the template that we defined in the previous stage and made available to this stage with the publish declaration.\ncreate_cmdb_record:  action: servicenow.create_record  input:  payload: {\u0026#34;name\u0026#34;:\u0026#34;\u0026lt;% ctx().template_name %\u0026gt;\u0026#34;,\u0026#34;description\u0026#34;:\u0026#34;CentOS 7 Demo Template\u0026#34;,\u0026#34;guest_os\u0026#34;:\u0026#34;\u0026lt;% ctx().template_data.guest_os_type %\u0026gt;\u0026#34;,\u0026#34;cpus\u0026#34;:\u0026#34;\u0026lt;% ctx().template_data.CPUs %\u0026gt;\u0026#34;,\u0026#34;memory\u0026#34;:\u0026#34;\u0026lt;% ctx().template_data.RAM %\u0026gt;\u0026#34;,\u0026#34;disk_size\u0026#34;:\u0026#34;\u0026lt;% ctx().template_data.disk_size %\u0026gt;\u0026#34;,\u0026#34;object_id\u0026#34;:\u0026#34;1\u0026#34;}  table: \u0026#34;cmdb_ci_vmware_template\u0026#34;  next:  - when: \u0026lt;% succeeded() %\u0026gt;  do: close_incident The screenshot below displays the CMDB record for the template created by the orchestration.\nClose Incident The nineth and final stage of the workflow is to close or resolve the incident in ServiceNow. This stage closes the loop on the change in ServiceNow with the template successfully being created. In the example we\u0026rsquo;re passing the incident_id variable that was returned from the very first stage in the workflow along with providing the state of \u0026ldquo;6\u0026rdquo; which is resolved and the required fields of code and notes.\nclose_incident:  action: servicenow.update  input:  table: \u0026#34;incident\u0026#34;  sysid: \u0026lt;% ctx().incident_id %\u0026gt;  payload: {\u0026#34;state\u0026#34;:\u0026#34;6\u0026#34;, \u0026#34;close_code\u0026#34;: \u0026#34;Solved (Permanently)\u0026#34;, \u0026#34;close_notes\u0026#34;: \u0026#34;Created \u0026lt;% ctx().template_name %\u0026gt; template\u0026#34;} The screenshot below displays the incident created by the first stage of the orchestration now in a \u0026ldquo;resolved\u0026rdquo; state due to closing out the incident as part of the orchestration.\nThis example could be extended much further to include additional systems and stages.\nThis example shows just some of the possibilities of creating advanced orchestration with a tool like StackStorm.\n","date":"06 Aug, 2019","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cpicture\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_545x0_resize_q95_h2_box.webp\" media=\"(max-width: 575px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_600x0_resize_q95_h2_box.webp\" media=\"(max-width: 767px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_700x0_resize_q95_h2_box.webp\" media=\"(max-width: 991px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_h2_box.webp\"\u003e\n  \u003cimg loading=\"lazy\" decoding=\"async\" class=\"w-100 img-fluid rounded\" src=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_box.jpg\" alt=\"\" width=\"970\" height=\"500\"\u003e\n\u003c/picture\u003e\n \n \n \n\n","permalink":"https://www.greenreedtech.com/advanced-vmware-vsphere-template-orchestration/","tags":["StackStorm","VMware","DevOps"],"title":"Advanced VMware vSphere Template Orchestration"},{"categories":["Puppet","Terraform"],"contents":"HashiCorp Terraform 0.12.2 added official support for a Puppet provisioner. One caveat is that the provisioner is only available in 0.12.x of Terraform. The provisioner provides a number of features such as adding data to the CSR for trusted facts, selecting between open source and enterprise agent versions along with autosigning the CSR.\nIn this post we\u0026rsquo;ll use the Terraform vSphere provisioner to provision a CentOS virtual machine. The provisioner autosign feature will be used for automatically signing the CSR on the Puppet master.\nWorkstation Setup The first thing we to do is setup our machine to deploy the virtual machine. Terraform and Puppet Bolt are what is needed for the setup.\nTerraform Installation HashiCorp Terraform is a Golang binary that just needs to be downloaded and added to the path on the system. The latest version of HashiCorp Terraform can be downloaded from the following link.\nhttps://www.terraform.io/downloads.html\nPuppet Bolt Installation Puppet Bolt is used by the Puppet provisioner to bootstrap the Puppet agent on the virtual machine being provisioned.\nInstall the Puppet Bolt repo\nAdd the Puppet repo to the system\nsudo rpm -Uvh https://yum.puppet.com/puppet6/puppet6-release-el-7.noarch.rpm Install Puppet Bolt\nInstall Puppet Bolt using yum\nsudo yum install -y puppet-bolt Create Bolt Directory\nCreate a directory for the Puppet Bolt configuration\nmkdir -p ~/.puppetlabs/bolt/ Create Puppet Bolt Configuration File\nThis step creates a Puppet Bolt global configuration file that the Terraform Puppet provisioner uses for the Bolt process.\ncat \u0026lt;\u0026lt; EOF \u0026gt; ~/.puppetlabs/bolt/bolt.yaml modulepath: \u0026#34;~/.puppetlabs/bolt-code/modules:~/.puppetlabs/bolt-code/site-modules\u0026#34; inventoryfile: \u0026#34;~/.puppetlabs/bolt/inventory.yaml\u0026#34; concurrency: 10 format: human ssh: host-key-check: false user: root private-key: ~/.ssh/bolt_id EOF Create Puppet Bolt Inventory File\nThe Terraform Puppet provisioner targets the Puppet master specified in the Terraform code but Bolt needs additional information for connecting to the Puppet master such as user credentials.\ncat \u0026lt;\u0026lt; EOF \u0026gt; ~/.puppetlabs/bolt/inventory.yaml --- config: ssh: host-key-check: false user: root private-key: ~/.ssh/bolt_id EOF Create Bolt Puppetfile\nThis step creates a Bolt Puppetfile for installing the two Puppet modules required by the Terraform Puppet provisioner.\ncat \u0026lt;\u0026lt; EOF \u0026gt; ~/.puppetlabs/bolt/Puppetfile # Modules from the Puppet Forge. mod \u0026#39;danieldreier/autosign\u0026#39; mod \u0026#39;puppetlabs/puppet_agent\u0026#39; EOF Install Bolt Puppet Modules\nThis step installs the Puppet modules specified in the Puppetfile from the previous step.\nbolt puppetfile install Puppet Master Configuration With the workstation configured we now need to configure the Puppet master to support the policy based autosign method supported by the Terraform Puppet provisioner.\nInstall the autosign ruby gem\nThis step installs the autosign (https://github.com/danieldreier/autosign) gem that our Puppet master will use for signing the certificate request.\n/opt/puppetlabs/puppet/bin/gem install autosign Create a new directory for autosign\nCreate a directory for the autosign installation.\nmkdir /var/autosign Update permissions on the autosign directory\nThe permissions for the autosign directory need to be updated to allow the Puppet server service to access the directory.\nchown pe-puppet:pe-puppet /var/autosign chmod 750 /var/autosign Create autosign log file\nCreate a log file for the autosign binary to log events.\ntouch /var/log/autosign.log Update permissions on the autosign log file\nThe permissions on the autosign log file need to be updated to allow it to be written to when the Puppet server attempts to autosign the CSR.\nchown pe-puppet:pe-puppet /var/log/autosign.log Setup the autosign gem\n/opt/puppetlabs/puppet/bin/autosign config setup The autosign setup creates a configuration file \u0026ldquo;/etc/autosign.conf\u0026rdquo; that needs to be modified. We need to ensure that the journalfile is configured for \u0026ldquo;/var/autosign/autosign.journal\u0026rdquo;.\n--- general:  loglevel: warn  logfile: \u0026#34;/var/log/autosign.log\u0026#34; jwt_token:  secret: 1s79TOyqMHw3zfKhE1h+1feKMaE=  validity: \u0026#39;7200\u0026#39;  journalfile: \u0026#34;/var/autosign/autosign.journal\u0026#34; The CSR will contain an invalid challenge password if the loglevel is set to debug\nWith the loglevel set to debug additional debug lines will be added to the challenge password in the CSR which causes the autosign process to fail.\nSet Puppet master autosigning\nThe Puppet master configuration file needs to be updated to use the autosign gem as the executable/binary to run when validating CSRs.\npuppet config set --section master autosign /opt/puppetlabs/puppet/bin/autosign-validator Restart the Puppet Server Service\nRestart the Puppet server service for the changes to take effect.\nsystemctl restart pe-puppetserver The Puppet master is now configured for autosigning certificate requests from the Puppet provisioner.\nTerraform With all of the setup complete we can focus on the Terraform code for provisioning our CentOS 7 virtual machine and using the new Puppet provisioner to integrate it with our Puppet master.\nThe Puppet provisioner resource block includes a number of configuration options that are covered in greater detail in the documentation but we\u0026rsquo;ll look at the ones that were used in this example.\nserver: The FQDN or IP address of the Puppet master\nserver_user: The name of the user on the Puppet master that the provisioner will connect as\nautosign: The Puppet provisioner includes code that leverages Puppet Bolt as well as policy based autosigning to automatically sign the nodes SSL certificate on the Puppet master\nopen_source: Whether to use the open source version of the Puppet agent or the enterprise version\ncertname: The CN for the agent\u0026rsquo;s SSL certificate\nextension_requests: A map of extension requests to be embedded in the certificate signing request before it is sent to the Puppet master CA. In our example we\u0026rsquo;re passing data to set the pp_role trusted fact that defines the role of the virtual machine.\nconnection: Standard Terraform provisioner connection details for connecting to the virtual machine via SSH or WinRM.\nExample Code The following code is the Terraform vSphere virtual machine resource along with the Puppet provisioner used in this example.\nresource \u0026#34;vsphere_virtual_machine\u0026#34; \u0026#34;puppet-demo\u0026#34; {  name = \u0026#34;puppet-demo.grt.local\u0026#34;  num_cpus = 1  memory = 4096  resource_pool_id = \u0026#34;${data.vsphere_resource_pool.pool.id}\u0026#34;  datastore_id = \u0026#34;${data.vsphere_datastore.datastore.id}\u0026#34;   guest_id = \u0026#34;${data.vsphere_virtual_machine.template.guest_id}\u0026#34;  scsi_type = \u0026#34;${data.vsphere_virtual_machine.template.scsi_type}\u0026#34;   network_interface {  network_id = \u0026#34;${data.vsphere_network.network.id}\u0026#34;  adapter_type = \u0026#34;${data.vsphere_virtual_machine.template.network_interface_types[0]}\u0026#34;  }   disk {  label = \u0026#34;disk0\u0026#34;  size = \u0026#34;60\u0026#34;  eagerly_scrub = \u0026#34;${data.vsphere_virtual_machine.template.disks.0.eagerly_scrub}\u0026#34;  thin_provisioned = \u0026#34;${data.vsphere_virtual_machine.template.disks.0.thin_provisioned}\u0026#34;  }   clone {  template_uuid = \u0026#34;${data.vsphere_virtual_machine.template.id}\u0026#34;   customize {  linux_options {  host_name = \u0026#34;puppet-demo\u0026#34;  domain = \u0026#34;grt.local\u0026#34;  }   network_interface {  ipv4_address = \u0026#34;10.0.0.30\u0026#34;  ipv4_netmask = 24  }  dns_server_list = [\u0026#34;10.0.0.200\u0026#34;]  ipv4_gateway = \u0026#34;10.0.0.1\u0026#34;  }  }   provisioner \u0026#34;puppet\u0026#34; {  server = \u0026#34;grtpemaster01.grt.local\u0026#34;  server_user = \u0026#34;root\u0026#34;  autosign = true  open_source = false  certname = \u0026#34;puppet-demo.grt.local\u0026#34;  extension_requests = {  pp_role = \u0026#34;demo\u0026#34;  }  connection {  type = \u0026#34;ssh\u0026#34;  host = \u0026#34;${self.default_ip_address}\u0026#34;  user = \u0026#34;root\u0026#34;  password = \u0026#34;${var.root_password}\u0026#34;  }  }   provisioner \u0026#34;remote-exec\u0026#34; {  when = \u0026#34;destroy\u0026#34;  inline = [  \u0026#34;puppet node purge puppet-demo.grt.local\u0026#34;,  ]  connection {  type = \u0026#34;ssh\u0026#34;  host = \u0026#34;grtpemaster01.grt.local\u0026#34;  user = \u0026#34;root\u0026#34;  password = \u0026#34;${var.root_password}\u0026#34;  }  } } Initialize Terraform\nWith the Terraform code in place we need to initialize the current working directory to download all of the necessary plugins.\nterraform init Plan Terraform Run\nNow that the Terraform has been initialized we can plan our Terraform run to see if there are any errors.\nterraform plan Apply Terraform\nThe next step once our plan was successful is to apply our Terraform code and validate that the Puppet provisioner worked.\nterraform apply --auto-approve Once the Terraform apply has completed successfully we should be able to see the new node in the Puppet Enterprise console.\nAdditionally we can see in the facts for the node that the \u0026ldquo;demo\u0026rdquo; role has been set for the pp_role fact we designated in our Terraform code.\nReferences Terraform Puppet Provisioner\nhttps://www.terraform.io/docs/provisioners/puppet.html\nDaniel Dreier Autosign Gem\nhttps://github.com/danieldreier/autosign\n","date":"10 Jul, 2019","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cpicture\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_545x0_resize_q95_h2_box.webp\" media=\"(max-width: 575px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_600x0_resize_q95_h2_box.webp\" media=\"(max-width: 767px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_700x0_resize_q95_h2_box.webp\" media=\"(max-width: 991px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_h2_box.webp\"\u003e\n  \u003cimg loading=\"lazy\" decoding=\"async\" class=\"w-100 img-fluid rounded\" src=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_box.jpg\" alt=\"\" width=\"970\" height=\"500\"\u003e\n\u003c/picture\u003e\n \n \n \n\n","permalink":"https://www.greenreedtech.com/terraform-puppet-provisioner/","tags":["Puppet","Terraform","HashiCorp","DevOps"],"title":"Terraform Puppet Provisioner"},{"categories":["Career"],"contents":"I\u0026rsquo;m now in my second week at Puppet which is best known for its configuration management software that shares the same name as the company. I\u0026rsquo;m extremely excited about this new chapter in my professional life and this blog post covers some of the reasons why. Hopefully this will provide anyone reading this with helpful information as they come to an inflection point in their career. I decided to use a number of \u0026ldquo;P\u0026rdquo; words to describe what this transition is for me.\nPassion The first \u0026ldquo;P\u0026rdquo; is passion. The majority of my career has been focused on client facing consulting. Working for a software vendor is a first for me. While I am still very passionate about helping clients achieve their business objectives through the use of technology, I\u0026rsquo;m really looking forward to accomplishing that through the development of high quality training content. I really love to share the knowledge that I\u0026rsquo;ve gained with others.\nPosition What will I be doing at Puppet? The position I have accepted at Puppet is that of a Principal Field Solutions Developer with the education team. What that means for me is that I get to help with things related to the training and enablement Puppet provides to customers as well as internal Puppet resources. The most amazing part for me is that this includes still being very technical with solution development of the hands on technical training but also the non hands on aspects like documentation and content development.\nPortfolio Puppet is most known for its place within the configuration management space that it occupies with Chef, Ansible and a number of other products. In recent years through acquisition and a number of internally built products Puppet has added other platforms and tools such as Bolt, Continuous Delivery for Puppet Enterprise, and others to its portfolio. Trying to create something out of nothing is challenging but it can also be very rewarding to be part of that process.\nPortland Puppet is headquartered in Portland, Oregon and is where I spent my first week with the company. Since completing my orientation I\u0026rsquo;ve been working with my distributed team in a remote fashion. I truly love the ability to work remotely and with those that understand some of the challenges of a distributed team. While I\u0026rsquo;ve worked remotely for an extended period of time in the past I have a greater awareness of some of the difficulties from a professional and personal standpoint. One of the things the team and the broader organization are big on is video calls to help \u0026ldquo;connect\u0026rdquo; people.\nProfile My background is in IT operations or system administration. My skillset spans numerous technologies from networking to storage to server and desktop virtualization to security and even the gamut of \u0026ldquo;DevOps\u0026rdquo; tools. Most of my career has been spent working with a lot of different technologies on a daily basis. One of the initial thoughts about moving to a vendor or product centric company was potentially limiting future opportunities due to focusing on a narrow set of tools. I honestly believe that it actually give an amazing opportunity to learn aspects of the IT business I would never get otherwise along with a different perspective. Ultimately my most valuable skill is an ability to learn so I feel really comfortable with my decision.\nPuppet The last \u0026ldquo;P\u0026rdquo; is Puppet and more of the reason why I chose to work for Puppet. Puppet was the first tool that really propelled me down the whole \u0026ldquo;DevOps\u0026rdquo; path and is a tool that I\u0026rsquo;ve spent quite a bit of time learning and using. In a strange sort of symbolic way I feel like I\u0026rsquo;m going through a similar journey to Puppet in my professional career. Having had quite a bit of success and now to continue to become better in ways that were previously unexplored.\n","date":"26 Jun, 2019","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cpicture\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_545x0_resize_q95_h2_box.webp\" media=\"(max-width: 575px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_600x0_resize_q95_h2_box.webp\" media=\"(max-width: 767px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_700x0_resize_q95_h2_box.webp\" media=\"(max-width: 991px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_h2_box.webp\"\u003e\n  \u003cimg loading=\"lazy\" decoding=\"async\" class=\"w-100 img-fluid rounded\" src=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_box.jpg\" alt=\"\" width=\"970\" height=\"500\"\u003e\n\u003c/picture\u003e\n \n \n \n\n","permalink":"https://www.greenreedtech.com/the-next-chapter-at-puppet/","tags":["Career"],"title":"The Next Chapter at Puppet"},{"categories":["Vault"],"contents":"I gave a talk for HashiCorp\u0026rsquo;s HashiDays event earlier this year that centered around operational intelligence for HashiCorp Vault. The focus was on harnessing data and turning it into actionable insight to help drive informed decisions. One common insight that\u0026rsquo;s often required but not natively available for various reasons is a mechanism to identify what identities a policy is assigned to within Vault.\nWhile it has some limitations one way to mine such data from Vault is through scripting via the REST API. I was actually able to accomplish this using a python script that can be found at the following URL.\nhttps://github.com/martezr/vault-insights/blob/master/policy-insights.py\nAt a high level the script performs the following tasks:\n Retrieve a list of configured authentication methods (Only userpass and approle are currently supported) Retrieve a list of policies Iterate through the roles or users in the authentication methods and generate a list of assignments to policy Output the results in a JSON format for easy consumption into a tool like Splunk or ELK for dashboard visualization  An example of the script output can be found below.\n{  \u0026#34;admins\u0026#34;: {  \u0026#34;assignments\u0026#34;: [  \u0026#34;userpass_aboggs1\u0026#34;,  \u0026#34;userpass_acortez5\u0026#34;,  \u0026#34;userpass_admin\u0026#34;,  \u0026#34;userpass_dsmith\u0026#34;,  \u0026#34;userpass_dtaylor\u0026#34;,  \u0026#34;userpass_edequattro8\u0026#34;,  \u0026#34;userpass_efrazier6\u0026#34;,  \u0026#34;userpass_epeterson\u0026#34;,  \u0026#34;userpass_ering1\u0026#34;,  \u0026#34;userpass_whadden\u0026#34;  ],  \u0026#34;total_assignments\u0026#34;: 10  },  \u0026#34;app-readonly\u0026#34;: {  \u0026#34;assignments\u0026#34;: [  \u0026#34;approle_app8\u0026#34;,  \u0026#34;approle_app9\u0026#34;,  \u0026#34;userpass_appdb1\u0026#34;  ],  \u0026#34;total_assignments\u0026#34;: 3  },  \u0026#34;default\u0026#34;: {  \u0026#34;assignments\u0026#34;: [],  \u0026#34;total_assignments\u0026#34;: 0  },  \u0026#34;orphan_assignments\u0026#34;: {  \u0026#34;app9\u0026#34;: \u0026#34;newerpolicy\u0026#34;  },  \u0026#34;policywriter\u0026#34;: {  \u0026#34;assignments\u0026#34;: [  \u0026#34;approle_app1\u0026#34;,  \u0026#34;userpass_mreed\u0026#34;  ],  \u0026#34;total_assignments\u0026#34;: 2  },  \u0026#34;root\u0026#34;: {  \u0026#34;assignments\u0026#34;: [],  \u0026#34;total_assignments\u0026#34;: 0  },  \u0026#34;testpolicy\u0026#34;: {  \u0026#34;assignments\u0026#34;: [  \u0026#34;approle_app4\u0026#34;,  \u0026#34;approle_app8\u0026#34;,  \u0026#34;approle_hello-world\u0026#34;  ],  \u0026#34;total_assignments\u0026#34;: 3  },  \u0026#34;vaultreporter\u0026#34;: {  \u0026#34;assignments\u0026#34;: [  \u0026#34;userpass_vaultreporter\u0026#34;  ],  \u0026#34;total_assignments\u0026#34;: 1  } } The ultimate goal of the script is to generate metrics that could be used to help determine what level of access an identity has within Vault. It could also be used when looking to eliminate policy sprawl within a Vault cluster.\n","date":"04 Jun, 2019","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cpicture\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_545x0_resize_q95_h2_box.webp\" media=\"(max-width: 575px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_600x0_resize_q95_h2_box.webp\" media=\"(max-width: 767px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_700x0_resize_q95_h2_box.webp\" media=\"(max-width: 991px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_h2_box.webp\"\u003e\n  \u003cimg loading=\"lazy\" decoding=\"async\" class=\"w-100 img-fluid rounded\" src=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_box.jpg\" alt=\"\" width=\"970\" height=\"500\"\u003e\n\u003c/picture\u003e\n \n \n \n\n","permalink":"https://www.greenreedtech.com/hashicorp-vault-policy-metrics/","tags":["HashiCorp","Vault"],"title":"HashiCorp Vault Policy Metrics"},{"categories":["Vault"],"contents":"HashiCorp Vault is quickly becoming the defacto secrets management platform used in environments that rely on DevOps concepts for application delivery. Vault is incredibly easy and simple to get started with but takes a bit of thought and planning to operationalize it.\nOne of the challenges is ensuring that the installation of your security platform is secure. Chef InSpec is a compliance as code tool that allows us to create profiles that outline a desired security posture. In this post we\u0026rsquo;re looking at an example InSpec profile for Vault that mimics some of the common controls found in industry standards such as CIS benchmarks and DISA STIGs.\nhttps://github.com/martezr/inspec-vault\nRequirements\nThis InSpec profile assumes the following configuration.\n CentOS 7 Vault running as a SystemD service AuditD  Download and install Chef InSpec (https://downloads.chef.io/inspec)\nThe following command downloads and installs Chef InSpec.\nrpm -Uvh https://packages.chef.io/files/stable/inspec/2.2.50/el/7/inspec-2.2.50-1.el7.x86_64.rpm Run InSpec Profile\nThe following command runs the Vault InSpec profile against the local machine.\ninspec exec https://github.com/martezr/inspec-vault Profile: HashiCorp Vault InSpec Profile (inspec-vault) Version: 0.0.1 Target: local:// ✔ vault-1.1: Keep Vault up to date ✔ vault_version version should cmp \u0026gt;= \u0026#34;v0.10.1\u0026#34; × vault-1.2: Audit Vault executable × Auditd Rules lines should include \u0026#34;-w /usr/local/bin/vault -p rwxa -k vault\u0026#34; expected [\u0026#34;No rules\u0026#34;] to include \u0026#34;-w /usr/local/bin/vault -p rwxa -k vault\u0026#34; × vault-1.3: Secure Vault configuration files (2 failed) × Directory /opt/vault should not be readable by others expected Directory /opt/vault not to be readable by others ✔ Directory /opt/vault should not be writable by others × Directory /opt/vault should not be executable by others expected Directory /opt/vault not to be executable by others ✔ Directory /opt/vault owner should eq \u0026#34;vault\u0026#34; × vault-1.4: Audit Vault files and directories × Auditd Rules lines should include \u0026#34;-w /opt/vault/ -p rwxa -k vault\u0026#34; expected [\u0026#34;No rules\u0026#34;] to include \u0026#34;-w /opt/vault/ -p rwxa -k vault\u0026#34; × vault-1.5: Audit Vault service configuration × Auditd Rules lines should include \u0026#34;-w /etc/systemd/system/vault.service -p rwxa -k vault\u0026#34; expected [\u0026#34;No rules\u0026#34;] to include \u0026#34;-w /etc/systemd/system/vault.service -p rwxa -k vault\u0026#34; ✔ vault-1.6: Ensure that the vault service is running ✔ Service vault should be installed ✔ Service vault should be enabled ✔ Service vault should be running ✔ vault-1.7: Ensure Vault is not running as root ✔ Processes vault users should not eq [\u0026#34;root\u0026#34;] × vault-1.8: Ensure swap is disabled on the system × Command: `swapon -s | grep -v Filename` exit_status should eq 1 expected: 1 got: 0 (compared using ==) ✔ vault-1.9: Verify that vault.service file permissions are set to 644 or more restrictive ✔ File /etc/systemd/system/vault.service should exist ✔ File /etc/systemd/system/vault.service should be file ✔ File /etc/systemd/system/vault.service should be readable by owner ✔ File /etc/systemd/system/vault.service should be writable by owner ✔ File /etc/systemd/system/vault.service should be readable by group ✔ File /etc/systemd/system/vault.service should not be writable by group ✔ File /etc/systemd/system/vault.service should be readable by other ✔ File /etc/systemd/system/vault.service should not be writable by other ✔ File /etc/systemd/system/vault.service should not be executable Profile Summary: 4 successful controls, 5 control failures, 0 controls skipped Test Summary: 16 successful, 6 failures, 0 skipped Attributes InSpec attributes allow variables to be changed at runtime such as the name of a user or the path of a directory to check. This allows the InSpec profile be flexible enough to accommodate small differences in configurations.\nRunning the InSpec Profile with attributes The example below shows how we can use an attributes file to change some of the things that the InSpec profile looks for.\nThe readme of the InSpec profile lists what attributes are available such as vault_dir or vault_user. We just need to create a yaml file such as \u0026ldquo;attr.yaml\u0026rdquo; and define the desired attributes like the example below.\nExample attributes.yaml file\nvault_user: bob vault_dir: /etc/vault Running the InSpec profile with an attributes file\ninspec exec https://github.com/martezr/inspec-vault --attributes file_path/attributes.yaml This InSpec profile is in the early stages of development and continues to evolve but it provides an example of how InSpec can be used as a tool to shift security left.\nReferences HashiCorp Vault InSpec Profile\nhttps://github.com/martezr/inspec-vault\nInSpec Profiles\nhttps://www.inspec.io/docs/reference/profiles/\n","date":"07 Aug, 2018","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cpicture\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_545x0_resize_q95_h2_box.webp\" media=\"(max-width: 575px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_600x0_resize_q95_h2_box.webp\" media=\"(max-width: 767px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_700x0_resize_q95_h2_box.webp\" media=\"(max-width: 991px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_h2_box.webp\"\u003e\n  \u003cimg loading=\"lazy\" decoding=\"async\" class=\"w-100 img-fluid rounded\" src=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_box.jpg\" alt=\"\" width=\"970\" height=\"500\"\u003e\n\u003c/picture\u003e\n \n \n \n\n","permalink":"https://www.greenreedtech.com/vault-hardening-compliance-using-chef-inspec/","tags":["DevOps","Vault","Security","InSpec"],"title":"Vault Hardening Compliance using Chef InSpec"},{"categories":["Vault"],"contents":"Vault (https://www.vaultproject.io/) is a secrets management tool created by HashiCorp that is extremely popular. Given the sensitive nature of the data being stored by a Vault server it is critical that auditing be configured appropriately to provide a record of who accessed sensitive data and when it was accessed. In this blog post we\u0026rsquo;ll walk through configuring a Vault server for auditing and dump the log entries to an AWS S3 bucket for centralized storage.\nEnable Auditing The following is an example of a write event to vault.\nExample Audit entry\n{  \u0026#34;time\u0026#34;: \u0026#34;2018-02-01T14:40:03.226772711Z\u0026#34;,  \u0026#34;type\u0026#34;: \u0026#34;response\u0026#34;,  \u0026#34;auth\u0026#34;: {  \u0026#34;client_token\u0026#34;: \u0026#34;hmac-sha256:09b0c08f04bc69bf10a0b8c1d2fa3d84ad4efc470d4f3125950cb5979e606843\u0026#34;,  \u0026#34;accessor\u0026#34;: \u0026#34;hmac-sha256:34d5c7474ecac552772fbe091aee99dfc60b6461d42504a117b09928552cfad8\u0026#34;,  \u0026#34;display_name\u0026#34;: \u0026#34;root\u0026#34;,  \u0026#34;policies\u0026#34;: [  \u0026#34;root\u0026#34;  ],  \u0026#34;metadata\u0026#34;: null,  \u0026#34;entity_id\u0026#34;: \u0026#34;\u0026#34;  },  \u0026#34;request\u0026#34;: {  \u0026#34;id\u0026#34;: \u0026#34;1af553d4-f2a8-4fac-66ec-b333b392011a\u0026#34;,  \u0026#34;operation\u0026#34;: \u0026#34;create\u0026#34;,  \u0026#34;client_token\u0026#34;: \u0026#34;hmac-sha256:09b0c08f04bc69bf10a0b8c1d2fa3d84ad4efc470d4f3125950cb5979e606843\u0026#34;,  \u0026#34;client_token_accessor\u0026#34;: \u0026#34;hmac-sha256:34d5c7474ecac552772fbe091aee99dfc60b6461d42504a117b09928552cfad8\u0026#34;,  \u0026#34;path\u0026#34;: \u0026#34;secret/audittest\u0026#34;,  \u0026#34;data\u0026#34;: {  \u0026#34;value\u0026#34;: \u0026#34;hmac-sha256:f0dbb2e3574400553d45f259391f17a2c09e7845deb310faf3151ea2527e6c51\u0026#34;  },  \u0026#34;policy_override\u0026#34;: false,  \u0026#34;remote_address\u0026#34;: \u0026#34;172.28.128.6\u0026#34;,  \u0026#34;wrap_ttl\u0026#34;: 0,  \u0026#34;headers\u0026#34;: {}  },  \u0026#34;response\u0026#34;: {},  \u0026#34;error\u0026#34;: \u0026#34;\u0026#34; } Security Considerations The audit log entry contains the value of the secret in a hashed format but can be deciphered with the appropriate access to the vault server. The log files should be encrypted and access to the log files restricted.\n The audit logs contain the full request and response objects for every interaction with Vault. The request and response can be matched utilizing a unique identifier assigned to each request. The data in the request and the data in the response (including secrets and authentication tokens) will be hashed with a salt using HMAC-SHA256.\n  The purpose of the hash is so that secrets aren\u0026rsquo;t in plaintext within your audit logs. However, you\u0026rsquo;re still able to check the value of secrets by generating HMACs yourself; this can be done with the audit device\u0026rsquo;s hash function and salt by using the /sys/audit-hash API endpoint (see the documentation for more details). Currently Vault supports three auditing methods\n Audit Methods Vault supports the following three methods for writing audit entries.\n Socket Syslog File  Socket The socket audit device writes to a TCP, UDP, or UNIX socket. Due the warning in the HashiCorp documentation below we are avoiding this method for a more reliable method as the loss of a single audit entry is not desired.\n Warning: Due to the nature of the underlying protocols used in this device there exists a case when the connection to a socket is lost a single audit entry could be omitted from the logs and the request will still succeed. Using this device in conjunction with another audit device will help to improve accuracy, but the socket device should not be used if strong guarantees are needed for audit logs.\n Syslog Syslog is the de-facto standard for logging and is partially supported by Vault as the method is limited to sending only to local syslog and not a remote destination.\nFile Given the limitations of the other two methods the file audit method is the ideal audit method. To centralize the logging we\u0026rsquo;ll use Fluentd to dump the logs to an S3 bucket.\nvault audit enable file file_path=/var/log/vault_audit.log Installing Fluentd Increase Max number of File Descriptors\nPlease add following lines to your /etc/security/limits.conf file and reboot your machine.\nroot soft nofile 65536 root hard nofile 65536 * soft nofile 65536 * hard nofile 65536 Install Fluentd\ncurl -L https://toolbelt.treasuredata.com/sh/install-redhat-td-agent3.sh | sh Enable Fluentd to start on boot\nsudo systemctl enable td-agent.service Start the Fluentd service\nsudo systemctl start td-agent.service With the Fluentd agent installed and the service started we now need to create an entry for our source which is the vault audit log file and a destination which will be our S3 bucket for persistently storing the log entries.\nFluentd Configuration\nWe need to write following input and output stanzas to the Fluentd agent configuration file (/etc/td-agent/td-agent.conf).\nInput\n\u0026lt;source\u0026gt; @type tail path /var/log/vault_audit.log pos_file /var/log/td-agent/vault.audit_log.pos \u0026lt;parse\u0026gt; @type json \u0026lt;/parse\u0026gt; tag s3.vault.audit \u0026lt;/source\u0026gt; S3 Output\n\u0026lt;match s3.*.*\u0026gt; @type s3 aws_key_id YOUR_AWS_KEY_ID aws_sec_key YOUR_AWS_SECRET/KEY s3_bucket YOUR_S3_BUCKET_NAME path logs/ \u0026lt;buffer\u0026gt; @type file path /var/log/td-agent/s3 timekey_wait 1m chunk_limit_size 256m \u0026lt;/buffer\u0026gt; time_slice_format %Y%m%d%H%M \u0026lt;/match\u0026gt; Validating Auditing Now that we\u0026rsquo;ve gotten everything configured we just need to test it to make sure everything is working properly. To do this we need to perform some vault action like writing a secret or reading a secret to trigger the creation of an audit entry.\nvault write secret/audittest value=testingvaultauditing Once the command is run we should see a new entry created in the /var/log/vault_audit.log file and after a few minutes we should see a folder created in our S3 bucket for storing our Vault audit logs.\nReferences Installing Fluentd\nhttps://docs.fluentd.org/v1.0/articles/before-install\nHashiCorp Vault Auditing\nhttps://www.vaultproject.io/docs/audit/index.html\n","date":"01 Feb, 2018","image":"\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://s3.us-west-2.amazonaws.com/greenreedtech.com/deployment-pipeline-chaos-engineering-with-stackstorm-and-chaostoolkit/chaos_update.png\" alt=\"\" class=\"w-100 img-fluid rounded\" height=\"\" width=\"\"\u003e\n","permalink":"https://www.greenreedtech.com/vault-audit-logging/","tags":["HashiCorp","Vault","DevOps","Security"],"title":"Vault Audit Logging"},{"categories":["Terraform"],"contents":"What is immutable infrastructure? Immutable infrastructure is the concept of utilizing an infrastructure component in an ephemeral manner. This means that the component can be destroyed and recreated at will without major impact.\nAdvantages\n Troublesome instances can easily be destroyed and recreated. System patching processes are replaced by just provisioning instances from a new template.  Terraform\nTerraform provides us with the ability to create vSphere infrastructure with code. Terraform enables us to quickly tear down and provision new infrastructure which allows us to quickly transition all of our VMs to a new template within a maintenance window.\nvSphere Immutable Infrastructure Design Our example application is a node.js application with a Mongodb backend.\n  Web Server: Deployment of our web instance is fairly simple as the web server stores no data that needs to persists across iterations of the instance.\n  Database Server: Deploying an immutable database server is particularly challenging given the requirement that the data in the database must persist across iterations of the instance.\n  Data persistence is achieved by decoupling the data drive or .VMDK file from the instantiation of the virtual machine. The virtual machine is created and attaches an existing hard disk that stores the data for our Mongodb database. When we destroy the database VM the .VMDK file is detached and the virtual machine is destroyed.\nFolder Layout The design uses three directories to store the Terraform code to allow us to abstract the code that manages the data disk for our database instance from the database instance.\n dbinstantiation: The code in this directory manages the database instance we\u0026rsquo;ll use to prep our data disk. instances: The code in this directory manages the web and database instances. persistentdisks: The code in this directory manages the data disk for the database instance.  vsphereimmutable/ ├── dbinstantiation │ ├── main.tf │ └── mongodbinstall.sh ├── instances │ ├── main.tf │ ├── mongodbinstall.sh │ └── nodeinstall.sh └── persistentdisks └── main.tf Terraform Code Now that we\u0026rsquo;ve walked through the immutable design we\u0026rsquo;ll jump into the Terraform code and helper scripts to build out the design in our vSphere environment.\nAll code used in this example can be found on Github.\nhttps://github.com/martezr/terraform-immutable-vsphere\nDatabase Hard Disk The second hard drive or data drive for our database instance is the first resource we\u0026rsquo;ll need to create using Terraform. The .VMDK file will be stored in the root of the datastore in this example but can easily be placed into a subfolder for persistent disks.\nresource \u0026#34;vsphere_virtual_disk\u0026#34; \u0026#34;DBDisk01\u0026#34; { size = 20 vmdk_path = \u0026#34;DBDisk01.vmdk\u0026#34; datacenter = \u0026#34;Datacenter\u0026#34; datastore = \u0026#34;local\u0026#34; type = \u0026#34;thin\u0026#34; adapter_type = \u0026#34;lsiLogic\u0026#34; } Let\u0026rsquo;s go ahead and run Terraform apply to create the data drive.\nTerraform apply Database Instantiation Instance The next thing we need to do is create the instance to prep the data drive for use with our immutable database instance.\nFirst database boot script We need to partition our data drive separate of the database bootstrap script as we only want to partition the data drive only once.\n#!/bib/bash # Partition the second disk ( echo n # Add a new partition echo p # Primary partition echo 1 # Partition number echo # First sector (Accept default: 1) echo # Last sector (Accept default: varies) echo w # Write changes ) | sudo fdisk /dev/sdb echo yes | mkfs.ext4 /dev/sdb mkdir /mongodb echo \u0026#39;/dev/sdb /mongodb ext4 defaults 0 0\u0026#39; \u0026gt;\u0026gt; /etc/fstab mount -a cat \u0026lt;\u0026lt; EOF \u0026gt; /etc/yum.repos.d/mongodb-org.repo [mongodb-org-3.4] name=MongoDB Repository baseurl=https://repo.mongodb.org/yum/redhat/7Server/mongodb-org/3.4/x86_64/ gpgcheck=1 enabled=1 gpgkey=https://www.mongodb.org/static/pgp/server-3.4.asc EOF yum -y install mongodb-org cat \u0026lt;\u0026lt; EOF \u0026gt; /etc/mongod.conf # where to write logging data. systemLog: destination: file logAppend: true path: /var/log/mongodb/mongod.log # Where and how to store data. storage: dbPath: /mongo journal: enabled: true # engine: # mmapv1: # wiredTiger: # how the process runs processManagement: fork: true # fork and run in background pidFilePath: /var/run/mongodb/mongod.pid # location of pidfile # network interfaces net: port: 27017 # bindIp: 127.0.0.1 # Listen to local interface only, comment to listen on all interfaces. EOF systemctl start mongod Now we need to run Terraform apply to create the instance.\nTerraform apply Once our instance has been successfully bootstrapped using the mongodbprep script we\u0026rsquo;ll need to run the Terraform destroy command to destroy our instantiation instance.\nTerraform destroy Production Instances Now that the data drive for our Mongodb VM has been prepped we just need to provision our production instances and start adding data.\nresource \u0026#34;vsphere_virtual_machine\u0026#34; \u0026#34;Node01\u0026#34; { name = \u0026#34;terraform-node\u0026#34; vcpu = 2 memory = 4096 network_interface { label = \u0026#34;VM Network\u0026#34; ipv4_address = \u0026#34;192.168.1.4\u0026#34; ipv4_prefix_length = \u0026#34;24\u0026#34; ipv4_gateway = \u0026#34;192.168.1.254\u0026#34; } provisioner \u0026#34;file\u0026#34; { source = \u0026#34;nodeinstall.sh\u0026#34; destination = \u0026#34;nodeinstall.sh\u0026#34; connection { type = \u0026#34;ssh\u0026#34; host = \u0026#34;192.168.1.4\u0026#34; user = \u0026#34;root\u0026#34; password = \u0026#34;password\u0026#34; } } provisioner \u0026#34;remote-exec\u0026#34; { connection { type = \u0026#34;ssh\u0026#34; host = \u0026#34;192.168.1.4\u0026#34; user = \u0026#34;root\u0026#34; password = \u0026#34;password\u0026#34; } inline = [ \u0026#34;setenforce 0\u0026#34;, \u0026#34;chmod +x /root/nodeinstall.sh \u0026amp;\u0026amp; sh /root/nodeinstall.sh\u0026#34; ] } disk { template = \u0026#34;centosnodetemp\u0026#34; type = \u0026#34;thin\u0026#34; datastore = \u0026#34;Local_Storage\u0026#34; } } resource \u0026#34;vsphere_virtual_machine\u0026#34; \u0026#34;DB01\u0026#34; { name = \u0026#34;terraform-db\u0026#34; vcpu = 2 memory = 4096 detach_unknown_disks_on_delete = \u0026#34;true\u0026#34; enable_disk_uuid = \u0026#34;true\u0026#34; network_interface { label = \u0026#34;VM Network\u0026#34; ipv4_address = \u0026#34;192.168.1.5\u0026#34; ipv4_prefix_length = \u0026#34;24\u0026#34; ipv4_gateway = \u0026#34;192.168.1.254\u0026#34; } provisioner \u0026#34;file\u0026#34; { source = \u0026#34;mongodbinstall.sh\u0026#34; destination = \u0026#34;mongodbinstall.sh\u0026#34; connection { type = \u0026#34;ssh\u0026#34; host = \u0026#34;192.168.1.5\u0026#34; user = \u0026#34;root\u0026#34; password = \u0026#34;password\u0026#34; } } provisioner \u0026#34;remote-exec\u0026#34; { connection { type = \u0026#34;ssh\u0026#34; host = \u0026#34;192.168.1.5\u0026#34; user = \u0026#34;root\u0026#34; password = \u0026#34;password\u0026#34; } inline = [ \u0026#34;mkdir /mongodb\u0026#34;, \u0026#34;echo \u0026#39;/dev/sdb /mongodb ext4 defaults 0 0\u0026#39; \u0026gt;\u0026gt; /etc/fstab\u0026#34;, \u0026#34;mount -a\u0026#34;, \u0026#34;chmod +x /root/mongodbinstall.sh \u0026amp;\u0026amp; sh /root/mongodbinstall.sh\u0026#34; ] } disk { template = \u0026#34;centos7temp\u0026#34; type = \u0026#34;thin\u0026#34; } disk { vmdk = \u0026#34;DBDisk01.vmdk\u0026#34; datastore = \u0026#34;Local_Storage\u0026#34; type = \u0026#34;thin\u0026#34; keep_on_remove = \u0026#34;true\u0026#34; } } Let\u0026rsquo;s go ahead and run a Terraform apply to build our production instances.\nWith the node.js app started we should be able to access the example node.js app from our web browser.\nhttp://web_instance:8080\nLet\u0026rsquo;s add some todo items to the list so we can validate that the data persists across instantiations of the database virtual machine.\nAdd a few tasks to the list, in our example a few whimsical todo items have been added to the list.\n Datatbase can be immutable too Stateful apps don\u0026rsquo;t have to be pets The data is all that needs to persist  Immutability Test The last thing we need to do now is to actually test the immutability of our design by destroying and rebuilding the infrastructure. From within the \u0026ldquo;instances\u0026rdquo; directory we need to run the terraform destroy command to destroy the node.js and database virtual machines.\nTerraform destroy Once both of our VMs are destroyed we need recreate them using terraform apply and validate that the todo items are still available in our app.\nTerraform apply If everything works as expected then we should see the todo items we created earlier still in the list. This indicates that we were able to destroy and recreate both the web and database instances without losing any data.\nThis solution enables us to decouple the underlying virtual machines from the business value provided by our application and allows us to take advantage of immutable infrastructure.\nReferences Terraform Provisioner Connection https://www.terraform.io/docs/provisioners/connection.html\nTerraform vSphere Provider https://www.terraform.io/docs/providers/vsphere/index.html\nNode.js Todo Example App https://scotch.io/tutorials/creating-a-single-page-todo-app-with-node-and-angular\nGithub example code https://github.com/martezr/terraform-immutable-vsphere\n","date":"13 Oct, 2017","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cpicture\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_545x0_resize_q95_h2_box.webp\" media=\"(max-width: 575px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_600x0_resize_q95_h2_box.webp\" media=\"(max-width: 767px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_700x0_resize_q95_h2_box.webp\" media=\"(max-width: 991px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_h2_box.webp\"\u003e\n  \u003cimg loading=\"lazy\" decoding=\"async\" class=\"w-100 img-fluid rounded\" src=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_box.jpg\" alt=\"\" width=\"970\" height=\"500\"\u003e\n\u003c/picture\u003e\n \n \n \n\n","permalink":"https://www.greenreedtech.com/vsphere-immutable-infrastructure-with-terraform/","tags":["DevOps","Terraform","VMware"],"title":"vSphere Immutable Infrastructure with Terraform"},{"categories":["Jenkins"],"contents":"This post covers the section listed below on the Certified Jenkins Engineer (CJE) exam.\nSection #3: Building Continuous Delivery (CD) Pipelines\nFolders\n How to control access to items in Jenkins with folders Referencing jobs in folders  What are folders? Jenkins provides the ability to organize jobs into a hierarchical manner with the CloudBees Folders Plugin. This allows us to manage the jobs much like we would files on a file system. Folders can also be used to manage permissions on a per folder basis to ease security administration.\nBelow is the config.xml file of a folder which holds the folder\u0026rsquo;s configuration much like the config.xml configuration file for a job.\n\u0026lt;?xml version=\u0026#39;1.0\u0026#39; encoding=\u0026#39;UTF-8\u0026#39;?\u0026gt; \u0026lt;com.cloudbees.hudson.plugins.folder.Folder plugin=\u0026#34;cloudbees-folder@5.12\u0026#34;\u0026gt;  \u0026lt;actions/\u0026gt;  \u0026lt;description\u0026gt;\u0026lt;/description\u0026gt;  \u0026lt;properties/\u0026gt;  \u0026lt;views\u0026gt;  \u0026lt;hudson.model.AllView\u0026gt;  \u0026lt;owner class=\u0026#34;com.cloudbees.hudson.plugins.folder.Folder\u0026#34; reference=\u0026#34;../../..\u0026#34;/\u0026gt;  \u0026lt;name\u0026gt;All\u0026lt;/name\u0026gt;  \u0026lt;filterExecutors\u0026gt;false\u0026lt;/filterExecutors\u0026gt;  \u0026lt;filterQueue\u0026gt;false\u0026lt;/filterQueue\u0026gt;  \u0026lt;properties class=\u0026#34;hudson.model.View$PropertyList\u0026#34;/\u0026gt;  \u0026lt;/hudson.model.AllView\u0026gt;  \u0026lt;/views\u0026gt;  \u0026lt;viewsTabBar class=\u0026#34;hudson.views.DefaultViewsTabBar\u0026#34;/\u0026gt;  \u0026lt;healthMetrics\u0026gt;  \u0026lt;com.cloudbees.hudson.plugins.folder.health.WorstChildHealthMetric/\u0026gt;  \u0026lt;/healthMetrics\u0026gt;  \u0026lt;icon class=\u0026#34;com.cloudbees.hudson.plugins.folder.icons.StockFolderIcon\u0026#34;/\u0026gt;  Creating folders Now that we\u0026rsquo;ve gone over what folders are let\u0026rsquo;s go ahead and create a folder.\nStep #1: Click \u0026ldquo;New Item\u0026rdquo; to create a new item (job/folder) Step #2: Enter a name in the textbox, select \u0026ldquo;Folder\u0026rdquo; and click \u0026ldquo;OK\u0026rdquo; to create the folder. Step #3: Configure the folder settings as desired and click \u0026ldquo;Save\u0026rdquo; to apply the changes.  Creating a job in a folder This section will cover creating a new job within a folder.\nStep #1: From within the desired folder, click \u0026ldquo;New Item\u0026rdquo; to create a new item (job/folder). In this case we\u0026rsquo;re using the \u0026ldquo;Finance\u0026rdquo; folder we created in the previous task. Step #2: Enter a name in the textbox and select \u0026ldquo;Freestyle project\u0026rdquo; and click \u0026ldquo;OK\u0026rdquo; to create the job. Step #3: Configure the job settings as desired and click \u0026ldquo;Save\u0026rdquo; to apply the changes.\nIf we go back to the folder we now see that the \u0026ldquo;Monthly_Report\u0026rdquo; job we just created is listed in the folder.\n Moving an existing job to a folder In addition to creating a new job in our folder we can move our existing jobs to folder. We\u0026rsquo;ll move a job named \u0026ldquo;Test\u0026rdquo; to our Finance folder.\nStep 1: Click the arrow to the right of the job name and select \u0026ldquo;Move\u0026rdquo;. Step 2: Select the desired folder from the drop down menu and click \u0026ldquo;Move\u0026rdquo;. In our case we\u0026rsquo;ll select the Finance folder. We now see that the \u0026ldquo;Test\u0026rdquo; job has been added to our \u0026ldquo;Finance\u0026rdquo; folder.\n Referencing jobs in folders With the creation of folders we need to understand the namespace used for referencing files within the folder such as the folder config.xml and jobs in the folder.\nIn this example we\u0026rsquo;re going to use a folder named \u0026ldquo;JenkinsFolder\u0026rdquo; and a job name \u0026ldquo;JenkinsJob\u0026rdquo;.\nhttp://**JenkinsServer/job/JenkinsFolder/job/JenkinsJob\nServer Name: JenkinsServer\nFolder Name: JenkinsFolder\nJob Name: JenkinsJob\nWe can see that a \u0026ldquo;job\u0026rdquo; section is prepended to both the folder name as well as the job name. Now that we have an idea of how to navigate folders, let\u0026rsquo;s try an example. We need to create a link for the following job nested within a folder.\nServer Name: JenkinsServer\nFolder Name: Finance\nJob Name: MonthlyReport\nSolution: http://**JenkinsServer/job/Finance/job/MonthlyReport\nNow that we\u0026rsquo;re comfortable with accessing jobs within let\u0026rsquo;s try accessing a file within the folder.\nServer Name: JenkinsServer\nFolder Name: HR\nFile Name: config.xml\nSolution: http://**JenkinsServer/job/HR/config.xml\n How to control access to items in Jenkins with folders We can utilize our folders for managing user access to jobs by providing users with global read privileges and then assigning the user additional rights at the folder level to allow them to manage the jobs within the folder but not access any other folders on the Jenkins server.\nA security realm needs to be configured\nStep #1: Click \u0026ldquo;Manage Jenkins\u0026rdquo; from the sidebar. Step #2: Click \u0026ldquo;Configure Global Security\u0026rdquo;. Step #3: Select the \u0026ldquo;Project-based Matrix Authorization Strategy\u0026rdquo; under the \u0026ldquo;Authorization\u0026rdquo; section and select the appropriate permissions. Step #4: From the folder configuration section select the \u0026ldquo;Enable project-based security\u0026rdquo; checkbox and assign the desired permissions to the user or group. With access control being managed at the folder level we are able to segregate jobs on our Jenkins server to different departments or groups.\nReference\nhttps://www.cloudbees.com/products/cloudbees-jenkins-platform/team-edition/features/folders-plugin\nhttps://go.cloudbees.com/docs/cloudbees-documentation/cje-user-guide/chapter-folder.html#\n","date":"14 Jul, 2016","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cpicture\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_545x0_resize_q95_h2_box.webp\" media=\"(max-width: 575px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_600x0_resize_q95_h2_box.webp\" media=\"(max-width: 767px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_700x0_resize_q95_h2_box.webp\" media=\"(max-width: 991px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_h2_box.webp\"\u003e\n  \u003cimg loading=\"lazy\" decoding=\"async\" class=\"w-100 img-fluid rounded\" src=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_box.jpg\" alt=\"\" width=\"970\" height=\"500\"\u003e\n\u003c/picture\u003e\n \n \n \n\n","permalink":"https://www.greenreedtech.com/jenkins-certified-engineer-folders/","tags":["DevOps","Jenkins"],"title":"Jenkins Certified Engineer: Folders"},{"categories":["Jenkins"],"contents":"This post covers the section listed below on the Certified Jenkins Engineer (CJE) exam.\nSection #1: Key CI/CD/Jenkins Concepts\nFingerprints\n What are fingerprints? How do fingerprints work?  What are fingerprints? Jenkins utilizes fingerprints for tracking a specific instance of a file. This is critically important when attempting to determine which particular version of a file was used during a build. The fingerprint of a file is simply a MD5 checksum that can be used for comparing files.\nAn example fingerprint (MD5 checksum) is provided below for reference.\nMD5: d41d8cd98f00b204e9800998ecf8427e\nHow do fingerprints work? The fingerprints are stored as xml files in the $JENKINS_HOME/fingerprints directory on the Jenkins master. The xml file contains the Jenkins job name, build number, MD5 checksum, and fingerprinted file name.\n\u0026lt;?xml version=\u0026#39;1.0\u0026#39; encoding=\u0026#39;UTF-8\u0026#39;?\u0026gt; \u0026lt;fingerprint\u0026gt;  \u0026lt;timestamp\u0026gt;2016-03-15 19:26:54.777 UTC\u0026lt;/timestamp\u0026gt;  \u0026lt;original\u0026gt;  \u0026lt;name\u0026gt;Test\u0026lt;/name\u0026gt;  \u0026lt;number\u0026gt;1\u0026lt;/number\u0026gt;  \u0026lt;/original\u0026gt;  \u0026lt;md5sum\u0026gt;d41d8cd98f00b204e9800998ecf8427e\u0026lt;/md5sum\u0026gt;  \u0026lt;fileName\u0026gt;testfile.txt\u0026lt;/fileName\u0026gt;  \u0026lt;usages\u0026gt;  \u0026lt;entry\u0026gt;  \u0026lt;string\u0026gt;Test\u0026lt;/string\u0026gt;  \u0026lt;ranges\u0026gt;1\u0026lt;/ranges\u0026gt;  \u0026lt;/entry\u0026gt;  \u0026lt;/usages\u0026gt;  \u0026lt;facets/\u0026gt; \u0026lt;/fingerprint\u0026gt; References https://wiki.jenkins-ci.org/display/JENKINS/Fingerprint\n","date":"15 Mar, 2016","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cpicture\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_545x0_resize_q95_h2_box.webp\" media=\"(max-width: 575px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_600x0_resize_q95_h2_box.webp\" media=\"(max-width: 767px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_700x0_resize_q95_h2_box.webp\" media=\"(max-width: 991px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_h2_box.webp\"\u003e\n  \u003cimg loading=\"lazy\" decoding=\"async\" class=\"w-100 img-fluid rounded\" src=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_box.jpg\" alt=\"\" width=\"970\" height=\"500\"\u003e\n\u003c/picture\u003e\n \n \n \n\n","permalink":"https://www.greenreedtech.com/jenkins-certified-engineer-fingerprints/","tags":["DevOps","Jenkins"],"title":"Jenkins Certified Engineer: Fingerprints"},{"categories":["Terraform"],"contents":"Recently a co-worker of mine (Thanks Ken Erwin) introduced me to Jenkins Job Builder (http://docs.openstack.org/infra/jenkins-job-builder/) which is a project created by the OpenStack infrastructure team that aims to automate the creation of Jenkins Jobs. The software is written in python and utilizes either yaml or json files as the framework for creating Jenkins jobs. A list of some of the primary features is provided below.\nFeatures:  Supports job creation, modification, and deletion Supports templates to speed up the creation of similar jobs Supports all major plugins as well as extending the software to incorporate additional plugins The software has built-in test functionality to test changes before being deployed into production Many additional features  Example: The following section covers an example job in yaml format.\n- job: name: puppet-module-profiles project-type: matrix execution-strategy: sequential: true axes: - axis: type: user-defined name: PUPPET_VERSION values: - 3.8.2 - 3.8.4 - 4.0.0 - 4.1.0 - axis: type: slave name: nodes values: - puppet description: \u0026#39;Jenkins job to perform syntax and style checking on {name} puppet module.\u0026#39; disabled: false node: puppet scm: - git: url: ssh://git@stash.grt.local:7999/infrastructure/puppet-module-profiles.git branches: - origin/pr/{branch} credentials-id: \u0026#39;f46gdb08-b25e-7h9e-ngef-nar52j732j98\u0026#39; browser: auto triggers: - pollscm: cron: \u0026#34;\u0026#34; wrappers: - ansicolor: colormap: xterm - workspace-cleanup: include: - \u0026#34;*\u0026#34; - rvm-env: implementation: 1.9.3@pupet-module-profiles builders: - system-groovy: command: | def currentBuild = Thread.currentThread().executable def PULL_REQUEST_URL = build.buildVariableResolver.resolve(\u0026#39;PULL_REQUEST_URL\u0026#39;) def PULL_REQUEST_ID = build.buildVariableResolver.resolve(\u0026#39;PULL_REQUEST_ID\u0026#39;) def description = \u0026#34;\u0026lt;a href=\u0026#39;$PULL_REQUEST_URL\u0026#39;\u0026gt;PR #$PULL_REQUEST_ID\u0026lt;/a\u0026gt;\u0026#34; currentBuild.setDescription(description) - conditional-step: condition-kind: regex-match regex: \u0026#39;^(BUTTON_TRIGGER|OPENED|REOPENED|UPDATED)$\u0026#39; label: ${{ENV,var=\u0026#34;ACTION\u0026#34;}} steps: - shell: | #!/bin/bash echo \u0026#34;Build Trigger $ACTION\u0026#34; export PUPPET_VERSION=\u0026#34;$PUPPET_VERSION\u0026#34; gem install bundler bundle install - shell: | find . -iname *.pp -exec puppet-lint --no-80chars-check --log-format \u0026#34;%{{path}}:%{{line}}:%{{check}}:%{{KIND}}:%{{message}}\u0026#34; {{}} ; - shell: | for file in $(find . -iname \u0026#39;*.pp\u0026#39;); do puppet parser validate --render-as s --modulepath=modules \u0026#34;$file\u0026#34; || exit 1; done - conditional-step: condition-kind: regex-match regex: \u0026#39;^(MERGED)$\u0026#39; label: ${{ENV,var=\u0026#34;ACTION\u0026#34;}} steps: - shell: | #!/bin/bash echo \u0026#34;Build Trigger $ACTION\u0026#34; publishers: - warnings: console-log-parsers: - Puppet-Lint run-always: true - stash: url: \u0026#39;https://stash.grt.local:8443\u0026#39; ignore-ssl: true References: http://docslide.us/software/continuous-delivery-with-docker-and-jenkins-job-builder.html\n","date":"13 Jan, 2016","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cpicture\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_545x0_resize_q95_h2_box.webp\" media=\"(max-width: 575px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_600x0_resize_q95_h2_box.webp\" media=\"(max-width: 767px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_700x0_resize_q95_h2_box.webp\" media=\"(max-width: 991px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_h2_box.webp\"\u003e\n  \u003cimg loading=\"lazy\" decoding=\"async\" class=\"w-100 img-fluid rounded\" src=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_box.jpg\" alt=\"\" width=\"970\" height=\"500\"\u003e\n\u003c/picture\u003e\n \n \n \n\n","permalink":"https://www.greenreedtech.com/jenkins-job-builder/","tags":["DevOps","Terraform","VMware"],"title":"Jenkins Job Builder"},{"categories":["DevOps"],"contents":"This post covers integrating Jenkins CI server with Microsoft Active Directory to provide centralized authentication.\nStep #1 - Install he Active Directory plugin  Click \u0026ldquo;Manage Jenkins\u0026rdquo; from the sidebar\n  Click \u0026ldquo;Manage Plugins\u0026rdquo; to install the Active Directory plugin\n  Click on the \u0026ldquo;Available\u0026rdquo; tab, enter \u0026ldquo;Active Directory\u0026rdquo; in the \u0026ldquo;Filter:\u0026rdquo; search box, click the checkbox next to \u0026ldquo;Active Directory plugin\u0026rdquo; and finally click \u0026ldquo;Download now and install after restart\u0026rdquo;\n  Click the \u0026ldquo;Restart Jenkins when install is complete and no jobs are running\u0026rdquo; checkbox to complete the plugin installation.\n  Click \u0026ldquo;Configure Global Security\u0026rdquo;\n  Select \u0026ldquo;Active Directory\u0026rdquo; and enter the domain and domain accounts information.\n  Select \u0026ldquo;Matrix-based security\u0026rdquo; and add the desired users and groups. user names and group names are case sensitive\n  Log into the web interface with AD credentials\n References: Jenkins AD Security http://justinramel.com/2013/01/15/active-directory-security/\nJenkins AD Plugin https://wiki.jenkins-ci.org/display/JENKINS/Active+Directory+Plugin\n","date":"20 Nov, 2015","image":"\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://s3.us-west-2.amazonaws.com/greenreedtech.com/jce_folders/post/05.jpg\" alt=\"\" class=\"w-100 img-fluid rounded\" height=\"\" width=\"\"\u003e\n","permalink":"https://www.greenreedtech.com/jenkins-active-directory/","tags":["DevOps","Jenkins"],"title":"Jenkins Active Directory"},{"categories":["Puppet"],"contents":"Puppet supports various hiera backends to pull in external data from various sources. This post will cover integrating open source puppet with a couchdb database using the hiera-http backend.\nCouchDB Setup and Configuration The following steps will cover the setup and configuration of the CouchDB database server that will be used as the data store. The following steps assume that docker has been installed\nStart CouchDB Docker container\nThe container is started with a name of \u0026ldquo;couchdb\u0026rdquo; and the CouchDB admin username and password are set during container creation. Port 5984 is mapped to port 5984 on the host machine.\ndocker run -d --name couchdb -p 5984:5984 -e COUCHDB_USERNAME=couchadmin -e COUCHDB_PASSWORD=Password couchdb Database configuration\nCouchDB provides a web interface for managing the databases as well as REST API.\nStep #1 - Log into the CouchDB web interface using the credentials created during the container provisioning.\nClick login at the bottom right corner of the web page\nhttp://ip_address:5984/_utils Enter the login credentials Step #2 - Create a new database In our example configuration we\u0026rsquo;ll use \u0026ldquo;hiera\u0026rdquo; as the database to store all the puppet related documents.\nClick \u0026ldquo;Create Database\u0026rdquo; to create the new database Enter the database name and click \u0026ldquo;Create\u0026rdquo; to create the database. Step #3 - Create a new document The documents will act like the individual .yaml files in the yaml backend to provide a customized hierarchy.\nClick \u0026ldquo;New Document\u0026rdquo; to create the new document Enter the appropriate value for the \u0026ldquo;_id\u0026rdquo; field and Click \u0026ldquo;Add Field\u0026rdquo; to add a new field. In our example we use common which replicates the common.yaml file in the yaml backend structure.\nEnter the desired hiera data and click \u0026ldquo;Save Document\u0026rdquo; when done. The additional fields are used to store the actual data such as classes, and class variables. In our example we\u0026rsquo;ll add a couchdbtest value for testing.\nThe example below shows what the code would look like in a yaml file.\n--- couchdbtest: \u0026#39;Does it really work\u0026#39; The database has now been configured so we can move on to the puppet configuration. REST API The previous steps for configuring the database and fields can be performed utilizing the REST API provided by CouchDB. Basic authentication is used to manage the database and documents.\nThe following command creates the \u0026ldquo;hiera\u0026rdquo; database\ncurl -X PUT http://couchadmin:Password@10.0.0.148:5984/hiera The following command creates the \u0026ldquo;common\u0026rdquo; document\ncurl -X PUT http://couchadmin:Password@10.0.0.148:5984/hiera/common -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39;{\u0026#34;couchdbtest\u0026#34;:\u0026#34;Does it really work\u0026#34;}\u0026#39; Puppet Master Configuration The following steps will cover configuring Puppet to communicate with the CouchDB database.\nInstall hiera-http The hiera-http is installed via ruby-gems\ngem install hiera-http Update hiera.yaml config file Use a text editor to modify the hiera.yaml file.\n--- :backends: [\u0026#39;http\u0026#39;,\u0026#39;yaml\u0026#39;]  :hierarchy:  - defaults  - \u0026#34;%{clientcert}\u0026#34;  - \u0026#34;%{environment}\u0026#34;  - global  :yaml: # datadir is empty here, so hiera uses its defaults: # - /var/lib/hiera on *nix # - %CommonAppData%PuppetLabshieravar on Windows # When specifying a datadir, make sure the directory exists.  :datadir:   :http:  :host: 10.0.0.148  :port: 5984  :output: json  :failure: graceful  :use_auth: true  :auth_user: \u0026#39;couchadmin\u0026#39;  :auth_pass: \u0026#39;Password\u0026#39;  :paths:  - /hiera/%{clientcert}  - /hiera/%{environment}  - /hiera/common Perform hiera lookup We’ll perform a hiera lookup to verify that everything is working.\nhiera couchdbtest -d References: CouchDB Docker Image https://hub.docker.com/r/frodenas/couchdb/\nHiera-http configuration http://www.craigdunn.org/2012/11/puppet-data-from-couchdb-using-hiera-http/\nOpen source puppet master install http://blog.fnaard.com/2015/04/build-puppet-master-on-centos-7-hella.html\n","date":"19 Nov, 2015","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cpicture\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_545x0_resize_q95_h2_box.webp\" media=\"(max-width: 575px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_600x0_resize_q95_h2_box.webp\" media=\"(max-width: 767px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_700x0_resize_q95_h2_box.webp\" media=\"(max-width: 991px)\"\u003e\n  \u003csource srcset=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_h2_box.webp\"\u003e\n  \u003cimg loading=\"lazy\" decoding=\"async\" class=\"w-100 img-fluid rounded\" src=\"/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_box.jpg\" alt=\"\" width=\"970\" height=\"500\"\u003e\n\u003c/picture\u003e\n \n \n \n\n","permalink":"https://www.greenreedtech.com/puppet-hiera-http-using-couchdb/","tags":["Puppet","Hiera"],"title":"Puppet Hiera HTTP Using CouchDB"}]