[{"categories":["HashiCorp"],"contents":"HashiCorp Vault supports several authentication methods for human and non-human access. Several of the non-human authentication methods are tied to specific platforms or clouds such as AWS, Kubernetes, Azure, and others. For workloads that are running on non-supported platforms, the AppRole authentication method is typically recommended for authentication. The AppRole method uses a role as the core construct as the name implies. In order to authenticate a role ID and a secret ID are required.\nAppRole Conundrum The role is typically used in a non-unique or shared manner in that it is used by multiple systems for authentication purposes. It is possible to create a role that is unique to an individual workload but can greatly increase the management overhead. This poses the classic challenge of correctly identifying which system accessed a given path in Vault. The audit log does include the remote IP address of the system that makes the request, but this method of identification relies upon a point in time mapping of the IP address to the system.\nSolution The Vault API supports the ability to add custom metadata to a generated AppRole secret ID that is displayed in the Vault audit logs. This enables the system that is trusted to generate secret IDs for a given role to associate a unique identity with the secret ID. The custom metadata appears in the requests and responses of access attempts by the token associated with the generated secret ID. The screenshot below displays an example that specifies virtual_machine_name as the identifying piece of metadata associated with the secret ID.\nWithout the addition of the additional metadata all that we would know was that the potentially shared role was used to access a given path within HashiCorp Vault.\nAdding Identity Metadata Now that we know how the solution works let’s walk through adding the custom metadata during an API call to generate a secret ID for an example role. The API expects the metadata field to be a JSON formatted string (https://www.vaultproject.io/api/auth/approle#generate-new-secret-id).\ncurl -k \\  --header \u0026#34;X-Vault-Token: s.Zdd50L8a35S6FKxC1UNzvhgU\u0026#34; \\  --request POST --data \u0026#39;{\u0026#34;metadata\u0026#34;: \u0026#34;{ \\\u0026#34;tag1\\\u0026#34;: \\\u0026#34;production\\\u0026#34; }\u0026#34;}\u0026#39; \\  https://127.0.0.1:8200/v1/auth/approle/role/vsphereapp/secret-id Conclusion In this blog post we took a quick look at how we can utilize the AppRole authentication method in HashiCorp Vault without sacrificing the benefit of named or unique identities from an auditing perspective.\n","date":"01 Dec, 2021","image":"\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://www.datocms-assets.com/2885/1620159869-brandvaultprimaryattributedcolor.svg\" alt=\"\" class=\"w-100 img-fluid rounded\" height=\"\" width=\"\"\u003e\n","permalink":"https://martezr.github.io/greenreedtech-website/hashicorp-vault-unique-approle-identity-logging/","tags":["DevOps","HashiCorp","Security","Vault"],"title":"HashiCorp Vault Unique AppRole Identity Logging"},{"categories":["VMware"],"contents":"Network booting operating systems isn’t a new concept and has been around for years. Bare metal deployments are typically where network booting is commonly used. Most network cards are equipped with a network boot rom that enables the server to boot from the network using the PXE protocol. The PXE protocol is an old protocol that offers limited functionality. iPXE (https://ipxe.org/) is an open source network boot firmware that extends PXE with additional functionality. Network booting comes with a number of challenges, primarily configuring and managing the associated infrastructure required (DHCP, TFTP, etc.). TFTP isn’t typically used in most IT environments and DHCP isn’t commonly available on networks or VLANs used for hosting servers.\nNetwork booting vSphere VMs\nSimilar to bare metal servers a VMware vSphere virtual machine supports network booting via the PXE protocol embedded in the virtual network card boot rom. As we mention earlier, the PXE protocol is limited in terms of functionality and requires a DHCP and TFTP server on the network. iPXE enables offers more functionality than PXE but we need a way to use the iPXE protocol instead of PXE. This is sometimes done through a process known as chain loading in which we boot iPXE with PXE but we still need DHCP and TFTP. We could also burn an ISO image to eliminate the DHCP and TFTP requirement but that means every VM needs to have an ISO mounted.\nThe solution to our problem is to use a custom network boot ROM that includes iPXE. This means that we can quasi natively use iPXE instead of PXE to network boot without DHCP or TFTP. iPXE supports interacting with VMware guest info to set the network information to boot an OS image from the network.\nWhy network boot vSphere VMs?\nWe now know that network booting is a pain but we have a fairly elegant solution but why even consider it for virtual machines when a template can easily be used. I view this as the next wave of workload management in which virtual machines are stateless shells that can be used for hosting containers that offers a simple one to one mapping with persistent storage handle further up in the proverbial stack. In my opinion this decouples the underlying VM guest operating system from the application and unlocks the ability to patch or update the guest os on the fly. Hosting the OS in memory and not writing it to disk means that every boot is an opportunity to swap out the underlying operating system. Projects like LinuxKit make this all the more viable when you think about hosting a MySQL database container on a minimal container host that only host the database container. This means I can potentially swap out the host OS at a moment’s notice while maintaining continuity at the database layer. This starts to enable a huge shift in how IT workloads are managed.\nBuilding a network boot ROM Now that we know that we can replace the network boot rom of a virtual machine we need to create an iPXE boot rom to replace the VMware shipped network boot rom.\nReduce ROM Size\nIn order to boot iPXE from the custom network boot rom we need to disable some of the iPXE features that are enabled by default. This is due to a size limitation documented in this Github issue (https://github.com/ipxe/ipxe/issues/168). After some testing the one big component that would get us under the 64kb limit on the network boot rom was unfortunately HTTPS support. We could easily chainload a version of iPXE via HTTP to then use an image with HTTPS support that ultimately loads the operating system that we want.\nEnable VMware Settings\nThe VMware guest info support is easily added by uncommented one of the VMware iPXE setting in the config/settings.h file before building the rom (https://ipxe.org/buildcfg/vmware_settings).\nBuild the ROM\nIn order to build the network boot rom for our vSphere VM requires a number of tools but we can use Docker to avoid installing a bunch of tools on our local machine. I’ve simplified this process by creating automation to handle the end to end process in this Github repo – https://github.com/martezr/vsphere-network-boot-rom-builder.\nMove the boot ROM\nWith the network boot rom built we now need to move it somewhere that can be accessed by our ESXi host. There are two common options of where to place the rom, in the virtual machine directory on the ESXi datastore or in a global location that isn’t tied to the lifecycle of an individual VM. In this case the global location made sense to avoid needing to keep track of rom build versions across potentially dozens or hundreds of VMs.\nThe custom network boot rom can be uploaded to vSphere in a number of ways but I often use SCP to simply copy it to a datastore of a single ESXi host. The rom can be uploaded to a shared datastore hosted on any of the supported storage backends such as NFS or iSCSI. Here’s an example scp command to copy the boot rom to an ESXi host.\nscp bins/15ad07b0.rom root@grtesxi02.grt.local:/vmfs/volumes/5826d416-0504a9f8-cf5f-0026b954baa6 Creating a Virtual Machine With the iPXE network boot rom created we now need to create a new virtual machine that will act as our shell for booting our operating system. All of the iPXE configuration will be handled using guest information presented to the virtual machine. We just need to provide the information during the provisioning process. The network boot rom related values (nx3bios.filename and ethernet0.opromsize) are covered in this blog post (https://thewayeye.net/2012/april/1/pxe-booting-virtual-machines-using-vmware-fusionworkstation-and-gpxe-or-ipxe/) along with the appropriate values for other network adapter types such as e1000 and e1000e.\nNameDescriptionValueguestinfo.ipxe.filenameThehttp://boot.ipxe.org/demo/boot.phpguestinfo.ipxe.scriptletAn iPXE script to run on bootifopen net0 \u0026amp;\u0026amp; chain ${filename}guestinfo.ipxe.net0.ipThe IP address used by the first network interface during the iPXE boot phase10.0.0.10guestinfo.ipxe.net0.gatewayThe network gateway used by the first network interface during the iPXE boot phase10.0.0.1guestinfo.ipxe.dnsThe DNS server used during the iPXE boot phase10.0.0.200nx3bios.filenameThe path of the network boot rom relative to the vSphere environment./vmfs/volumes/5826d416-0504a9f8-cf5f-0026b954baa6/15ad07b0.romethernet0.opromsizeThe size in kilobytes of the network rom file58880 There’s several ways to create a virtual machine in vSphere but we’ll be using Terraform in this example to simplify the creation process.\ndata \u0026#34;vsphere_datacenter\u0026#34; \u0026#34;dc\u0026#34; {  name = \u0026#34;GRT\u0026#34; }  data \u0026#34;vsphere_datastore\u0026#34; \u0026#34;datastore\u0026#34; {  name = \u0026#34;Local_Storage_2\u0026#34;  datacenter_id = data.vsphere_datacenter.dc.id }  data \u0026#34;vsphere_compute_cluster\u0026#34; \u0026#34;cluster\u0026#34; {  name = \u0026#34;GRT-Cluster\u0026#34;  datacenter_id = data.vsphere_datacenter.dc.id }  data \u0026#34;vsphere_network\u0026#34; \u0026#34;network\u0026#34; {  name = \u0026#34;VM Network\u0026#34;  datacenter_id = data.vsphere_datacenter.dc.id }  data \u0026#34;vsphere_host\u0026#34; \u0026#34;host\u0026#34; {  name = \u0026#34;grtesxi02.grt.local\u0026#34;  datacenter_id = data.vsphere_datacenter.dc.id }  resource \u0026#34;vsphere_virtual_machine\u0026#34; \u0026#34;vm\u0026#34; {  name = \u0026#34;ipxedemo01\u0026#34;  resource_pool_id = data.vsphere_compute_cluster.cluster.resource_pool_id  datastore_id = data.vsphere_datastore.datastore.id  host_system_id = data.vsphere_host.host.id   num_cpus = 2  memory = 2048  guest_id = \u0026#34;other3xLinux64Guest\u0026#34;  wait_for_guest_net_timeout = 0   network_interface {  network_id = data.vsphere_network.network.id  }   disk {  label = \u0026#34;disk0\u0026#34;  size = 10  }   extra_config = {  \u0026#34;guestinfo.ipxe.scriptlet\u0026#34; = \u0026#34;ifopen net0 \u0026amp;\u0026amp; chain $${filename}\u0026#34;  \u0026#34;guestinfo.ipxe.filename\u0026#34; = \u0026#34;http://boot.ipxe.org/demo/boot.php\u0026#34;  \u0026#34;guestinfo.ipxe.net0.ip\u0026#34; = \u0026#34;10.0.0.11\u0026#34;  \u0026#34;guestinfo.ipxe.net0.gateway\u0026#34; = \u0026#34;10.0.0.1\u0026#34;  \u0026#34;guestinfo.ipxe.dns\u0026#34; = \u0026#34;10.0.0.200\u0026#34;  \u0026#34;nx3bios.filename\u0026#34; = \u0026#34;/vmfs/volumes/60eaabc3-b06bcfb0-091a-e4d3f1d05084/15ad07b0.rom\u0026#34;  \u0026#34;ethernet0.opromsize\u0026#34; = \u0026#34;58880\u0026#34;  }  } Booting the Virtual Machine Now that everything has been configured we’re ready to boot the virtual machine using the network settings associated with the virtual machine. We’re using the iPXE demo linux image for testing the boot process, the image should boot pretty quickly if it everything works as expected.\nReferences The following sites were referenced during the creation of this blog post.\nhttps://ipxe.org/buildcfg/vmware_settings\nhttps://thewayeye.net/2012/april/1/pxe-booting-virtual-machines-using-vmware-fusionworkstation-and-gpxe-or-ipxe/\n","date":"27 Aug, 2021","image":"\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://s3.us-west-2.amazonaws.com/greenreedtech.com/vmware-vsphere-vm-ipxe-boot-without-dhcp/vSphere_ipxe_title.png\" alt=\"\" class=\"w-100 img-fluid rounded\" height=\"\" width=\"\"\u003e\n","permalink":"https://martezr.github.io/greenreedtech-website/vmware-vsphere-vm-ipxe-boot-without-dhcp/","tags":["DevOps","VMware"],"title":"VMware vSphere VM iPXE Boot without DHCP"},{"categories":["HashiCorp"],"contents":"HashiCorp Vault supports a number of authentication methods including methods that utilize what HashiCorp refers to as a \u0026ldquo;trusted platform\u0026rdquo;. These include public clouds such as AWS, Azure and GCP along with platforms like Kubernetes. This method of authentication simplifies the introduction of the initial credential or secret that a workload must present to Vault by making use of information about itself that it already knows. The information that is provided to the instance or Kubernetes pod by the platform is metadata typically in the form of cryptographic data. This metadata is presented to HashiCorp Vault for authentication and verified by an API call to the underlying platform.\nThe question is why isn\u0026rsquo;t this same method used in a VMware vSphere environment to enable this same functionality. All of the other platforms natively present metadata that is difficult to guess and accessible only to that entity from a workload context. VMware vSphere doesn\u0026rsquo;t have a native virtual machine metadata service to enable this functionality. At one point there was a metadata service that was part of the work done with vSphere Integrated OpenStack (VIO) to provide this virtual machine metadata. Ultimately, OpenStack never gained the traction that many anticipated and the technical implementation of the metadata service was difficult in my opinion given the networking requirements for presenting the service to workloads.\nTo overcome the challenge of the platform lacking a native metadata service we can take advantage of a native capability that vSphere has used for years for appliances that are deployed to vSphere. vSphere guest info is used by OVA/OVF appliances to effectively pass information from the admin to the guest operating system via VMware tools. The guest operating system uses the vmtoolsd command to perform an RPC operation that allows the virtual machine to query guest info that has been populated by the administrator.\nSolution Overview Now that we have some background and context on the problem let\u0026rsquo;s take a look into the solution. We need to present the virtual machine with a unique piece of data that no other virtual can fetch or easily guess. HashiCorp Vault provides an AppRole authentication method that is ideally used for machine authentication. The AppRole requires a role ID and a secret ID to be presented to Vault to authenticate. The solution is to use an external system such as VMware Event Broker Appliance to populate a virtual machine\u0026rsquo;s guest info with the role ID and Secret ID from Vault. This is only accessible by the virtual machine itself and vSphere accounts with the appropriate permissions. VMware tools allows the guest OS to query the credentials and ultimately use them to authenticate to Vault. Now we\u0026rsquo;re ready to walk through an example of configuring Vault and VEBA to enable this authentication method.\nVault Configuration The first thing we need to is configure HashiCorp Vault to enable and configure AppRole authentication. We also need to create a Vault policy and identity/token for our VEBA function to interact with Vault.\nEnable Vault AppRole authentication method\nvault auth enable approle Create a Vault policy to associate with the virtual machine. Write the following policy to a file named vsphereapp-policy.hcl.\n# Read-only permission on \u0026#39;secret/vspheresecret\u0026#39; path path \u0026#34;secret/data/vspheresecret\u0026#34; {  capabilities = [ \u0026#34;read\u0026#34; ] } Create the Vault policy using the policy file that was just created.\nvault policy write vsphereapp vsphereapp-policy.hcl\nCreate an AppRole role with the policy that was just created assigned.\nvault write auth/approle/role/vsphereapp token_policies=\u0026#34;vsphereapp\u0026#34; secret_id_ttl=10m token_ttl=20m token_max_ttl=180m Create the secret that the policy grants access to.\nvault kv put secret/vspheresecret password=SuperSecurePassword VEBA Function Now that Vault is configured, we\u0026rsquo;re ready to deploy the VEBA function but there are a few things that need to be done first.\nClone the example github repository\ngit clone https://github.com/martezr/vauth Change the working directory to the vebavauth directory\ncd vauth/vebavauth Update the vebaconfig.toml file with the appropriate vCenter and Vault details.\n[vcenter] server = \u0026#34;grtvcenter01.grt.local\u0026#34; user = \u0026#34;administrator@vsphere.local\u0026#34; password = \u0026#34;password\u0026#34; insecure = true  [vault] server = \u0026#34;http://grtmanage01.grt.local:8200\u0026#34; token = \u0026#34;vaultpassword\u0026#34; Update the stack.yml file with the correct gateway for the VMware Event Broker Appliance in your environment.\nversion: 1.0 provider:  name: openfaas  gateway: https://grtveba01.grt.local functions:  vebavauth:  lang: golang-http  handler: ./handler  image: public.ecr.aws/i4r5n0t9/vebavauth:latest  environment:  write_debug: true  read_debug: true  function_debug: false  secrets:  - vebaconfig  annotations:  topic: VmPoweredOnEvent Log into OpenFaaS\nVEBA_GATEWAY=https://grtveba01.grt.local export OPENFAAS_URL=${VEBA_GATEWAY} cat ~/faas_pass.txt | faas-cli login -g https://grtveba01.grt.local -u admin --password-stdin --tls-no-verify Create the OpenFaas secret\nfaas-cli secret create vebaconfig --from-file=vebaconfig.toml --tls-no-verify Deploy the function\nfaas-cli deploy -f stack.yml --tls-no-verify Virtual Machine Configuration The VEBA function will inject the Vault AppRole role ID and secret ID into the VM\u0026rsquo;s advanced settings when the virtual machine is powered on but it first must be assigned a role in the custom attributes. The attribute name must be vauth-role as that is what the VEBA function uses when interacting the HashiCorp Vault instance.\nThe guest operating system now has access to the approle role name, role id and secret id to authenticate to HashiCorp Vault. A configuration management tool such as Ansible, Chef or Puppet could be used to run the command to query the guest info and subsequently generate a HashiCorp Vault agent configuration with that information. We won\u0026rsquo;t delve into the specifics of how to automate that end to end process in this post but we\u0026rsquo;ll perform the authentication operation using a script. On the guest operating system (CentOS 7 in this example) create a file name vaultlogin.sh that handle interacting with the VMware guest info and fetching the secret from Vault.\n#!/bin/bash  VAULTADDR=$1 ROLE=$(/usr/bin/vmtoolsd --cmd \u0026#34;info-get guestinfo.vault.role\u0026#34;) ROLEID=$(/usr/bin/vmtoolsd --cmd \u0026#34;info-get guestinfo.vault.roleid\u0026#34;) SECRETID=$(/usr/bin/vmtoolsd --cmd \u0026#34;info-get guestinfo.vault.secretid\u0026#34;)  # Create JSON payload with role and secret IDs cat \u0026lt;\u0026lt; EOF \u0026gt; payload.json { \u0026#34;role_id\u0026#34;: \u0026#34;$ROLEID\u0026#34;, \u0026#34;secret_id\u0026#34;: \u0026#34;$SECRETID\u0026#34; } EOF # Fetch Vault token  TOKENDATA=$(curl -s --request POST --data @payload.json $VAULTADDR/v1/auth/approle/login)  # Extract Vault token  TOKEN=$(echo $TOKENDATA | python -c \\  \u0026#39;import json,sys;print json.load(sys.stdin)[\u0026#34;auth\u0026#34;][\u0026#34;client_token\u0026#34;]\u0026#39;)  # Output Vault token  echo $TOKEN  # Fetch example secret  curl -s --header \u0026#34;X-Vault-Token: $TOKEN\u0026#34; $VAULTADDR/v1/secret/data/vspheresecret | python -m json.tool With the script created we are ready to run the script, the vault address must be passed to the script.\n./vaultlogin.sh http://grtvault01.grt.local:8200 The script should return the following output which is the token used for authentication and the JSON payload for the Vault secret. The authentication token output is for testing purposes only and should be removed for production environments.\ns.XBfeDjLCamnxpTik5zMpkNa8 {  \u0026#34;auth\u0026#34;: null,  \u0026#34;data\u0026#34;: {  \u0026#34;data\u0026#34;: {  \u0026#34;password\u0026#34;: \u0026#34;SuperSecurePassword\u0026#34;  },  \u0026#34;metadata\u0026#34;: {  \u0026#34;created_time\u0026#34;: \u0026#34;2021-02-16T16:09:47.867249231Z\u0026#34;,  \u0026#34;deletion_time\u0026#34;: \u0026#34;\u0026#34;,  \u0026#34;destroyed\u0026#34;: false,  \u0026#34;version\u0026#34;: 2  }  },  \u0026#34;lease_duration\u0026#34;: 0,  \u0026#34;lease_id\u0026#34;: \u0026#34;\u0026#34;,  \u0026#34;renewable\u0026#34;: false,  \u0026#34;request_id\u0026#34;: \u0026#34;dd2e325a-09f5-d416-6fb8-7b50041cd204\u0026#34;,  \u0026#34;warnings\u0026#34;: null,  \u0026#34;wrap_info\u0026#34;: null } There are a number of updates that can be made to the function to enable it to be used in a real environment but this blog post is to showcase how the solution can be used to simplify the authentication of VMware vSphere workloads to HashiCorp Vault.\nReferences https://www.vaultproject.io/docs/auth/approle https://www.vaultproject.io/api/secret/kv/kv-v2\n","date":"16 Feb, 2021","image":"\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://s3.us-west-2.amazonaws.com/greenreedtech.com/vmware-vsphere-vm-ipxe-boot-without-dhcp/vSphere_ipxe_title.png\" alt=\"\" class=\"w-100 img-fluid rounded\" height=\"\" width=\"\"\u003e\n","permalink":"https://martezr.github.io/greenreedtech-website/hashicorp-vault-vsphere-authentication-with-vmware-event-broker-appliance-veba/","tags":["DevOps","HashiCorp","Security","Vault"],"title":"HashiCorp Vault vSphere Authentication with VMware Event Broker Appliance (VEBA)"},{"categories":["Career"],"contents":"Security of a HashiCorp Vault deployment is of paramount importance given the sensitive nature of the information contained within the platform. During the initial configuration process the root token is used to perform the setup and should be used to create less privileged named accounts. These accounts should be used for day to day administration of the Vault deployment and the root token should only be used in scenarios where it is absolutely necessary. The reason for this is the all-powerful privileges that the root token wields on the platform. Based upon this information it is critical to know whenever the root token is used to log into the Vault deployment and that\u0026rsquo;s what will be covered in this blog post.\nAll operations in HashiCorp Vault are audited and can be sent to an audit log or syslog that is ultimately shipped to a centralized logging server for greater security. In this example we want to utilize the audit log to find out when the root token is used for a login operation. We\u0026rsquo;ll use Splunk for our centralized logging server.\nVault Configuration The Vault instance needs to be configured to ship the log files to our logging server and in this example the logging server is a Splunk instance. This post assumes that you know how to configure Vault audit logging and ship the logs to a centralized logging server. Details on how to configure this are configured in a previous post (https://www.greenreedtech.com/vault-audit-logging/) but at a high level audit logging must be enabled in Vault, configured to write to a log file or syslog and configured to ship those logs to a centralized logging server.\nSplunk Configuration With the audit log now being shipped to our Splunk instance we can see the operations occurring in the Vault cluster. Now we need to figure out what a root login looks like in the audit log. The easiest way to do this was to perform a root login and identify what that operation was from a log perspective. The payload below is a root token login event.\n {  \u0026#34;auth\u0026#34;: {  \u0026#34;accessor\u0026#34;: \u0026#34;hmac-sha256:67869871d870282745682c729d86cee81acb5346c3dbecb573b7d44ea5506d06\u0026#34;,  \u0026#34;client_token\u0026#34;: \u0026#34;hmac-sha256:8fe52f85c93aad7df87c7203f864a9900d25451a1cc88c486ae0c951bd3a8936\u0026#34;,  \u0026#34;display_name\u0026#34;: \u0026#34;root\u0026#34;,  \u0026#34;policies\u0026#34;: [  \u0026#34;root\u0026#34;  ],  \u0026#34;token_policies\u0026#34;: [  \u0026#34;root\u0026#34;  ],  \u0026#34;token_type\u0026#34;: \u0026#34;service\u0026#34;  },  \u0026#34;request\u0026#34;: {  \u0026#34;client_token\u0026#34;: \u0026#34;hmac-sha256:8fe52f85c93aad7df87c7203f864a9900d25451a1cc88c486ae0c951bd3a8936\u0026#34;,  \u0026#34;client_token_accessor\u0026#34;: \u0026#34;hmac-sha256:67869871d870282745682c729d86cee81acb5346c3dbecb573b7d44ea5506d06\u0026#34;,  \u0026#34;id\u0026#34;: \u0026#34;3ab2651a-899b-0a98-c626-73c405d89d02\u0026#34;,  \u0026#34;namespace\u0026#34;: {  \u0026#34;id\u0026#34;: \u0026#34;root\u0026#34;  },  \u0026#34;operation\u0026#34;: \u0026#34;read\u0026#34;,  \u0026#34;path\u0026#34;: \u0026#34;auth/token/lookup-self\u0026#34;,  \u0026#34;remote_address\u0026#34;: \u0026#34;10.0.0.70\u0026#34;  },  \u0026#34;response\u0026#34;: {  \u0026#34;data\u0026#34;: {  \u0026#34;accessor\u0026#34;: \u0026#34;hmac-sha256:67869871d870282745682c729d86cee81acb5346c3dbecb573b7d44ea5506d06\u0026#34;,  \u0026#34;creation_time\u0026#34;: 1576768685,  \u0026#34;creation_ttl\u0026#34;: 0,  \u0026#34;display_name\u0026#34;: \u0026#34;hmac-sha256:6b89bf27681e54af63afe4a0b936bbf618f8d9b17bcc68df8c11470f7328d745\u0026#34;,  \u0026#34;entity_id\u0026#34;: \u0026#34;hmac-sha256:de212e047ea6043f736d83549f3dae8612c688af0d5a6b4d19a262473c5b8bea\u0026#34;,  \u0026#34;expire_time\u0026#34;: null,  \u0026#34;explicit_max_ttl\u0026#34;: 0,  \u0026#34;id\u0026#34;: \u0026#34;hmac-sha256:8fe52f85c93aad7df87c7203f864a9900d25451a1cc88c486ae0c951bd3a8936\u0026#34;,  \u0026#34;meta\u0026#34;: null,  \u0026#34;num_uses\u0026#34;: 0,  \u0026#34;orphan\u0026#34;: true,  \u0026#34;path\u0026#34;: \u0026#34;hmac-sha256:20039952cb073210bc9cb0fa1dc3dec3e49bcd8a72b5dd2a9f9ce415010c91a0\u0026#34;,  \u0026#34;policies\u0026#34;: [  \u0026#34;hmac-sha256:6b89bf27681e54af63afe4a0b936bbf618f8d9b17bcc68df8c11470f7328d745\u0026#34;  ],  \u0026#34;ttl\u0026#34;: 0,  \u0026#34;type\u0026#34;: \u0026#34;hmac-sha256:05148f41a98c981f657d9a0cb0b647e1f32a764719da2e75f27a497485eb9b7a\u0026#34;  }  },  \u0026#34;time\u0026#34;: \u0026#34;2020-01-31T13:14:37.132982729Z\u0026#34;,  \u0026#34;type\u0026#34;: \u0026#34;response\u0026#34; } Now that we know what we want to look for we can create our Splunk search string to return only root login events.\nauth.display_name=\u0026#34;root\u0026#34; type=\u0026#34;response\u0026#34; request.path=\u0026#34;auth/token/lookup-self\u0026#34; If there has been a recent login with the root token we should see results from our search similar to those below.\nThis can be extended to send a notification or create a helpdesk ticket, most logging platforms have the ability to trigger an action when a designated event occurs. As an example, Splunk supports a handful of alert actions such as sending an email, outputting results to a lookup file or sending a webhook to an external system.\nReferences The following content was used to create this blog post.\nVault Production Guide\nhttps://www.vaultproject.io/guides/operations/production\nSplunk Alert Webhooks\nhttps://docs.splunk.com/Documentation/Splunk/8.0.1/Alert/Webhooks\n","date":"01 Feb, 2021","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cpicture\u003e\n  \u003csource srcset=\"/greenreedtech-website/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_545x0_resize_q95_h2_box.webp\" media=\"(max-width: 575px)\"\u003e\n  \u003csource srcset=\"/greenreedtech-website/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_600x0_resize_q95_h2_box.webp\" media=\"(max-width: 767px)\"\u003e\n  \u003csource srcset=\"/greenreedtech-website/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_700x0_resize_q95_h2_box.webp\" media=\"(max-width: 991px)\"\u003e\n  \u003csource srcset=\"/greenreedtech-website/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_h2_box.webp\"\u003e\n  \u003cimg loading=\"lazy\" decoding=\"async\" class=\"w-100 img-fluid rounded\" src=\"/greenreedtech-website/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_box.jpg\" alt=\"\" width=\"970\" height=\"500\"\u003e\n\u003c/picture\u003e\n \n \n \n\n","permalink":"https://martezr.github.io/greenreedtech-website/detecting-hashicorp-vault-root-login/","tags":["DevOps","Terraform","VMware"],"title":"Detecting HashiCorp Vault Root Login"},{"categories":["Career"],"contents":"HashiCorp Vault generates a default root token during installation and best practice dictates that the token should be revoked once the deployment has been setup. There are certain critical operations that can only be carried out by a root token and requires that a new root token be generated. Given the immense power that the root token garners it would be ideal to identify when a root token is generated. In this example we\u0026rsquo;ll utilize the Vault audit log to determine when the process to generate a new root token is started and when it is successfully completed. Splunk will be used as our centralized logging server in this example.\nVault 1.4.0 added logging for root token generation events to the audit log to enable the easy identification of these events. Prior to Vault 1.4.0 there was no record of this operation in the Vault audit log to identify when a new root token is generated. The Vault system log output did show the execution of the root token generation commands.\nThe generation of a new root token involves multiple steps such as initializing the operation which generates an OTP, providing the unseal information and finally decoding the encode version of the generated root token.\nDetect Root Token Init\nWe want to be able to detect when someone attempts to generate a root token. The following command is used to initialize the root token generation process from the command line.\nvault operator generate-root -init The following search string returns the appropriate entry from the Vault audit log for this event.\nsource=\u0026#34;/opt/vault/logs/vault_audit.log\u0026#34; type=response request.path=\u0026#34;sys/generate-root/attempt\u0026#34; request.operation=\u0026#34;update\u0026#34; Successful Root Token Generation\nWe want to be able to detect when someone is able to successfully generate a new root token. The following search string returns the audit log entry from when the encoded token is generated.\nsource=\u0026#34;/opt/vault/logs/vault_audit.log\u0026#34; type=response request.path=\u0026#34;sys/generate-root/update\u0026#34; response.data.complete=true References The following content was used to create this blog post:\nRoot Token Generation Audit Log Pull Request\nhttps://github.com/hashicorp/vault/pull/8301\nVault Root Token Generation Lesson\nhttps://learn.hashicorp.com/vault/operations/ops-generate-root\nVault Root Token Command Reference\nhttps://www.vaultproject.io/docs/commands/operator/generate-root\n","date":"01 Feb, 2021","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cpicture\u003e\n  \u003csource srcset=\"/greenreedtech-website/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_545x0_resize_q95_h2_box.webp\" media=\"(max-width: 575px)\"\u003e\n  \u003csource srcset=\"/greenreedtech-website/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_600x0_resize_q95_h2_box.webp\" media=\"(max-width: 767px)\"\u003e\n  \u003csource srcset=\"/greenreedtech-website/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_700x0_resize_q95_h2_box.webp\" media=\"(max-width: 991px)\"\u003e\n  \u003csource srcset=\"/greenreedtech-website/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_h2_box.webp\"\u003e\n  \u003cimg loading=\"lazy\" decoding=\"async\" class=\"w-100 img-fluid rounded\" src=\"/greenreedtech-website/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_box.jpg\" alt=\"\" width=\"970\" height=\"500\"\u003e\n\u003c/picture\u003e\n \n \n \n\n","permalink":"https://martezr.github.io/greenreedtech-website/detecting-hashicorp-vault-root-token-generation/","tags":["DevOps","Terraform","VMware"],"title":"Detecting HashiCorp Vault Root Token Generation"},{"categories":["Career"],"contents":"I have made the decision to become the director of technical marketing at Morpheus Data and I start that new position today. Morpheus Data is a company that I became aware of during Cloud Field Day 3 (CFD3) while serving as a delegate for the event. Cloud Field Day is one of several events hosted by Gestalt IT in which several vendors present one at a time to a group of IT professionals that ask questions and provide feedback about the presentations. I was intrigued by what was covered during the presentation as it is a product that is right in my technical wheelhouse.\nWhat is Morpheus Data? Let\u0026rsquo;s get the first question out of the way for those that may have never heard of Morpheus Data. Morpheus Data is a software platform that fits in the Cloud Management Platform (CMP) space and can be be used for a lot more. One of the fundamental challenges it helps address is the complexity of provisioning workloads, especially on-premises workloads that often require multiple manual hand offs between teams to fully provision a server. The Morpheus platform has over 80 integrations with common tools and platforms such as Ansible, VMware NSX, InfoBlox and more to tie together the various parts of an end to end provisioning workflow. In addition to provisioning the platform has a number of other features that provide organizations with a centralized platform to enable IT agility along with governance at an enterprise scale.\nWhy Morpheus Data? One of the key value propositions of Morpheus Data is tying together disparate tools to help create business value. An aspect of technology that I enjoy most is creating complex orchestration workflows to solve complex problems. The broad number of Morpheus integrations allows me to tap into my varied experiences in IT and make use of the breadth of knowledge that I\u0026rsquo;ve gained during my career. In addition to the technical fit I like the opportunity that smaller companies provide to be able to wear a few different hats to directly impact the success of the organization. Something I took note of as I followed Morpheus Data from a far was the pace at which they were adding new features and capabilities to the platform and the ability to be able to speak to new and interesting things on a regular cadence is very appealing.\nWhy Technical Marketing? The majority of my career has been spent as an IT practitioner in various roles from desktop support to architecting large automation projects. The last several years I have focused on sharing the knowledge that I\u0026rsquo;ve gained over my years in IT. I\u0026rsquo;ve blogged for a number of years and began speaking at tech conferences a couple of years ago so I\u0026rsquo;ve had platforms through which I was able to fulfill that desire. In my role at Morpheus Data I\u0026rsquo;ll be creating marketing content with a focus on the technical aspects of the product and partner integrations. This is a dream role for me as it forces me to continue to grow non hands on keyboard skills such as storytelling and written communication. This role also allows me to to focus on the strategic aspects of helping to grow a business.\nI\u0026rsquo;m extremely excited about the opportunity to help grow Morpheus Data and the new challenge that I am about to begin.\n","date":"01 Feb, 2021","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cpicture\u003e\n  \u003csource srcset=\"/greenreedtech-website/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_545x0_resize_q95_h2_box.webp\" media=\"(max-width: 575px)\"\u003e\n  \u003csource srcset=\"/greenreedtech-website/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_600x0_resize_q95_h2_box.webp\" media=\"(max-width: 767px)\"\u003e\n  \u003csource srcset=\"/greenreedtech-website/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_700x0_resize_q95_h2_box.webp\" media=\"(max-width: 991px)\"\u003e\n  \u003csource srcset=\"/greenreedtech-website/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_h2_box.webp\"\u003e\n  \u003cimg loading=\"lazy\" decoding=\"async\" class=\"w-100 img-fluid rounded\" src=\"/greenreedtech-website/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_box.jpg\" alt=\"\" width=\"970\" height=\"500\"\u003e\n\u003c/picture\u003e\n \n \n \n\n","permalink":"https://martezr.github.io/greenreedtech-website/moving-to-morpheus-data/","tags":["DevOps","Terraform","VMware"],"title":"Moving to Morpheus Data"},{"categories":["Career"],"contents":"In this blog post we\u0026rsquo;ll look at the HashiCorp Vault plugin for Puppet Bolt that enables authentication credentials for Bolt to be retrieved from an instance of HashiCorp Vault. HashiCorp Vault is a secrets management platform that is commonly used to store secrets such as API keys, passwords and SSH private keys. This solution helps to avoid secret sprawl where passwords and credentials are widely distributed across an environment making it difficult to track where they are.\nThe plugin was added to Puppet Bolt in version 1.28.0 and natively supports token and userpass Vault authentication methods. Documentation for the plugin can be found here - https://forge.puppet.com/puppetlabs/vault/readme.\nVault Setup We\u0026rsquo;ll configure a development instance of HashiCorp Vault to walk through the plugin\u0026rsquo;s functionality. The following assumes a basic knowledge of how to setup at least a dev instance of HashiCorp Vault. This information can be found on HashiCorp\u0026rsquo;s website if necessary (https://learn.hashicorp.com/vault).\nWrite the Windows administrator password to Vault.\nWe need to write the password for the Windows machine to the secret/credentials/windows path in Vault.\nvault kv put secret/credentials/windows password=Puppet123 Write the Linux SSH private key to Vault\nWe need to write the SSH private key for the Linux machine to the secret/credentials/linux path in Vault. The private key has been saved to a file named bolt_id_rsa which is being uploaded as a secret.\nvault kv put secret/credentials/linux privatekey=@bolt_id_rsa Create a Vault Policy\nWe need to create a Vault Policy that allows the token to read the secrets in the \u0026ldquo;credentials\u0026rdquo; secret structure but nothing else under the \u0026ldquo;secret\u0026rdquo; space. The policy should be saved to a file name bolt-policy.hcl which we\u0026rsquo;ll use in the next command to actually create the policy in Vault.\npath \u0026#34;secret/data/credentials/*\u0026#34; {  capabilities = [\u0026#34;read\u0026#34;] } path \u0026#34;secret/metadata/credentials/*\u0026#34; {  capabilities = [\u0026#34;list\u0026#34;,\u0026#34;read\u0026#34;] } The following command creates a policy named \u0026ldquo;bolt\u0026rdquo; using the policy file that was just created in the previous step.\nvault policy write bolt bolt-policy.hcl Token Authentication HashiCorp Vault supports token authentication that allows a token generated by an existing token or another authentication method to be used for interacting with a Vault instance. When logging into Vault via an authentication method a token is generated which is assigned privileges based upon the policies associated with the token during creation.\nGenerate Token\nLogged in with the root token we can generate a token with the bolt policy we created in a previous step associated.\nLogging in with the root token is not recommended for day to day administration in production environments\nvault token create -policy=bolt The output should be similar to that below and the \u0026ldquo;token\u0026rdquo; is what we\u0026rsquo;ll use in our Bolt inventory file to authenticate.\nKey Value — —– token s.3649w1Fh80RtwSteoDzWuDUi token_accessor Ki4onGqPfwdnMVJQFX40ddqZ token_duration 768h token_renewable true token_policies [\u0026#34;bolt\u0026#34; \u0026#34;default\u0026#34;] identity_policies [] policies [\u0026#34;bolt\u0026#34; \u0026#34;default\u0026#34;] Bolt Configuration File\nThe Bolt configuration file is used to set the global configuration for Bolt and in this example we\u0026rsquo;re adding the configuration for the Vault plugin to this file. The token has been added in plaintext to the file but we can specify the \u0026ldquo;VAULT_TOKEN\u0026rdquo; environment variable or use another plugin for encryption such as the PKCS7 to avoid the token being in plaintext in the Bolt config file.\nmodulepath: \u0026#34;~/.puppetlabs/bolt-code/modules:~/.puppetlabs/bolt-code/site-modules\u0026#34; concurrency: 10 format: human winrm:  ssl: false ssh:  host-key-check: false plugins:  vault:  server_url: http://127.0.0.1:8200  auth:  method: token  token: s.3649w1Fh80RtwSteoDzWuDUi In addition to generating a token using an existing token HashiCorp Vault generates a token upon login when using other authentication methods, such as those covered below.\nHuman Interaction\nHashiCorp Vault supports a number of authentication methods that are intended for a human or interactive login. The following methods are commonly utilized but is not a complete list.\n LDAP Okta Radius Github  Machine Interaction\nThere is often a need to run automation as part of a pipeline or scheduled task. This means that we can\u0026rsquo;t expect a human to perform a login operation to fetch a token. In this case we need to use one of HashiCorp Vault\u0026rsquo;s authentication methods intended for non-human interaction.\nThe following authentication methods are intended for non-human authentication.\n AWS Kubernetes TLS Azure AppRole  UserPass Authentication In addition to the token authentication method the plugin also supports userpass authentication. HashiCorp Vault supports a userpass authentication method that is a local user database in Vault that utilizes a username and password for authentication.\nEnable UserPass Authentication\nThe authentication engine or backend needs to be enabled before we can use that authentication method.\nvault auth enable userpass Create a user account\nThe userpass authentication method has been enabled and now we need to create a user account with a password and associate the Bolt Vault policy.\nvault write auth/userpass/users/puppetbolt password=Password123 policies=bolt We can validate that the user was successfully created and that the bolt policy is associated by running the vault login command below, a password prompt will be presented\nvault login -method=userpass username=puppetbolt Output similar to that shown below will be displayed and we can see that the \u0026ldquo;bolt\u0026rdquo; policy is associated with the credentials.\nSuccess! You are now authenticated. The token information displayed below is already stored in the token helper. You do NOT need to run \u0026#34;vault login\u0026#34; again. Future Vault requests will automatically use this token.  Key Value — —– token s.Xd4v1qoCtnKEnDHjzYTRm1KC token_accessor 1ZRZUUYfWRJOGNBj8qnbRGvf token_duration 768h token_renewable true token_policies [\u0026#34;bolt\u0026#34; \u0026#34;default\u0026#34;] identity_policies [] policies [\u0026#34;bolt\u0026#34; \u0026#34;default\u0026#34;] token_meta_username puppetbolt Bolt Configuration File\nThe Bolt configuration file is used to set the global configuration for Bolt and in this example we’re adding the configuration for the Vault plugin to this file. The username and password have been added in plaintext to the file. Similar to the token authentication method we can use another plugin for encryption such as the PKCS7 to avoid the password being in plaintext in the Bolt config file.\nmodulepath: \u0026#34;~/.puppetlabs/bolt-code/modules:~/.puppetlabs/bolt-code/site-modules\u0026#34; concurrency: 10 format: human winrm:  ssl: false ssh:  host-key-check: false plugins:  vault:  server_url: http://127.0.0.1:8200  auth:  method: userpass  user: puppetbolt  pass: Password123 Bolt Inventory File With either the authentication method configured for the Vault plugin now we just need to create an inventory file for specifying the path in Vault where Bolt will fetch the secret from.\nWindows\nThe Bolt inventory file below is an example of using the plugin to retrieve the password used by Bolt for connecting to Windows nodes via WinRM.\nversion: 2 targets:  – uri: winnode1  config:  transport: winrm  winrm:  user: administrator  password:  _plugin: vault  path: secret/credentials/windows  field: password  version: 2 With the inventory file created we can run a simple command to check that the plugin is able to fetch the credentials from Vault.\nbolt plan run facts -i inventory.yaml –targets=winnode1 Linux\nThe Bolt inventory file below is an example of using the plugin to retrieve the SSH private key used by Bolt for connecting to Linux nodes via SSH.\nversion: 2 targets:  – uri: linuxnode1  config:  transport: ssh  ssh:  user: root  private-key:  key-data:  _plugin: vault  path: secret/credentials/linux  field: privatekey  version: 2 With the inventory file created we can run a simple command to check that the plugin is able to fetch the credentials from Vault.\nbolt plan run facts -i inventory.yaml –targets=linuxnode1 The plugin provides the ability to allow Puppet Bolt to offload a critical component of any automation process to a dedicated platform in Vault. This enables a more robust solution for managing secrets in a secure and automated manner.\nReferences Puppet Bolt HashiCorp Vault plugin documentation\nhttps://forge.puppet.com/puppetlabs/vault/readme\nPuppet Bolt Inventory File\nhttps://puppet.com/docs/bolt/latest/inventory_file_v2.html\n","date":"01 Feb, 2021","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cpicture\u003e\n  \u003csource srcset=\"/greenreedtech-website/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_545x0_resize_q95_h2_box.webp\" media=\"(max-width: 575px)\"\u003e\n  \u003csource srcset=\"/greenreedtech-website/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_600x0_resize_q95_h2_box.webp\" media=\"(max-width: 767px)\"\u003e\n  \u003csource srcset=\"/greenreedtech-website/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_700x0_resize_q95_h2_box.webp\" media=\"(max-width: 991px)\"\u003e\n  \u003csource srcset=\"/greenreedtech-website/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_h2_box.webp\"\u003e\n  \u003cimg loading=\"lazy\" decoding=\"async\" class=\"w-100 img-fluid rounded\" src=\"/greenreedtech-website/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_box.jpg\" alt=\"\" width=\"970\" height=\"500\"\u003e\n\u003c/picture\u003e\n \n \n \n\n","permalink":"https://martezr.github.io/greenreedtech-website/puppet-bolt-vault-inventory-plugin/","tags":["DevOps","Terraform","VMware"],"title":"Puppet Bolt Vault Inventory Plugin"},{"categories":["DevOps"],"contents":"HashiCorp Terraform is a popular Infrastructure as Code (IaC) tool that is used for provisioning virtual machines or cloud instances along with other resources. Once a virtual machine or cloud instance is provisioned typically it still needs to be configured which includes security baselines, application dependency configuration and even application deployment. The tasks are generally accomplished with a Configuration Management (CM) tool such as Puppet or Puppet Bolt.\nThe Puppet Bolt Terraform inventory plugin parses the Terraform state for resources that have been created by Terraform and it enables Bolt to be run against infrastructure created by Terraform without explicitly specifying connection information such as the IP address. This is especially useful in public cloud environments where the IP address associated with an instance is often dynamic. The Puppet Bolt Terraform plugin supports state retrieval for local state backends as well as remote backends.\nIn this blog post we\u0026rsquo;ll look at how to use the Puppet Bolt Terraform inventory plugin to dynamically retrieve the IP address of a virtual machine provisioned in a VMware vSphere environment which will allow us to run Puppet Bolt against the virtual machine using dynamic information.\nTerraform Manifest The Terraform code creates a single CentOS 7 virtual machine in a VMware vSphere environment. The Terraform state is stored in HashiCorp Consul for centralized state management.\nterraform {  backend \u0026#34;consul\u0026#34; {  address = \u0026#34;10.0.0.6:8500\u0026#34;  scheme = \u0026#34;http\u0026#34;  path = \u0026#34;terraform/bolt/boltserver/terraform.tfstate\u0026#34;  datacenter = \u0026#34;puppet-bolt\u0026#34;  } }  resource \u0026#34;vsphere_virtual_machine\u0026#34; \u0026#34;boltserver\u0026#34; {  name = \u0026#34;boltserver\u0026#34;  num_cpus = 2  memory = 4096   resource_pool_id = \u0026#34;${data.vsphere_compute_cluster.cluster.resource_pool_id}\u0026#34;  datastore_id = \u0026#34;${data.vsphere_datastore.datastore.id}\u0026#34;   guest_id = \u0026#34;${data.vsphere_virtual_machine.template.guest_id}\u0026#34;  scsi_type = \u0026#34;${data.vsphere_virtual_machine.template.scsi_type}\u0026#34;   network_interface {  network_id = \u0026#34;${data.vsphere_network.network.id}\u0026#34;  adapter_type = \u0026#34;${data.vsphere_virtual_machine.template.network_interface_types[0]}\u0026#34;  }   disk {  label = \u0026#34;disk0\u0026#34;  size = \u0026#34;${data.vsphere_virtual_machine.template.disks.0.size}\u0026#34;  eagerly_scrub = \u0026#34;${data.vsphere_virtual_machine.template.disks.0.eagerly_scrub}\u0026#34;  thin_provisioned = \u0026#34;${data.vsphere_virtual_machine.template.disks.0.thin_provisioned}\u0026#34;  }   clone {  template_uuid = \u0026#34;${data.vsphere_virtual_machine.template.id}\u0026#34;   customize {  linux_options {  host_name = \u0026#34;boltserver\u0026#34;  domain = \u0026#34;grt.local\u0026#34;  }   network_interface {}  dns_server_list = [\u0026#34;10.0.0.200\u0026#34;]  ipv4_gateway = \u0026#34;10.0.0.1\u0026#34;  }  } } The virtual machine can be provisioned using the standard Terraform workflow of init, plan and apply.\nPuppet Bolt Inventory The Puppet Bolt inventory file defines resource information such as how to the connect to the resource such as the IP address and credentials for the resource. With the virtual machine provisioned the Bolt inventory file needs to be created to dynamically access the virtual machine.\nDisplayed below is the Puppet Bolt inventory file used in this example.\nversion: 2 groups:  – name: boltservers  targets:  – _plugin: terraform  resource_type: vsphere_virtual_machine.boltserver  uri: default_ip_address  dir: .  name: name  backend: remote Inventory Properties The Terraform plugin accepts a number of parameters for interacting with the Terraform state.\nresource_type: The Terraform resource that should be matched with support for regular expressions (i.e. - resource_type.resource_name).\nuri: The property of the Terraform resource used to connect to the resource such as the IP address or DNS name.\ndir: The inventory plugin executes the Terraform state pull command to retrieve the state. The directory specified should be where the Terraform manifests for the resources resided.\nname: The property of the Terraform resource to use as the target name\nbackend: The type of backend to load the Terraform state from. The two supported types are local and remote.\nPuppet Bolt Plan With the resource provisioned and the inventory file configured Puppet Bolt can be run against the provisioned resource without explicitly defining the IP address or DNS name.\nThe command below runs Puppet Bolt against the virtual machine provisioned with Terraform. In this example we\u0026rsquo;re just using the facts plan that comes with Bolt to show the functionality but a much more complex plan could be used to deploy an application or configure the security baseline as an example.\nbolt plan run facts -i inventory.yaml nodes=boltservers The command generates output similar to that shown below which dynamically retrieved the IP address to access the virtual machine.\nbolt plan run facts -i inventory.yaml nodes=boltservers Starting: plan facts Starting: task facts on 10.0.0.15 Finished: task facts with 0 failures in 5.94 sec Finished: plan facts in 5.97 sec Finished on 10.0.0.15:  {  \u0026#34;os\u0026#34;: {  \u0026#34;name\u0026#34;: \u0026#34;CentOS\u0026#34;,  \u0026#34;distro\u0026#34;: {  \u0026#34;codename\u0026#34;: \u0026#34;Core \u0026#34;  },  \u0026#34;release\u0026#34;: {  \u0026#34;full\u0026#34;: \u0026#34;7.6.1810\u0026#34;,  \u0026#34;major\u0026#34;: \u0026#34;7\u0026#34;,  \u0026#34;minor\u0026#34;: \u0026#34;6\u0026#34;  },  \u0026#34;family\u0026#34;: \u0026#34;RedHat\u0026#34;  }  } Successful on 1 node: boltserver Ran on 1 node This integration greatly simplifies configuring machines provisioned with Terraform and eliminates the need to specify the IP address of virtual machine before running Puppet Bolt. This plugin can be especially useful in deploying complex application stacks with dependencies between machines.\nReference Puppet Bolt Inventory Plugin\nhttps://puppet.com/docs/bolt/latest/inventory_file_v2.html#plugins-and-dynamic-inventory\n","date":"06 Oct, 2019","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cpicture\u003e\n  \u003csource srcset=\"/greenreedtech-website/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_545x0_resize_q95_h2_box.webp\" media=\"(max-width: 575px)\"\u003e\n  \u003csource srcset=\"/greenreedtech-website/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_600x0_resize_q95_h2_box.webp\" media=\"(max-width: 767px)\"\u003e\n  \u003csource srcset=\"/greenreedtech-website/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_700x0_resize_q95_h2_box.webp\" media=\"(max-width: 991px)\"\u003e\n  \u003csource srcset=\"/greenreedtech-website/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_h2_box.webp\"\u003e\n  \u003cimg loading=\"lazy\" decoding=\"async\" class=\"w-100 img-fluid rounded\" src=\"/greenreedtech-website/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_box.jpg\" alt=\"\" width=\"970\" height=\"500\"\u003e\n\u003c/picture\u003e\n \n \n \n\n","permalink":"https://martezr.github.io/greenreedtech-website/puppet-bolt-terraform-inventory-plugin/","tags":["DevOps","Puppet","Terraform"],"title":"Puppet Bolt Terraform Inventory Plugin"},{"categories":["Terraform"],"contents":"What is immutable infrastructure? Immutable infrastructure is the concept of utilizing an infrastructure component in an ephemeral manner. This means that the component can be destroyed and recreated at will without major impact.\nAdvantages\n Troublesome instances can easily be destroyed and recreated. System patching processes are replaced by just provisioning instances from a new template.  Terraform\nTerraform provides us with the ability to create vSphere infrastructure with code. Terraform enables us to quickly tear down and provision new infrastructure which allows us to quickly transition all of our VMs to a new template within a maintenance window.\nvSphere Immutable Infrastructure Design Our example application is a node.js application with a Mongodb backend.\n  Web Server: Deployment of our web instance is fairly simple as the web server stores no data that needs to persists across iterations of the instance.\n  Database Server: Deploying an immutable database server is particularly challenging given the requirement that the data in the database must persist across iterations of the instance.\n  Data persistence is achieved by decoupling the data drive or .VMDK file from the instantiation of the virtual machine. The virtual machine is created and attaches an existing hard disk that stores the data for our Mongodb database. When we destroy the database VM the .VMDK file is detached and the virtual machine is destroyed.\nFolder Layout The design uses three directories to store the Terraform code to allow us to abstract the code that manages the data disk for our database instance from the database instance.\n dbinstantiation: The code in this directory manages the database instance we\u0026rsquo;ll use to prep our data disk. instances: The code in this directory manages the web and database instances. persistentdisks: The code in this directory manages the data disk for the database instance.  vsphereimmutable/ ├── dbinstantiation │ ├── main.tf │ └── mongodbinstall.sh ├── instances │ ├── main.tf │ ├── mongodbinstall.sh │ └── nodeinstall.sh └── persistentdisks └── main.tf Terraform Code Now that we\u0026rsquo;ve walked through the immutable design we\u0026rsquo;ll jump into the Terraform code and helper scripts to build out the design in our vSphere environment.\nAll code used in this example can be found on Github.\nhttps://github.com/martezr/terraform-immutable-vsphere\nDatabase Hard Disk The second hard drive or data drive for our database instance is the first resource we\u0026rsquo;ll need to create using Terraform. The .VMDK file will be stored in the root of the datastore in this example but can easily be placed into a subfolder for persistent disks.\nresource \u0026#34;vsphere_virtual_disk\u0026#34; \u0026#34;DBDisk01\u0026#34; { size = 20 vmdk_path = \u0026#34;DBDisk01.vmdk\u0026#34; datacenter = \u0026#34;Datacenter\u0026#34; datastore = \u0026#34;local\u0026#34; type = \u0026#34;thin\u0026#34; adapter_type = \u0026#34;lsiLogic\u0026#34; } Let\u0026rsquo;s go ahead and run Terraform apply to create the data drive.\nTerraform apply Database Instantiation Instance The next thing we need to do is create the instance to prep the data drive for use with our immutable database instance.\nFirst database boot script We need to partition our data drive separate of the database bootstrap script as we only want to partition the data drive only once.\n#!/bib/bash # Partition the second disk ( echo n # Add a new partition echo p # Primary partition echo 1 # Partition number echo # First sector (Accept default: 1) echo # Last sector (Accept default: varies) echo w # Write changes ) | sudo fdisk /dev/sdb echo yes | mkfs.ext4 /dev/sdb mkdir /mongodb echo \u0026#39;/dev/sdb /mongodb ext4 defaults 0 0\u0026#39; \u0026gt;\u0026gt; /etc/fstab mount -a cat \u0026lt;\u0026lt; EOF \u0026gt; /etc/yum.repos.d/mongodb-org.repo [mongodb-org-3.4] name=MongoDB Repository baseurl=https://repo.mongodb.org/yum/redhat/7Server/mongodb-org/3.4/x86_64/ gpgcheck=1 enabled=1 gpgkey=https://www.mongodb.org/static/pgp/server-3.4.asc EOF yum -y install mongodb-org cat \u0026lt;\u0026lt; EOF \u0026gt; /etc/mongod.conf # where to write logging data. systemLog: destination: file logAppend: true path: /var/log/mongodb/mongod.log # Where and how to store data. storage: dbPath: /mongo journal: enabled: true # engine: # mmapv1: # wiredTiger: # how the process runs processManagement: fork: true # fork and run in background pidFilePath: /var/run/mongodb/mongod.pid # location of pidfile # network interfaces net: port: 27017 # bindIp: 127.0.0.1 # Listen to local interface only, comment to listen on all interfaces. EOF systemctl start mongod Now we need to run Terraform apply to create the instance.\nTerraform apply Once our instance has been successfully bootstrapped using the mongodbprep script we\u0026rsquo;ll need to run the Terraform destroy command to destroy our instantiation instance.\nTerraform destroy Production Instances Now that the data drive for our Mongodb VM has been prepped we just need to provision our production instances and start adding data.\nresource \u0026#34;vsphere_virtual_machine\u0026#34; \u0026#34;Node01\u0026#34; { name = \u0026#34;terraform-node\u0026#34; vcpu = 2 memory = 4096 network_interface { label = \u0026#34;VM Network\u0026#34; ipv4_address = \u0026#34;192.168.1.4\u0026#34; ipv4_prefix_length = \u0026#34;24\u0026#34; ipv4_gateway = \u0026#34;192.168.1.254\u0026#34; } provisioner \u0026#34;file\u0026#34; { source = \u0026#34;nodeinstall.sh\u0026#34; destination = \u0026#34;nodeinstall.sh\u0026#34; connection { type = \u0026#34;ssh\u0026#34; host = \u0026#34;192.168.1.4\u0026#34; user = \u0026#34;root\u0026#34; password = \u0026#34;password\u0026#34; } } provisioner \u0026#34;remote-exec\u0026#34; { connection { type = \u0026#34;ssh\u0026#34; host = \u0026#34;192.168.1.4\u0026#34; user = \u0026#34;root\u0026#34; password = \u0026#34;password\u0026#34; } inline = [ \u0026#34;setenforce 0\u0026#34;, \u0026#34;chmod +x /root/nodeinstall.sh \u0026amp;\u0026amp; sh /root/nodeinstall.sh\u0026#34; ] } disk { template = \u0026#34;centosnodetemp\u0026#34; type = \u0026#34;thin\u0026#34; datastore = \u0026#34;Local_Storage\u0026#34; } } resource \u0026#34;vsphere_virtual_machine\u0026#34; \u0026#34;DB01\u0026#34; { name = \u0026#34;terraform-db\u0026#34; vcpu = 2 memory = 4096 detach_unknown_disks_on_delete = \u0026#34;true\u0026#34; enable_disk_uuid = \u0026#34;true\u0026#34; network_interface { label = \u0026#34;VM Network\u0026#34; ipv4_address = \u0026#34;192.168.1.5\u0026#34; ipv4_prefix_length = \u0026#34;24\u0026#34; ipv4_gateway = \u0026#34;192.168.1.254\u0026#34; } provisioner \u0026#34;file\u0026#34; { source = \u0026#34;mongodbinstall.sh\u0026#34; destination = \u0026#34;mongodbinstall.sh\u0026#34; connection { type = \u0026#34;ssh\u0026#34; host = \u0026#34;192.168.1.5\u0026#34; user = \u0026#34;root\u0026#34; password = \u0026#34;password\u0026#34; } } provisioner \u0026#34;remote-exec\u0026#34; { connection { type = \u0026#34;ssh\u0026#34; host = \u0026#34;192.168.1.5\u0026#34; user = \u0026#34;root\u0026#34; password = \u0026#34;password\u0026#34; } inline = [ \u0026#34;mkdir /mongodb\u0026#34;, \u0026#34;echo \u0026#39;/dev/sdb /mongodb ext4 defaults 0 0\u0026#39; \u0026gt;\u0026gt; /etc/fstab\u0026#34;, \u0026#34;mount -a\u0026#34;, \u0026#34;chmod +x /root/mongodbinstall.sh \u0026amp;\u0026amp; sh /root/mongodbinstall.sh\u0026#34; ] } disk { template = \u0026#34;centos7temp\u0026#34; type = \u0026#34;thin\u0026#34; } disk { vmdk = \u0026#34;DBDisk01.vmdk\u0026#34; datastore = \u0026#34;Local_Storage\u0026#34; type = \u0026#34;thin\u0026#34; keep_on_remove = \u0026#34;true\u0026#34; } } Let\u0026rsquo;s go ahead and run a Terraform apply to build our production instances.\nWith the node.js app started we should be able to access the example node.js app from our web browser.\nhttp://web_instance:8080\nLet\u0026rsquo;s add some todo items to the list so we can validate that the data persists across instantiations of the database virtual machine.\nAdd a few tasks to the list, in our example a few whimsical todo items have been added to the list.\n Datatbase can be immutable too Stateful apps don\u0026rsquo;t have to be pets The data is all that needs to persist  Immutability Test The last thing we need to do now is to actually test the immutability of our design by destroying and rebuilding the infrastructure. From within the \u0026ldquo;instances\u0026rdquo; directory we need to run the terraform destroy command to destroy the node.js and database virtual machines.\nTerraform destroy Once both of our VMs are destroyed we need recreate them using terraform apply and validate that the todo items are still available in our app.\nTerraform apply If everything works as expected then we should see the todo items we created earlier still in the list. This indicates that we were able to destroy and recreate both the web and database instances without losing any data.\nThis solution enables us to decouple the underlying virtual machines from the business value provided by our application and allows us to take advantage of immutable infrastructure.\nReferences Terraform Provisioner Connection https://www.terraform.io/docs/provisioners/connection.html\nTerraform vSphere Provider https://www.terraform.io/docs/providers/vsphere/index.html\nNode.js Todo Example App https://scotch.io/tutorials/creating-a-single-page-todo-app-with-node-and-angular\nGithub example code https://github.com/martezr/terraform-immutable-vsphere\n","date":"13 Oct, 2017","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cpicture\u003e\n  \u003csource srcset=\"/greenreedtech-website/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_545x0_resize_q95_h2_box.webp\" media=\"(max-width: 575px)\"\u003e\n  \u003csource srcset=\"/greenreedtech-website/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_600x0_resize_q95_h2_box.webp\" media=\"(max-width: 767px)\"\u003e\n  \u003csource srcset=\"/greenreedtech-website/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_700x0_resize_q95_h2_box.webp\" media=\"(max-width: 991px)\"\u003e\n  \u003csource srcset=\"/greenreedtech-website/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_h2_box.webp\"\u003e\n  \u003cimg loading=\"lazy\" decoding=\"async\" class=\"w-100 img-fluid rounded\" src=\"/greenreedtech-website/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_box.jpg\" alt=\"\" width=\"970\" height=\"500\"\u003e\n\u003c/picture\u003e\n \n \n \n\n","permalink":"https://martezr.github.io/greenreedtech-website/vsphere-immutable-infrastructure-with-terraform/","tags":["DevOps","Terraform","VMware"],"title":"vSphere Immutable Infrastructure with Terraform"},{"categories":["Terraform"],"contents":"Recently a co-worker of mine (Thanks Ken Erwin) introduced me to Jenkins Job Builder (http://docs.openstack.org/infra/jenkins-job-builder/) which is a project created by the OpenStack infrastructure team that aims to automate the creation of Jenkins Jobs. The software is written in python and utilizes either yaml or json files as the framework for creating Jenkins jobs. A list of some of the primary features is provided below.\nFeatures:  Supports job creation, modification, and deletion Supports templates to speed up the creation of similar jobs Supports all major plugins as well as extending the software to incorporate additional plugins The software has built-in test functionality to test changes before being deployed into production Many additional features  Example: The following section covers an example job in yaml format.\n- job: name: puppet-module-profiles project-type: matrix execution-strategy: sequential: true axes: - axis: type: user-defined name: PUPPET_VERSION values: - 3.8.2 - 3.8.4 - 4.0.0 - 4.1.0 - axis: type: slave name: nodes values: - puppet description: \u0026#39;Jenkins job to perform syntax and style checking on {name} puppet module.\u0026#39; disabled: false node: puppet scm: - git: url: ssh://git@stash.grt.local:7999/infrastructure/puppet-module-profiles.git branches: - origin/pr/{branch} credentials-id: \u0026#39;f46gdb08-b25e-7h9e-ngef-nar52j732j98\u0026#39; browser: auto triggers: - pollscm: cron: \u0026#34;\u0026#34; wrappers: - ansicolor: colormap: xterm - workspace-cleanup: include: - \u0026#34;*\u0026#34; - rvm-env: implementation: 1.9.3@pupet-module-profiles builders: - system-groovy: command: | def currentBuild = Thread.currentThread().executable def PULL_REQUEST_URL = build.buildVariableResolver.resolve(\u0026#39;PULL_REQUEST_URL\u0026#39;) def PULL_REQUEST_ID = build.buildVariableResolver.resolve(\u0026#39;PULL_REQUEST_ID\u0026#39;) def description = \u0026#34;\u0026lt;a href=\u0026#39;$PULL_REQUEST_URL\u0026#39;\u0026gt;PR #$PULL_REQUEST_ID\u0026lt;/a\u0026gt;\u0026#34; currentBuild.setDescription(description) - conditional-step: condition-kind: regex-match regex: \u0026#39;^(BUTTON_TRIGGER|OPENED|REOPENED|UPDATED)$\u0026#39; label: ${{ENV,var=\u0026#34;ACTION\u0026#34;}} steps: - shell: | #!/bin/bash echo \u0026#34;Build Trigger $ACTION\u0026#34; export PUPPET_VERSION=\u0026#34;$PUPPET_VERSION\u0026#34; gem install bundler bundle install - shell: | find . -iname *.pp -exec puppet-lint --no-80chars-check --log-format \u0026#34;%{{path}}:%{{line}}:%{{check}}:%{{KIND}}:%{{message}}\u0026#34; {{}} ; - shell: | for file in $(find . -iname \u0026#39;*.pp\u0026#39;); do puppet parser validate --render-as s --modulepath=modules \u0026#34;$file\u0026#34; || exit 1; done - conditional-step: condition-kind: regex-match regex: \u0026#39;^(MERGED)$\u0026#39; label: ${{ENV,var=\u0026#34;ACTION\u0026#34;}} steps: - shell: | #!/bin/bash echo \u0026#34;Build Trigger $ACTION\u0026#34; publishers: - warnings: console-log-parsers: - Puppet-Lint run-always: true - stash: url: \u0026#39;https://stash.grt.local:8443\u0026#39; ignore-ssl: true References: http://docslide.us/software/continuous-delivery-with-docker-and-jenkins-job-builder.html\n","date":"13 Jan, 2016","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cpicture\u003e\n  \u003csource srcset=\"/greenreedtech-website/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_545x0_resize_q95_h2_box.webp\" media=\"(max-width: 575px)\"\u003e\n  \u003csource srcset=\"/greenreedtech-website/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_600x0_resize_q95_h2_box.webp\" media=\"(max-width: 767px)\"\u003e\n  \u003csource srcset=\"/greenreedtech-website/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_700x0_resize_q95_h2_box.webp\" media=\"(max-width: 991px)\"\u003e\n  \u003csource srcset=\"/greenreedtech-website/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_h2_box.webp\"\u003e\n  \u003cimg loading=\"lazy\" decoding=\"async\" class=\"w-100 img-fluid rounded\" src=\"/greenreedtech-website/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_box.jpg\" alt=\"\" width=\"970\" height=\"500\"\u003e\n\u003c/picture\u003e\n \n \n \n\n","permalink":"https://martezr.github.io/greenreedtech-website/jenkins-job-builder/","tags":["DevOps","Terraform","VMware"],"title":"Jenkins Job Builder"},{"categories":["DevOps"],"contents":"This post covers integrating Jenkins CI server with Microsoft Active Directory to provide centralized authentication.\nStep #1 - Install he Active Directory plugin  Click \u0026ldquo;Manage Jenkins\u0026rdquo; from the sidebar\n  Click \u0026ldquo;Manage Plugins\u0026rdquo; to install the Active Directory plugin\n  Click on the \u0026ldquo;Available\u0026rdquo; tab, enter \u0026ldquo;Active Directory\u0026rdquo; in the \u0026ldquo;Filter:\u0026rdquo; search box, click the checkbox next to \u0026ldquo;Active Directory plugin\u0026rdquo; and finally click \u0026ldquo;Download now and install after restart\u0026rdquo;\n  Click the \u0026ldquo;Restart Jenkins when install is complete and no jobs are running\u0026rdquo; checkbox to complete the plugin installation.\n  Click \u0026ldquo;Configure Global Security\u0026rdquo;\n  Select \u0026ldquo;Active Directory\u0026rdquo; and enter the domain and domain accounts information.\n  **Select \u0026ldquo;Matrix-based security\u0026rdquo; and add the desired users and groups. **user names and group names are case sensitive****\n  Log into the web interface with AD credentials\n References: Jenkins AD Security http://justinramel.com/2013/01/15/active-directory-security/\nJenkins AD Plugin https://wiki.jenkins-ci.org/display/JENKINS/Active+Directory+Plugin\n","date":"20 Nov, 2015","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cpicture\u003e\n  \u003csource srcset=\"/greenreedtech-website/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_545x0_resize_q95_h2_box.webp\" media=\"(max-width: 575px)\"\u003e\n  \u003csource srcset=\"/greenreedtech-website/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_600x0_resize_q95_h2_box.webp\" media=\"(max-width: 767px)\"\u003e\n  \u003csource srcset=\"/greenreedtech-website/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_700x0_resize_q95_h2_box.webp\" media=\"(max-width: 991px)\"\u003e\n  \u003csource srcset=\"/greenreedtech-website/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_h2_box.webp\"\u003e\n  \u003cimg loading=\"lazy\" decoding=\"async\" class=\"w-100 img-fluid rounded\" src=\"/greenreedtech-website/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_box.jpg\" alt=\"\" width=\"970\" height=\"500\"\u003e\n\u003c/picture\u003e\n \n \n \n\n","permalink":"https://martezr.github.io/greenreedtech-website/jenkins-active-directory/","tags":["DevOps","Jenkins"],"title":"Jenkins Active Directory"},{"categories":["Puppet"],"contents":"Puppet supports various hiera backends to pull in external data from various sources. This post will cover integrating open source puppet with a couchdb database using the hiera-http backend.\nCouchDB Setup and Configuration The following steps will cover the setup and configuration of the CouchDB database server that will be used as the data store. The following steps assume that docker has been installed\nStart CouchDB Docker container\nThe container is started with a name of \u0026ldquo;couchdb\u0026rdquo; and the CouchDB admin username and password are set during container creation. Port 5984 is mapped to port 5984 on the host machine.\ndocker run -d --name couchdb -p 5984:5984 -e COUCHDB_USERNAME=couchadmin -e COUCHDB_PASSWORD=Password couchdb Database configuration\nCouchDB provides a web interface for managing the databases as well as REST API.\nStep #1 - Log into the CouchDB web interface using the credentials created during the container provisioning.\nClick login at the bottom right corner of the web page\nhttp://ip_address:5984/_utils Enter the login credentials Step #2 - Create a new database In our example configuration we\u0026rsquo;ll use \u0026ldquo;hiera\u0026rdquo; as the database to store all the puppet related documents.\nClick \u0026ldquo;Create Database\u0026rdquo; to create the new database Enter the database name and click \u0026ldquo;Create\u0026rdquo; to create the database. Step #3 - Create a new document The documents will act like the individual .yaml files in the yaml backend to provide a customized hierarchy.\nClick \u0026ldquo;New Document\u0026rdquo; to create the new document Enter the appropriate value for the \u0026ldquo;_id\u0026rdquo; field and Click \u0026ldquo;Add Field\u0026rdquo; to add a new field. In our example we use common which replicates the common.yaml file in the yaml backend structure.\nEnter the desired hiera data and click \u0026ldquo;Save Document\u0026rdquo; when done. The additional fields are used to store the actual data such as classes, and class variables. In our example we\u0026rsquo;ll add a couchdbtest value for testing.\nThe example below shows what the code would look like in a yaml file.\n--- couchdbtest: \u0026#39;Does it really work\u0026#39; The database has now been configured so we can move on to the puppet configuration. REST API The previous steps for configuring the database and fields can be performed utilizing the REST API provided by CouchDB. Basic authentication is used to manage the database and documents.\nThe following command creates the \u0026ldquo;hiera\u0026rdquo; database\ncurl -X PUT http://couchadmin:Password@10.0.0.148:5984/hiera The following command creates the \u0026ldquo;common\u0026rdquo; document\ncurl -X PUT http://couchadmin:Password@10.0.0.148:5984/hiera/common -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39;{\u0026#34;couchdbtest\u0026#34;:\u0026#34;Does it really work\u0026#34;}\u0026#39; Puppet Master Configuration The following steps will cover configuring Puppet to communicate with the CouchDB database.\nInstall hiera-http The hiera-http is installed via ruby-gems\ngem install hiera-http Update hiera.yaml config file Use a text editor to modify the hiera.yaml file.\n--- :backends: [\u0026#39;http\u0026#39;,\u0026#39;yaml\u0026#39;]  :hierarchy:  - defaults  - \u0026#34;%{clientcert}\u0026#34;  - \u0026#34;%{environment}\u0026#34;  - global  :yaml: # datadir is empty here, so hiera uses its defaults: # - /var/lib/hiera on *nix # - %CommonAppData%PuppetLabshieravar on Windows # When specifying a datadir, make sure the directory exists.  :datadir:   :http:  :host: 10.0.0.148  :port: 5984  :output: json  :failure: graceful  :use_auth: true  :auth_user: \u0026#39;couchadmin\u0026#39;  :auth_pass: \u0026#39;Password\u0026#39;  :paths:  - /hiera/%{clientcert}  - /hiera/%{environment}  - /hiera/common Perform hiera lookup We’ll perform a hiera lookup to verify that everything is working.\nhiera couchdbtest -d References: CouchDB Docker Image https://hub.docker.com/r/frodenas/couchdb/\nHiera-http configuration http://www.craigdunn.org/2012/11/puppet-data-from-couchdb-using-hiera-http/\nOpen source puppet master install http://blog.fnaard.com/2015/04/build-puppet-master-on-centos-7-hella.html\n","date":"19 Nov, 2015","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cpicture\u003e\n  \u003csource srcset=\"/greenreedtech-website/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_545x0_resize_q95_h2_box.webp\" media=\"(max-width: 575px)\"\u003e\n  \u003csource srcset=\"/greenreedtech-website/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_600x0_resize_q95_h2_box.webp\" media=\"(max-width: 767px)\"\u003e\n  \u003csource srcset=\"/greenreedtech-website/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_700x0_resize_q95_h2_box.webp\" media=\"(max-width: 991px)\"\u003e\n  \u003csource srcset=\"/greenreedtech-website/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_h2_box.webp\"\u003e\n  \u003cimg loading=\"lazy\" decoding=\"async\" class=\"w-100 img-fluid rounded\" src=\"/greenreedtech-website/images/post/05_hu9df52e9b6213b2ec47133dfa0e607f46_19199_1110x0_resize_q95_box.jpg\" alt=\"\" width=\"970\" height=\"500\"\u003e\n\u003c/picture\u003e\n \n \n \n\n","permalink":"https://martezr.github.io/greenreedtech-website/puppet-hiera-http-using-couchdb/","tags":["Puppet","Hiera"],"title":"Puppet Hiera HTTP Using CouchDB"}]